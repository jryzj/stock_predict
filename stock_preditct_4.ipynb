{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stock_preditct_4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OM3cHJETg0om",
        "uhyvAKdihCrC",
        "JZusNrhchOaS",
        "WkqwirDJnxSs",
        "RIfx1kh8nkAq",
        "v7l4RbsFikd4",
        "x17JDv8Wi12w",
        "lUmQNnUyjYPO",
        "llpn-tNzjwO3",
        "uzFRlwZzkAfw",
        "bgdFoC0ZqnOx",
        "ycLBn2g9eJoz",
        "yMaIpUCXdKAd",
        "TtE3stnXDcz1",
        "cH6D_yy4Dc0K",
        "4ji2nW0xNK9-",
        "0n5kGdKdNK-L",
        "XitBIfkFNK-d",
        "NeK-E3zCsBub",
        "QCpq_WYJsBux",
        "IOpu9aoFsBu-",
        "mFHZhz3NsBvU",
        "8WUuXH9nsBvj",
        "V1_k6_jXNQx6",
        "6qYIH7kzfsNh",
        "Vj_SJEQ_f2-h",
        "DyRaLilwhXDe",
        "51tkVCF26Agg",
        "LBQKvnNyvv76",
        "L7ibvQRGvv8G",
        "AQeHx4HBvv8O",
        "lJUabvmyvv8e",
        "NCk527zzvv85",
        "kKXBcctzvv95",
        "P8iDRzhCvv-S",
        "cKV8H5ywvv-m",
        "3TdN1QEEvv-5",
        "jo_Nwseevv_d",
        "V9ukse_8iNG1",
        "XqH6I3rXiNHC",
        "BN8mbQlliNHU",
        "szBLpsH5iNHt",
        "ASbuU208iNHz",
        "zkuQIxtaJ7jr"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jryzj/stock_predict/blob/master/stock_preditct_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhCsGpjYwSIn"
      },
      "source": [
        "##Begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwEQMN-Eeza0"
      },
      "source": [
        "##mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNldvRWte7Q6",
        "outputId": "5ad4e839-3859-4d8e-c2ad-29f92a731a11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdriver')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdriver\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pxq9oTrwWEX",
        "outputId": "a041496c-f772-44ec-b5a9-7578c40c926d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Oct  3 07:41:32 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8B6uYPSrUhK",
        "outputId": "9d7adaa7-7557-45ac-ebd1-f4fae40609c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "TCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6197923708dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'TCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMkJNgh2VhLp",
        "outputId": "759ba468-9fc3-4a57-d963-9372004074e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "#for tensorflow2\n",
        "\n",
        "import sys\n",
        "sys.path\n",
        "# sys.path[0]='/tensorflow-2.1.0/python3.6'\n",
        "sys.path[0]='/usr/local/lib/python3.6'\n",
        "sys.path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/usr/local/lib/python3.6',\n",
              " '/env/python',\n",
              " '/usr/lib/python36.zip',\n",
              " '/usr/lib/python3.6',\n",
              " '/usr/lib/python3.6/lib-dynload',\n",
              " '/usr/local/lib/python3.6/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n",
              " '/root/.ipython']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkMcs1dkWG__",
        "outputId": "e382b562-bfab-4c0e-9e24-f89860d3f86b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n",
        "\n",
        "# tf.compat.v1.disable_eager_execution()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNUuY7ea-XCf"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVxo7raE4Psq",
        "outputId": "ab5e91ea-1f1e-4d08-913a-58e71c7f4efc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "!ls -l /"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 104\n",
            "drwxr-xr-x   1 root root 4096 Sep 28 16:28 bin\n",
            "drwxr-xr-x   2 root root 4096 Apr 24  2018 boot\n",
            "drwxr-xr-x   1 root root 4096 Oct  1 06:34 content\n",
            "drwxr-xr-x   1 root root 4096 Sep 29 16:40 datalab\n",
            "drwxr-xr-x   5 root root  440 Oct  1 06:27 dev\n",
            "drwxr-xr-x   1 root root 4096 Oct  1 06:27 etc\n",
            "drwxr-xr-x   2 root root 4096 Apr 24  2018 home\n",
            "drwxr-xr-x   1 root root 4096 Sep 28 16:30 lib\n",
            "drwxr-xr-x   2 root root 4096 Sep 28 16:23 lib32\n",
            "drwxr-xr-x   2 root root 4096 Aug  7 22:41 lib64\n",
            "drwxr-xr-x   2 root root 4096 Aug  7 22:39 media\n",
            "drwxr-xr-x   2 root root 4096 Aug  7 22:39 mnt\n",
            "drwxr-xr-x   1 root root 4096 Oct  1 06:27 opt\n",
            "dr-xr-xr-x 141 root root    0 Oct  1 06:27 proc\n",
            "drwx------   1 root root 4096 Oct  1 07:10 root\n",
            "drwxr-xr-x   1 root root 4096 Sep 28 16:25 run\n",
            "drwxr-xr-x   1 root root 4096 Sep 28 16:28 sbin\n",
            "drwxr-xr-x   2 root root 4096 Aug  7 22:39 srv\n",
            "drwxr-xr-x   4 root root 4096 Sep 29 16:40 swift\n",
            "dr-xr-xr-x  12 root root    0 Oct  1 06:40 sys\n",
            "drwxr-xr-x   4 root root 4096 Sep 29 16:36 tensorflow-1.15.2\n",
            "drwxrwxrwt   1 root root 4096 Oct  1 09:46 tmp\n",
            "drwxr-xr-x   1 root root 4096 Sep 29 16:40 tools\n",
            "drwxr-xr-x   1 root root 4096 Oct  1 06:27 usr\n",
            "drwxr-xr-x   1 root root 4096 Oct  1 06:27 var\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-hnrLCb6zC6",
        "outputId": "074de70d-2f86-47db-8cd1-b6b4328d338e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        }
      },
      "source": [
        "# !pip install tensorflow==2.3.1\n",
        "# !pip install tf-nightly\n",
        "!pip install tf-nightly-gpu\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/00/ed4255da5bf45bbe7f833c5cc1b1caa4069349f84259ec1d6c8917cf5273/tf_nightly_gpu-2.4.0.dev20201001-cp36-cp36m-manylinux2010_x86_64.whl (391.8MB)\n",
            "\u001b[K     |████████████████████████████████| 391.8MB 36kB/s \n",
            "\u001b[?25hCollecting flatbuffers>=1.12\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.6.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.12.4)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.35.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.7.4.3)\n",
            "Collecting tb-nightly<3.0.0a0,>=2.4.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/16/60ac560dcc8822ba7f2ca07efa7828a4214b0c73689792ed0d17c5d5cf5d/tb_nightly-2.4.0a20201001-py3-none-any.whl (10.6MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6MB 62.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.3.0)\n",
            "Collecting tf-estimator-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/71/4ad680a5b14763cbf219f5c78448eb2a1329744b5cd05107ff7d5d8f4747/tf_estimator_nightly-2.4.0.dev2020100101-py2.py3-none-any.whl (461kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 56.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tf-nightly-gpu) (50.3.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (1.7.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (2.0.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (3.2.0)\n",
            "Installing collected packages: flatbuffers, tb-nightly, tf-estimator-nightly, tf-nightly-gpu\n",
            "Successfully installed flatbuffers-1.12 tb-nightly-2.4.0a20201001 tf-estimator-nightly-2.4.0.dev2020100101 tf-nightly-gpu-2.4.0.dev20201001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GnFSbgvWbzH",
        "outputId": "26d7e071-b237-4c3d-aace-ff4d1c8284b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6Yc7lCufBu2"
      },
      "source": [
        "#change path to notebook in cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw16QijjfKWc"
      },
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "# os.chdir(\"/../content/gdriver/My Drive/Colab Notebooks (1)\")\n",
        "os.chdir(\"/../content/gdriver/My Drive/Colab Notebooks\")\n",
        "\n",
        "# os.chdir(\"/../content/gdriver/My Drive\")\n",
        "# os.chdir(\"gdriver/My Drive/Colab Notebooks\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyT1uBAgFb2U"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZpj2K30fRlh"
      },
      "source": [
        "#set constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye2mvVArfUru"
      },
      "source": [
        "#常数设置\n",
        "\n",
        "threshold = 3.0 #涨幅\n",
        "start_date = \"2006-01-01\"\n",
        "end_date = \"2019-06-23\"\n",
        "stock_prefix = \"sh.6|sz.0|sz.300\"\n",
        "\n",
        "window_size = 90\n",
        "predict_window_size = 5\n",
        "# adjustflag = \"1\" #后复权\n",
        "adjustflag = \"3\" #不复权\n",
        "days_from_ipo = 40 #新股上市一般会连涨很多天，排除这些比较异常的天数\n",
        "\n",
        "data_fields = [\"open\", \"high\", \"low\", \"close\", \"turn\",\"pctChg\",\"tradestatus\",\"isST\",\"code\"]\n",
        "all_data_fields = \"date,code,open,high,low,close,preclose,volume,amount,adjustflag,turn,tradestatus,pctChg,isST\"\n",
        "data_fields_market_index = [\"open\", \"high\", \"low\", \"close\", \"turn\",\"pctChg\",\"date\"]\n",
        "\n",
        "\n",
        "# factor_num = len(data_fields) - 4 #pctChg, tradestatus, code isST不参与模型计算\n",
        "factor_num = 12\n",
        "\n",
        "all_stock_data = \"all_stock_data.csv\"\n",
        "all_stock_data_1 = \"all_stock_data_1.csv\"\n",
        "all_stock_data_2 = \"all_stock_data_2.csv\" #不复权\n",
        "all_stock_data_3 = \"all_stock_data_3.csv\" #with all data fields, no divid adjusting\n",
        "\n",
        "ts_code_file = \"ts_code.csv\"\n",
        "market_index = [\"sh.000001\", \"sz.399106\"]\n",
        "market_index_data_file = \"market_index_data.csv\"\n",
        "\n",
        "# predict_next_days = 1\n",
        "predict_next_days = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4pkQsmZepWu"
      },
      "source": [
        "#intall pack necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1OEz0tQfgFo",
        "outputId": "95fdf4c0-d060-4ef9-85cc-2dc96d40f4f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "!pip install baostock\n",
        "import baostock as bs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting baostock\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/e2/b367c78db42bafcf752442b7d582ba2a724286313d9f126c5fee06064fb2/baostock-0.8.8-py3-none-any.whl (55kB)\n",
            "\r\u001b[K     |██████                          | 10kB 20.4MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 40kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from baostock) (1.0.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->baostock) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->baostock) (1.18.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->baostock) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.18.0->baostock) (1.15.0)\n",
            "Installing collected packages: baostock\n",
            "Successfully installed baostock-0.8.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nveGeL_Mfl7K"
      },
      "source": [
        "#import basic pack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9pLaOARfpbN",
        "outputId": "26ef1136-7fb5-49c0-ec51-4908579ecff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "#引入基本包\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# import baostock as bs\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "import gc\n",
        "\n",
        "import tracemalloc\n",
        "\n",
        "\n",
        "np.random.seed(2019)\n",
        "\n",
        "!pip install gputil\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "\n",
        "def show_mem(gpu = False):\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " if gpu:\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "show_mem(gpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7411 sha256=98209c959f665719eda6665f734b82bc6cdc343789814ca96df123f585b56a6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Gen RAM Free: 26.4 GB  | Proc size: 112.7 MB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVjGRTMSfvxZ"
      },
      "source": [
        "#import keras pack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm0RuSbhgDQ4"
      },
      "source": [
        "##for tensorflow2\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "\n",
        "class Running(Callback):\n",
        "  def on_train_begin(self, sign_gen):\n",
        "    next(sign_gen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTrvFj1EQ3AQ",
        "outputId": "a86eab9e-a509-4a7e-c0c7-f0cec7d08cf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "!pip install --upgrade keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\r\u001b[K     |▉                               | 10kB 26.9MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 31.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 34.9MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 38.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 33.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 35.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 31.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102kB 27.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 27.2MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122kB 27.2MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133kB 27.2MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143kB 27.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 27.2MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163kB 27.2MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174kB 27.2MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184kB 27.2MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194kB 27.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204kB 27.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 215kB 27.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 27.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 27.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245kB 27.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256kB 27.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 266kB 27.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 276kB 27.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 286kB 27.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296kB 27.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307kB 27.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 317kB 27.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 327kB 27.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 337kB 27.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348kB 27.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358kB 27.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 368kB 27.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 27.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "Successfully installed keras-2.3.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKa8wdFDCWee",
        "outputId": "af7e8ebd-a391-45da-cb6b-cdf543b51b13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(keras.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD7sGnxvgEkL"
      },
      "source": [
        "#setup tensorboard if need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3XacfEsgJjo"
      },
      "source": [
        "LOG_DIR = \"/content/gdriver/My\\ Drive/Colab\\ Notebooks/tensor_board/stock_predict_2\"\n",
        "LOG_DIR1 = \"/content/gdriver/My Drive/Colab Notebooks/tensor_board/stock_predict_2\"\n",
        "\n",
        "from keras.callbacks import TensorBoard\n",
        "tbCallBack = TensorBoard(log_dir= LOG_DIR1 , histogram_freq=1,\n",
        "#                          write_graph=True,\n",
        "                         write_grads=True,\n",
        "                         batch_size=batch_size,\n",
        "#                          write_images=True\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXxNrLWKgX1m"
      },
      "source": [
        "#setup ngrok if need"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwbHlvXUgjp-"
      },
      "source": [
        "##install ngrok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5GO9QfOgR_h"
      },
      "source": [
        "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM3cHJETg0om"
      },
      "source": [
        "##setup ngrok account"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z7OQRlDgn0i"
      },
      "source": [
        "!./ngrok authtoken 6wz1ai2QDnxrk1HYUor4U_4zjD6PD8wUkGyMnWnmXiQ\n",
        "# get_ipython().system_raw('./ngrok authtoken 6wz1ai2QDnxrk1HYUor4U_4zjD6PD8wUkGyMnWnmXiQ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhyvAKdihCrC"
      },
      "source": [
        "##start ngrok service"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfjG1F1UhInS"
      },
      "source": [
        "#开启ngrok service，绑定port 6006(tensorboard)\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "# !./ngrok http 6006"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZusNrhchOaS"
      },
      "source": [
        "##get ngrok website"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6V1nPulhmJs"
      },
      "source": [
        "! curl http://localhost:4040/api/tunnels | python3 -c \\\n",
        "\"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkqwirDJnxSs"
      },
      "source": [
        "#get ts code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2dEYovPn1EZ"
      },
      "source": [
        "#获取股票代码清单\n",
        "#### 登陆系统 ####\n",
        "lg = bs.login()\n",
        "# 显示登陆返回信息\n",
        "print('login respond error_code:'+lg.error_code)\n",
        "print('login respond  error_msg:'+lg.error_msg)\n",
        "\n",
        "#### 获取证券信息 ####\n",
        "rs = bs.query_all_stock(day=\"2019-06-28\")\n",
        "print('query_all_stock respond error_code:'+rs.error_code)\n",
        "print('query_all_stock respond  error_msg:'+rs.error_msg)\n",
        "\n",
        "#### 打印结果集 ####\n",
        "data_list = []\n",
        "while (rs.error_code == '0') & rs.next():\n",
        "    # 获取一条记录，将记录合并在一起\n",
        "    data_list.append(rs.get_row_data())\n",
        "ts_code = pd.DataFrame(data_list, columns=rs.fields)\n",
        "\n",
        "#### 结果集输出到csv文件 ####   \n",
        "# ts_code.to_csv(\"D:\\\\all_stock.csv\", encoding=\"gbk\", index=False)\n",
        "print(ts_code)\n",
        "\n",
        "#### 登出系统 ####\n",
        "bs.logout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIfx1kh8nkAq"
      },
      "source": [
        "#get ts histroy data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SETWlt3InopO",
        "outputId": "ebafd0ed-126c-4fe0-abf6-f48ebbf75a34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#获取股票历史数据\n",
        "#用DataFrame.append的方法是需要重新分配内存，完成追加数据\n",
        "#先用list.append的方法是用指针指向追加数据，内存不重新分配，数据大的时候，效率高\n",
        "#每次获取一支股票的数据，追加到csv文件中。\n",
        "#不复权\n",
        "\n",
        "adjustflag = \"3\"  #不复权\n",
        "\n",
        "result = pd.DataFrame([],columns = all_data_fields.split(\",\"))\n",
        "result.to_csv(all_stock_data_3)\n",
        "\n",
        "bs.login()\n",
        "for i in range(len(ts_code)):\n",
        "    if i % 50 == 0:\n",
        "        print(i)\n",
        "    rs = bs.query_history_k_data_plus(ts_code.iloc[i][\"code\"], all_data_fields,\\\n",
        "                start_date = start_date, end_date = end_date,\\\n",
        "                  frequency = \"d\", adjustflag = adjustflag) #不复权\n",
        "\n",
        "    data_list = []\n",
        "    while (rs.error_code == \"0\") and rs.next():\n",
        "        row_data = rs.get_row_data()\n",
        "        # if(row_data[6] == \"1\"):    #  停牌的日子还是应该获取的。   \n",
        "        data_list.append(row_data)\n",
        "    result = pd.DataFrame(data_list, columns = rs.fields)\n",
        "    result.to_csv(all_stock_data_3, mode = \"a\", header=False)\n",
        "\n",
        "bs.logout()\n",
        "\n",
        "\n",
        "print(result.head())\n",
        "print(result.tail())\n",
        "print(result.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "login success!\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n",
            "550\n",
            "600\n",
            "650\n",
            "700\n",
            "750\n",
            "800\n",
            "850\n",
            "900\n",
            "950\n",
            "1000\n",
            "1050\n",
            "1100\n",
            "1150\n",
            "1200\n",
            "1250\n",
            "1300\n",
            "1350\n",
            "1400\n",
            "1450\n",
            "1500\n",
            "1550\n",
            "1600\n",
            "1650\n",
            "1700\n",
            "1750\n",
            "1800\n",
            "1850\n",
            "1900\n",
            "1950\n",
            "2000\n",
            "2050\n",
            "2100\n",
            "2150\n",
            "2200\n",
            "2250\n",
            "2300\n",
            "2350\n",
            "2400\n",
            "2450\n",
            "2500\n",
            "2550\n",
            "2600\n",
            "2650\n",
            "2700\n",
            "2750\n",
            "2800\n",
            "2850\n",
            "2900\n",
            "2950\n",
            "3000\n",
            "3050\n",
            "3100\n",
            "3150\n",
            "3200\n",
            "3250\n",
            "3300\n",
            "3350\n",
            "3400\n",
            "3450\n",
            "3500\n",
            "3550\n",
            "3600\n",
            "logout success!\n",
            "         date       code     open  ... tradestatus     pctChg isST\n",
            "0  2019-06-18  sz.300782  42.3500  ...           1  44.006800    0\n",
            "1  2019-06-19  sz.300782  55.9000  ...           1   9.996069    0\n",
            "2  2019-06-20  sz.300782  61.4900  ...           1  10.000000    0\n",
            "3  2019-06-21  sz.300782  67.6400  ...           1  10.001620    0\n",
            "\n",
            "[4 rows x 14 columns]\n",
            "         date       code     open  ... tradestatus     pctChg isST\n",
            "0  2019-06-18  sz.300782  42.3500  ...           1  44.006800    0\n",
            "1  2019-06-19  sz.300782  55.9000  ...           1   9.996069    0\n",
            "2  2019-06-20  sz.300782  61.4900  ...           1  10.000000    0\n",
            "3  2019-06-21  sz.300782  67.6400  ...           1  10.001620    0\n",
            "\n",
            "[4 rows x 14 columns]\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLnYyu6HqGbE",
        "outputId": "db94b553-3099-4fe9-d09c-fd40efdc89cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "#\n",
        "\n",
        "adjustflag = \"3\"  #不复权\n",
        "\n",
        "result = pd.DataFrame([],columns = all_data_fields.split(\",\"))\n",
        "result.to_csv(market_index_data)\n",
        "\n",
        "bs.login()\n",
        "for i in range(len(market_index)):\n",
        "  print(market_index[i])\n",
        "  rs = bs.query_history_k_data_plus(market_index[i], all_data_fields,\\\n",
        "                start_date = start_date, end_date = end_date,\\\n",
        "                  frequency = \"d\", adjustflag = adjustflag) #不复权\n",
        "\n",
        "  data_list = []\n",
        "  while (rs.error_code == \"0\") and rs.next():\n",
        "      row_data = rs.get_row_data() \n",
        "      data_list.append(row_data)\n",
        "  result = pd.DataFrame(data_list, columns = rs.fields)\n",
        "  result.to_csv(market_index_data, mode = \"a\", header=False)\n",
        "\n",
        "bs.logout()\n",
        "\n",
        "\n",
        "print(result.head())\n",
        "print(result.tail())\n",
        "print(result.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "login success!\n",
            "sh.000001\n",
            "sz.399106\n",
            "logout success!\n",
            "         date       code      open  ... tradestatus    pctChg isST\n",
            "0  2006-01-04  sz.399106  278.9900  ...           1  1.696865    0\n",
            "1  2006-01-05  sz.399106  283.8000  ...           1  1.993084    0\n",
            "2  2006-01-06  sz.399106  289.5700  ...           1  0.747070    0\n",
            "3  2006-01-09  sz.399106  291.4800  ...           1  1.023029    0\n",
            "4  2006-01-10  sz.399106  294.1500  ...           1  0.584498    0\n",
            "\n",
            "[5 rows x 14 columns]\n",
            "            date       code       open  ... tradestatus     pctChg isST\n",
            "3268  2019-06-17  sz.399106  1504.2410  ...           1  -0.195000    0\n",
            "3269  2019-06-18  sz.399106  1502.9070  ...           1   0.162700    0\n",
            "3270  2019-06-19  sz.399106  1542.4600  ...           1   1.475500    0\n",
            "3271  2019-06-20  sz.399106  1525.5030  ...           1   1.953800    0\n",
            "3272  2019-06-21  sz.399106  1567.0710  ...           1   1.339000    0\n",
            "\n",
            "[5 rows x 14 columns]\n",
            "3273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJZpV7D7ht8X"
      },
      "source": [
        "#load stock data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJrO2bDihxV1",
        "outputId": "26b3d20d-6649-4f22-b50e-6890fc34fc7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "ts_data = pd.read_csv(all_stock_data_3)\n",
        "print(ts_data.head())\n",
        "# print(ts_data.tail())\n",
        "\n",
        "ts_code = pd.read_csv(ts_code_file)\n",
        "# print(ts_code.head())\n",
        "\n",
        "market_index_data = pd.read_csv(market_index_data_file)\n",
        "print(market_index_data.head())\n",
        "# print(market_index_data.tail())\n",
        "\n",
        "market_index_data_sh = market_index_data[market_index_data[\"code\"] == market_index[0]]\n",
        "market_index_data_sh = market_index_data_sh[data_fields_market_index]\n",
        "print(market_index_data_sh.head())\n",
        "print(market_index_data_sh.tail())\n",
        "market_index_data_sz = market_index_data[market_index_data[\"code\"] == market_index[1]].reset_index()\n",
        "market_index_data_sz = market_index_data_sz[data_fields_market_index]\n",
        "print(market_index_data_sz.head())\n",
        "print(market_index_data_sz.tail())\n",
        "market_index_data = (market_index_data_sh.values,\\\n",
        "                     market_index_data_sz.values)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0        date       code  ...  tradestatus    pctChg  isST\n",
            "0           0  2006-01-04  sh.600000  ...            1  5.128205     0\n",
            "1           1  2006-01-05  sh.600000  ...            1  1.658537     0\n",
            "2           2  2006-01-06  sh.600000  ...            1  2.879081     0\n",
            "3           3  2006-01-09  sh.600000  ...            1 -1.212688     0\n",
            "4           4  2006-01-10  sh.600000  ...            1  0.283284     0\n",
            "\n",
            "[5 rows x 15 columns]\n",
            "   Unnamed: 0        date       code  ...  tradestatus    pctChg  isST\n",
            "0           0  2006-01-04  sh.000001  ...            1  1.714473   0.0\n",
            "1           1  2006-01-05  sh.000001  ...            1  1.380740   0.0\n",
            "2           2  2006-01-06  sh.000001  ...            1  1.015056   0.0\n",
            "3           3  2006-01-09  sh.000001  ...            1  0.516443   0.0\n",
            "4           4  2006-01-10  sh.000001  ...            1  0.407190   0.0\n",
            "\n",
            "[5 rows x 15 columns]\n",
            "       open      high       low     close      turn    pctChg        date\n",
            "0  1163.878  1181.004  1161.906  1180.963  0.015049  1.714473  2006-01-04\n",
            "1  1183.305  1197.837  1180.451  1197.269  0.019075  1.380740  2006-01-05\n",
            "2  1198.811  1215.536  1191.614  1209.422  0.022207  1.015056  2006-01-06\n",
            "3  1210.320  1217.314  1205.248  1215.668  0.018558  0.516443  2006-01-09\n",
            "4  1215.848  1220.756  1203.651  1220.618  0.017307  0.407190  2006-01-10\n",
            "          open      high       low     close      turn  pctChg        date\n",
            "3268  2880.422  2902.480  2877.394  2887.622  0.451135  0.1960  2019-06-17\n",
            "3269  2891.091  2898.330  2874.312  2890.158  0.432766  0.0878  2019-06-18\n",
            "3270  2944.115  2953.335  2916.214  2917.802  0.675970  0.9565  2019-06-19\n",
            "3271  2917.331  2997.388  2915.089  2987.118  0.852394  2.3756  2019-06-20\n",
            "3272  2990.368  3010.349  2989.245  3001.980  0.840151  0.4975  2019-06-21\n",
            "     open    high     low   close      turn    pctChg        date\n",
            "0  278.99  283.48  278.99  283.48  0.012810  1.696865  2006-01-04\n",
            "1  283.80  289.22  283.61  289.13  0.019449  1.993084  2006-01-05\n",
            "2  289.57  292.88  287.43  291.29  0.021566  0.747070  2006-01-06\n",
            "3  291.48  294.34  290.45  294.27  0.019832  1.023029  2006-01-09\n",
            "4  294.15  296.07  291.57  295.99  0.017998  0.584498  2006-01-10\n",
            "          open      high       low     close      turn  pctChg        date\n",
            "3268  1504.241  1513.623  1496.824  1502.122  1.243835 -0.1950  2019-06-17\n",
            "3269  1502.907  1510.965  1492.585  1504.566  1.173195  0.1627  2019-06-18\n",
            "3270  1542.460  1548.719  1526.071  1526.766  1.666313  1.4755  2019-06-19\n",
            "3271  1525.503  1563.322  1520.045  1556.596  1.918687  1.9538  2019-06-20\n",
            "3272  1567.071  1582.275  1566.933  1577.438  2.125991  1.3390  2019-06-21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnry9CHxaC6a",
        "outputId": "bf01b0c6-7685-4775-b7c7-5c5d4d82ee07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "type(ts_data.date[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_9rM78cheUj",
        "outputId": "58174e64-be72-4af3-a080-c1f9bcaab20a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "market_index_data[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, '2006-01-04', 'sh.000001', ..., 1, 1.714473, 0.0],\n",
              "       [1, '2006-01-05', 'sh.000001', ..., 1, 1.3807399999999999, 0.0],\n",
              "       [2, '2006-01-06', 'sh.000001', ..., 1, 1.015056, 0.0],\n",
              "       ...,\n",
              "       [3270, '2019-06-19', 'sh.000001', ..., 1, 0.9565, 0.0],\n",
              "       [3271, '2019-06-20', 'sh.000001', ..., 1, 2.3756, 0.0],\n",
              "       [3272, '2019-06-21', 'sh.000001', ..., 1, 0.4975, 0.0]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlNy3GK9cJFe",
        "outputId": "e5a39fc0-6dd4-4d35-944f-39292d0ddce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "ts_code = pd.read_csv(ts_code_file)\n",
        "print(ts_code.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0       code code_name\n",
            "0           0  sh.600000      浦发银行\n",
            "1           1  sh.600004      白云机场\n",
            "2           2  sh.600006      东风汽车\n",
            "3           3  sh.600007      中国国贸\n",
            "4           4  sh.600008      首创股份\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST2PaXXb0Fp_"
      },
      "source": [
        "#prepare dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eeF-RTo0OT_"
      },
      "source": [
        "##define dataset generating function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NjqnYAcG_Ja"
      },
      "source": [
        "def normals(np_array):\n",
        "  max_price = np_array.max()\n",
        "  min_price = np_array.min()\n",
        "  scale = max_price - min_price\n",
        "  return (np_array - min_price)/scale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kEmGkQd0D6N"
      },
      "source": [
        "\n",
        "\n",
        "def gntor_like(ts_data, ts_code, mark_index_data, pct_of_stock=0.05, edge=10.0,\\\n",
        "                    shuffle = 0, y_cate = 1):  \n",
        "\n",
        "  stock_qty = len(ts_code)\n",
        "\n",
        "  \n",
        "  if pct_of_stock != 1:\n",
        "    index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "    ts_code = ts_code[ts_code.index.isin(index)].code.values\n",
        "  else:\n",
        "    index = ts_code.code.values\n",
        "  \n",
        "  print(len(index))                   \n",
        "\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  for i in index:\n",
        "\n",
        "    stk_data = ts_data[ts_data[\"code\"] == i]\n",
        "    length = len(stk_data)\n",
        "    if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "      for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "        ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "            [[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\"]].values.astype(\"float\")\n",
        "        if np.all(ohlct[:,5] != 1) and \\\n",
        "          np.all(ohlct[:,4] != 0) and \\\n",
        "        np.all(ohlct[:,6] >= -10.1) and \\\n",
        "        np.all(ohlct[:,6] <= 10.1) :\n",
        "          ohlct[:-predict_next_days,:4] = normals(ohlct[:-predict_next_days,:4])\n",
        "\n",
        "          date_range = stk_data.iloc[l : l + window_size + predict_next_days][\"date\"]\n",
        "\n",
        "          sh_index_data = market_index_data[(market_index_data[\"code\"] == market_index[0])\\\n",
        "           & (market_index_data[\"date\"].isin(date_range))]\n",
        "          sh_index_data = sh_index_data[:-predict_next_days][[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "          sh_index_data = normals(sh_index_data)\n",
        "\n",
        "\n",
        "          sz_index_data = market_index_data[(market_index_data[\"code\"] == market_index[1])\\\n",
        "           & (market_index_data[\"date\"].isin(date_range))]\n",
        "          sz_index_data = sz_index_data[:-predict_next_days][[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "          sz_index_data = normals(sz_index_data) \n",
        "\n",
        "          x.append(np.expand_dims(np.hstack((ohlct[:-predict_next_days,:4], sh_index_data, sz_index_data)), axis=-1))\n",
        "\n",
        "          # next_days_pctChg = ohlct[-predict_next_days:,-1].sum()  #wrong\n",
        "          next_days_pctChg = (ohlct[-1,3] - ohlct[-predict_next_days,3]) * 100 / ohlct[-predict_next_days,3]\n",
        "\n",
        "          if next_days_pctChg  <= -edge:\n",
        "            y.append(0)\n",
        "          elif next_days_pctChg  <= 0:\n",
        "            y.append(1)\n",
        "          elif next_days_pctChg <= edge:\n",
        "            y.append(2)\n",
        "          else:\n",
        "            y.append(3)\n",
        "\n",
        "  if len(x) > 0:\n",
        "    X = np.array(x)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # upsample\n",
        "    # dis = district\n",
        "\n",
        "    dis_count, dis_edge = np.histogram(y, bins = 4, range=(0,3))\n",
        "\n",
        "    dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "    dis_count_max = dis_count.max()\n",
        "\n",
        "    time = dis_count_max/dis_count\n",
        "\n",
        "    # time = time.astype(int)\n",
        "\n",
        "    # time_count = np.apply_along_axis((lambda d: time[d.astype(int)]), \\\n",
        "    #                                 0 , y)\n",
        "\n",
        "    # X = np.repeat(X, time_count, axis = 0)\n",
        "    # y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "    # shuffle dataset\n",
        "    if shuffle:\n",
        "      length = X.shape[0]\n",
        "\n",
        "      r_index = np.arange(length)\n",
        "      np.random.shuffle(r_index)\n",
        "\n",
        "      X = X[r_index]\n",
        "      y = y[r_index]\n",
        "\n",
        "    # r_index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "    # X = X[r_index]\n",
        "    # y = y[r_index]\n",
        "\n",
        "    if y_cate:\n",
        "      y = utils.to_categorical(y, num_classes=4)\n",
        "    return X, y, time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqdiohvyhSe1"
      },
      "source": [
        "##make dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czNCx7cYhYWx",
        "outputId": "912805d5-ca62-4c41-f0eb-16d9d8ce619b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pct_of_stock = 1\n",
        "edge = 10.0\n",
        "\n",
        "X1, y1, cls_weight = gntor(result, ts_code[:10], market_index_data, pct_of_stock, edge, [], 1, 0)\n",
        "\n",
        "np.save(\"X-3-1-7-w1000.npy\", X1)\n",
        "np.save(\"y-3-1-7-w1000.npy\", y1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iYKSNjCGwLG"
      },
      "source": [
        "##generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_lTbB-JGzzQ"
      },
      "source": [
        "###G1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZv5MMkQG2MW"
      },
      "source": [
        "def G1(ts_data, ts_code, mark_index_data, pct_of_stock=0.05, edge=10.0,\\\n",
        "        shuffle = 0, y_cate = 1, batch_size=256, generator = 1, repeat_time = 10):  \n",
        "\n",
        "  stock_qty = len(ts_code)\n",
        "\n",
        "  while True:\n",
        "    if pct_of_stock != 1:\n",
        "      index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "      index = ts_code[ts_code.index.isin(index)].code.values\n",
        "    else:\n",
        "      index = ts_code.code.values\n",
        "    \n",
        "    print(len(index))                   \n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in index:\n",
        "      stk_data = ts_data[ts_data[\"code\"] == i]\n",
        "      length = len(stk_data)\n",
        "      if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "        # print(\"0:\", datetime.datetime.today())\n",
        "        for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "          # print(\"1:\", datetime.datetime.today())\n",
        "          ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "              [[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\"]].values.astype(\"float\")\n",
        "          if np.all(ohlct[:,5] != 1) and \\\n",
        "            np.all(ohlct[:,4] != 0) and \\\n",
        "          np.all(ohlct[:,6] >= -10.1) and \\\n",
        "          np.all(ohlct[:,6] <= 10.1) :\n",
        "            # print(\"2:\", datetime.datetime.today())\n",
        "            ohlct[:-predict_next_days,:4] = normals(ohlct[:-predict_next_days,:4])\n",
        "\n",
        "            date_range = stk_data.iloc[l : l + window_size + predict_next_days][\"date\"]\n",
        "            # print(date_range.iloc[0])\n",
        "            # print(type(date_range))\n",
        "\n",
        "            # print(\"2.1:\", datetime.datetime.today())\n",
        "            index_begin = market_index_data[0][market_index_data[0][\"date\"] == date_range.iloc[0]].index.tolist()[0]\n",
        "            # print(\"2.2:\", datetime.datetime.today())\n",
        "            sh_index_data = market_index_data[0][index_begin: index_begin + window_size]\n",
        "            # print(\"2.3:\", datetime.datetime.today())\n",
        "            sh_index_data = sh_index_data[[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "            # print(\"2.4:\", datetime.datetime.today())\n",
        "            sh_index_data = normals(sh_index_data)\n",
        "            # print(\"2.5:\", datetime.datetime.today())\n",
        "\n",
        "            index_begin = market_index_data[1][market_index_data[1][\"date\"] == date_range.iloc[0]].index.tolist()[0]\n",
        "            sz_index_data = market_index_data[1][index_begin: index_begin + window_size]\n",
        "            sz_index_data = sz_index_data[[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "            sz_index_data = normals(sz_index_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # date_range = stk_data.iloc[l : l + window_size + predict_next_days][\"date\"]\n",
        "\n",
        "            # sh_index_data = market_index_data[0][market_index_data[0][\"date\"].isin(date_range)]\n",
        "            # sh_index_data = sh_index_data[:-predict_next_days][[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "            # sh_index_data = normals(sh_index_data)\n",
        "\n",
        "\n",
        "            # sz_index_data = market_index_data[1][market_index_data[1][\"date\"].isin(date_range)]\n",
        "            # sz_index_data = sz_index_data[:-predict_next_days][[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "            # sz_index_data = normals(sz_index_data) \n",
        "\n",
        "            # print(\"3:\", datetime.datetime.today())\n",
        "            x.append(np.expand_dims(np.hstack((ohlct[:-predict_next_days,:4], sh_index_data, sz_index_data)), axis=-1))\n",
        "\n",
        "            # print(\"4:\", datetime.datetime.today())\n",
        "            next_days_pctChg = (ohlct[-1,3] - ohlct[-predict_next_days,3]) * 100 / ohlct[-predict_next_days,3]\n",
        "\n",
        "            if next_days_pctChg  <= -edge:\n",
        "              y.append(0)\n",
        "            elif next_days_pctChg  <= 0:\n",
        "              y.append(1)\n",
        "            elif next_days_pctChg <= edge:\n",
        "              y.append(2)\n",
        "            else:\n",
        "              y.append(3)\n",
        "\n",
        "    if len(x) > 0:\n",
        "      print(len(x), datetime.datetime.today())\n",
        "      X = np.array(x)\n",
        "      y = np.array(y)\n",
        "\n",
        "      # upsample\n",
        "      # dis = district\n",
        "\n",
        "      dis_count, dis_edge = np.histogram(y, bins = 4, range=(0,3))\n",
        "\n",
        "      dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "      dis_count_min = dis_count.min()\n",
        "      print(dis_count_min)\n",
        "\n",
        "      time = dis_count_min/dis_count\n",
        "      print(time)\n",
        "\n",
        "      sample_weight = np.array(y).astype(\"float\")\n",
        "      sample_weight[sample_weight == 0] = time[0]\n",
        "      sample_weight[sample_weight == 1] = time[1]\n",
        "      sample_weight[sample_weight == 2] = time[2]\n",
        "      sample_weight[sample_weight == 3] = time[3]\n",
        "\n",
        "\n",
        "      # time = time.astype(int)\n",
        "\n",
        "      # time_count = np.apply_along_axis((lambda d: time[d.astype(int)]), \\\n",
        "      #                                 0 , y)\n",
        "\n",
        "      # X = np.repeat(X, time_count, axis = 0)\n",
        "      # y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "      # shuffle dataset\n",
        "      if shuffle:\n",
        "        length = X.shape[0]\n",
        "\n",
        "        r_index = np.arange(length)\n",
        "        np.random.shuffle(r_index)\n",
        "\n",
        "        X = X[r_index]\n",
        "        y = y[r_index]\n",
        "\n",
        "      # r_index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "      # X = X[r_index]\n",
        "      # y = y[r_index]\n",
        "\n",
        "      if y_cate:\n",
        "        y = utils.to_categorical(y, num_classes=4)\n",
        "      \n",
        "      if generator == 1:\n",
        "        for ii in range(repeat_time):\n",
        "          for i in range(0, y.shape[0], batch_size):\n",
        "            yield (X[i : i + batch_size], y[i : i + batch_size]\\\n",
        "                  , sample_weight[i : i + batch_size]\\\n",
        "                  )\n",
        "      else:\n",
        "        print(\"0\")\n",
        "        return X, y, sample_weight\n",
        "\n",
        "        \n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzt9JFfbWSMn"
      },
      "source": [
        "###G2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgbmxNZIWWb6"
      },
      "source": [
        "def for_isin(row, index, collection):\n",
        "  return row[index] in collection\n",
        "\n",
        "def G2(ts_data, ts_code, mark_index_data, pct_of_stock=0.05, edge=10.0,\\\n",
        "        shuffle = 0, y_cate = 1, batch_size=256, generator = 1, repeat_time = 10):  \n",
        "\n",
        "  stock_qty = len(ts_code)\n",
        "\n",
        "  # x = None\n",
        "  # y = None\n",
        "  # X = None\n",
        "  # sample_weight = None\n",
        "  # stk_data = None\n",
        "\n",
        "  while True:    \n",
        "\n",
        "\n",
        "    x = None\n",
        "    y = None\n",
        "    X = None\n",
        "    sample_weight = None\n",
        "    stk_data = None\n",
        "    print(\"collect\",gc.collect())\n",
        "\n",
        "    show_mem(gpu)\n",
        "\n",
        "    if pct_of_stock != 1:\n",
        "      index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "      index = ts_code[ts_code.index.isin(index)].code.values\n",
        "    else:\n",
        "      index = ts_code.code.values\n",
        "    \n",
        "    print(len(index))                   \n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in index:\n",
        "      stk_data = ts_data[ts_data[\"code\"] == i]\n",
        "      stk_data = stk_data[[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\", \"date\"]].values\n",
        "      length = len(stk_data)\n",
        "      if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "        # print(\"0:\", datetime.datetime.today())\n",
        "        for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "          # print(\"1:\", datetime.datetime.today())\n",
        "          ohlct = stk_data[l : l + window_size + predict_next_days,:-1].astype(\"float\")\n",
        "          # print(id(ohlct))\n",
        "          # print(id(stk_data[l : l + window_size + predict_next_days,:-1]))\n",
        "          if np.all(ohlct[:,5] != 1) and \\\n",
        "            np.all(ohlct[:,4] != 0) and \\\n",
        "            np.all(ohlct[:,6] >= -10.1) and \\\n",
        "            np.all(ohlct[:,6] <= 10.1) :\n",
        "            # print(\"2:\", datetime.datetime.today())\n",
        "            ohlct[:-predict_next_days,:4] = normals(ohlct[:-predict_next_days,:4])\n",
        "            # print(id(ohlct))\n",
        "            date_range = stk_data[l, 7]\n",
        "            # print(date_range.iloc[0])\n",
        "            # print(type(date_range))\n",
        "\n",
        "            # print(\"2.1:\", datetime.datetime.today())\n",
        "            index_begin = np.where(market_index_data[0][:,-1] == date_range)[0][0]\n",
        "            # print(index_begin)\n",
        "            # print(\"2.2:\", datetime.datetime.today())\n",
        "            sh_index_data = market_index_data[0][index_begin: index_begin + window_size, :]\n",
        "            # sh_index_data = market_index_data[0][np.in1d(market_index_data[0][:,-1],date_range)]\n",
        "            # sh_index_data = market_index_data[0][np.apply_along_axis(for_isin, 1, market_index_data[0], -1, date_range)]\n",
        "            # print(\"2.3:\", datetime.datetime.today())\n",
        "            sh_index_data = sh_index_data[:,:4].astype(\"float\")\n",
        "            # print(\"2.4:\", datetime.datetime.today())\n",
        "            sh_index_data = normals(sh_index_data)\n",
        "            # print(\"2.5:\", datetime.datetime.today())\n",
        "\n",
        "            sz_index_data = market_index_data[1][index_begin: index_begin + window_size, :]\n",
        "            # sz_index_data = market_index_data[1][np.in1d(market_index_data[1][:,-1],date_range)]\n",
        "            sz_index_data = sz_index_data[:,:4].astype(\"float\")\n",
        "            sz_index_data = normals(sz_index_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # date_range = stk_data.iloc[l : l + window_size + predict_next_days][\"date\"]\n",
        "\n",
        "            # sh_index_data = market_index_data[0][market_index_data[0][\"date\"].isin(date_range)]\n",
        "            # sh_index_data = sh_index_data[:-predict_next_days][[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "            # sh_index_data = normals(sh_index_data)\n",
        "\n",
        "\n",
        "            # sz_index_data = market_index_data[1][market_index_data[1][\"date\"].isin(date_range)]\n",
        "            # sz_index_data = sz_index_data[:-predict_next_days][[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "            # sz_index_data = normals(sz_index_data) \n",
        "\n",
        "            # print(\"3:\", datetime.datetime.today())\n",
        "            x.append(np.expand_dims(np.hstack((ohlct[:-predict_next_days,:4], sh_index_data, sz_index_data)), axis=-1))\n",
        "\n",
        "            # print(\"4:\", datetime.datetime.today())\n",
        "            next_days_pctChg = (ohlct[-1,3] - ohlct[-predict_next_days,3]) * 100 / ohlct[-predict_next_days,3]\n",
        "\n",
        "            if next_days_pctChg  <= -edge:\n",
        "              y.append(0)\n",
        "            elif next_days_pctChg  <= 0:\n",
        "              y.append(1)\n",
        "            elif next_days_pctChg <= edge:\n",
        "              y.append(2)\n",
        "            else:\n",
        "              y.append(3)\n",
        "\n",
        "\n",
        "    if len(x) > 0:\n",
        "      print(len(x), datetime.datetime.today())\n",
        "      X = np.array(x)\n",
        "      y = np.array(y)\n",
        "\n",
        "      # upsample\n",
        "      # dis = district\n",
        "\n",
        "      dis_count, dis_edge = np.histogram(y, bins = 4, range=(0,3))\n",
        "\n",
        "      dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "      dis_count_min = dis_count.min()\n",
        "      print(dis_count_min)\n",
        "\n",
        "      time = dis_count_min/dis_count\n",
        "      print(time)\n",
        "\n",
        "      sample_weight = np.array(y).astype(\"float\")\n",
        "      sample_weight[sample_weight == 0] = time[0]\n",
        "      sample_weight[sample_weight == 1] = time[1]\n",
        "      sample_weight[sample_weight == 2] = time[2]\n",
        "      sample_weight[sample_weight == 3] = time[3]\n",
        "\n",
        "\n",
        "      # time = time.astype(int)\n",
        "\n",
        "      # time_count = np.apply_along_axis((lambda d: time[d.astype(int)]), \\\n",
        "      #                                 0 , y)\n",
        "\n",
        "      # X = np.repeat(X, time_count, axis = 0)\n",
        "      # y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "      # shuffle dataset\n",
        "      if shuffle:\n",
        "        length = X.shape[0]\n",
        "\n",
        "        r_index = np.arange(length)\n",
        "        np.random.shuffle(r_index)\n",
        "\n",
        "        X = X[r_index]\n",
        "        y = y[r_index]\n",
        "        sample_weight = sample_weight[r_index]\n",
        "\n",
        "      # r_index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "      # X = X[r_index]\n",
        "      # y = y[r_index]\n",
        "\n",
        "      if y_cate:\n",
        "        y = utils.to_categorical(y, num_classes=4)\n",
        "      \n",
        "      if generator == 1:\n",
        "        set_length = int(y.shape[0]/batch_size) * batch_size\n",
        "        X = X[:set_length]\n",
        "        y = y[:set_length]\n",
        "        sample_weight = sample_weight[:set_length]        \n",
        "        for ii in range(repeat_time):\n",
        "          for i in range(0, set_length, batch_size):\n",
        "            # pppp= 0\n",
        "            yield (X[i : i + batch_size], y[i : i + batch_size]\\\n",
        "                  , sample_weight[i : i + batch_size]\\\n",
        "                  )\n",
        "        \n",
        "      else:\n",
        "        print(\"0\")\n",
        "        # return X, y, sample_weight\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbV86J7K5n1v"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTCPbOqg5sIM"
      },
      "source": [
        "###G2_1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOA3sFLE5sIO"
      },
      "source": [
        "def for_isin(row, index, collection):\n",
        "  return row[index] in collection\n",
        "\n",
        "def running_sign(msg=\"\"):\n",
        "    sign = [\"\\r\\\\\", \"\\r|\", \"\\r/\", \"\\r-\", \"\\r|\", \"\\r/\", \"\\r-\" ]\n",
        "    while True:\n",
        "        for i in range(len(sign)):\n",
        "            print(sign[i], end=msg)\n",
        "            yield i\n",
        "            \n",
        "r = running_sign()\n",
        "\n",
        "def G2_1(ts_data, ts_code, mark_index_data, pct_of_stock=0.05, edge=10.0,\\\n",
        "        shuffle = 0, y_cate = 1, batch_size=256, generator = 1, repeat_time = 10):  \n",
        "\n",
        "  stock_qty = len(ts_code)\n",
        "\n",
        "  # x = None\n",
        "  # y = None\n",
        "  # X = None\n",
        "  # sample_weight = None\n",
        "  # stk_data = None\n",
        "\n",
        "  while True:    \n",
        "\n",
        "\n",
        "    x = None\n",
        "    y = None\n",
        "    X = None\n",
        "    sample_weight = None\n",
        "    stk_data = None\n",
        "    print(\"collect\",gc.collect())\n",
        "\n",
        "    show_mem(gpu)\n",
        "\n",
        "    if pct_of_stock != 1:\n",
        "      index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "      index = ts_code[ts_code.index.isin(index)].code.values\n",
        "    else:\n",
        "      index = ts_code.code.values\n",
        "    \n",
        "    print(len(index))                   \n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in index:\n",
        "      stk_data = ts_data[ts_data[\"code\"] == i]\n",
        "      stk_data = stk_data[[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\", \"date\"]].values\n",
        "      length = len(stk_data)\n",
        "      if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "        # print(\"0:\", datetime.datetime.today())\n",
        "        for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "          # print(\"1:\", datetime.datetime.today())\n",
        "          ohlct = stk_data[l : l + window_size + predict_next_days,:-1].astype(\"float\")\n",
        "          # print(id(ohlct))\n",
        "          # print(id(stk_data[l : l + window_size + predict_next_days,:-1]))\n",
        "          if np.all(ohlct[:,5] != 1) and \\\n",
        "            np.all(ohlct[:,4] != 0) and \\\n",
        "            np.all(ohlct[:,6] >= -10.1) and \\\n",
        "            np.all(ohlct[:,6] <= 10.1) :\n",
        "            # print(\"2:\", datetime.datetime.today())\n",
        "            ohlct[:-predict_next_days,:4] = normals(ohlct[:-predict_next_days,:4])\n",
        "            # print(id(ohlct))\n",
        "            date_range = stk_data[l, 7]\n",
        "            # print(date_range.iloc[0])\n",
        "            # print(type(date_range))\n",
        "\n",
        "            # print(\"2.1:\", datetime.datetime.today())\n",
        "            index_begin = np.where(market_index_data[0][:,-1] == date_range)[0][0]\n",
        "            # print(index_begin)\n",
        "            # print(\"2.2:\", datetime.datetime.today())\n",
        "            sh_index_data = market_index_data[0][index_begin: index_begin + window_size, :]\n",
        "            # sh_index_data = market_index_data[0][np.in1d(market_index_data[0][:,-1],date_range)]\n",
        "            # sh_index_data = market_index_data[0][np.apply_along_axis(for_isin, 1, market_index_data[0], -1, date_range)]\n",
        "            # print(\"2.3:\", datetime.datetime.today())\n",
        "            sh_index_data = sh_index_data[:,:4].astype(\"float\")\n",
        "            # print(\"2.4:\", datetime.datetime.today())\n",
        "            sh_index_data = normals(sh_index_data)\n",
        "            # print(\"2.5:\", datetime.datetime.today())\n",
        "\n",
        "            sz_index_data = market_index_data[1][index_begin: index_begin + window_size, :]\n",
        "            # sz_index_data = market_index_data[1][np.in1d(market_index_data[1][:,-1],date_range)]\n",
        "            sz_index_data = sz_index_data[:,:4].astype(\"float\")\n",
        "            sz_index_data = normals(sz_index_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # date_range = stk_data.iloc[l : l + window_size + predict_next_days][\"date\"]\n",
        "\n",
        "            # sh_index_data = market_index_data[0][market_index_data[0][\"date\"].isin(date_range)]\n",
        "            # sh_index_data = sh_index_data[:-predict_next_days][[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "            # sh_index_data = normals(sh_index_data)\n",
        "\n",
        "\n",
        "            # sz_index_data = market_index_data[1][market_index_data[1][\"date\"].isin(date_range)]\n",
        "            # sz_index_data = sz_index_data[:-predict_next_days][[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "            # sz_index_data = normals(sz_index_data) \n",
        "\n",
        "            # print(\"3:\", datetime.datetime.today())\n",
        "            x.append(np.expand_dims(np.hstack((ohlct[:-predict_next_days,:4], sh_index_data, sz_index_data)), axis=-1))\n",
        "\n",
        "            # print(\"4:\", datetime.datetime.today())\n",
        "            next_days_pctChg = (ohlct[-1,3] - ohlct[-predict_next_days,3]) * 100 / ohlct[-predict_next_days,3]\n",
        "\n",
        "            if next_days_pctChg  <= -edge:\n",
        "              y.append(0)\n",
        "            elif next_days_pctChg  <= 0:\n",
        "              y.append(1)\n",
        "            elif next_days_pctChg <= edge:\n",
        "              y.append(2)\n",
        "            else:\n",
        "              y.append(3)\n",
        "\n",
        "            next(r)\n",
        "\n",
        "    if len(x) > 0:\n",
        "      print(len(x), datetime.datetime.today())\n",
        "      X = np.array(x)\n",
        "      y = np.array(y)\n",
        "\n",
        "      # upsample\n",
        "      # dis = district\n",
        "\n",
        "      dis_count, dis_edge = np.histogram(y, bins = 4, range=(0,3))\n",
        "\n",
        "      dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "      dis_count_min = dis_count.min()\n",
        "      print(dis_count_min)\n",
        "\n",
        "      time = dis_count_min/dis_count\n",
        "      print(time)\n",
        "\n",
        "      sample_weight = np.array(y).astype(\"float\")\n",
        "      sample_weight[sample_weight == 0] = time[0]\n",
        "      sample_weight[sample_weight == 1] = time[1]\n",
        "      sample_weight[sample_weight == 2] = time[2]\n",
        "      sample_weight[sample_weight == 3] = time[3]\n",
        "\n",
        "\n",
        "      # time = time.astype(int)\n",
        "\n",
        "      # time_count = np.apply_along_axis((lambda d: time[d.astype(int)]), \\\n",
        "      #                                 0 , y)\n",
        "\n",
        "      # X = np.repeat(X, time_count, axis = 0)\n",
        "      # y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "      # shuffle dataset\n",
        "      if shuffle:\n",
        "        length = X.shape[0]\n",
        "\n",
        "        r_index = np.arange(length)\n",
        "        np.random.shuffle(r_index)\n",
        "\n",
        "        X = X[r_index]\n",
        "        y = y[r_index]\n",
        "        sample_weight = sample_weight[r_index]\n",
        "\n",
        "      # r_index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "      # X = X[r_index]\n",
        "      # y = y[r_index]\n",
        "\n",
        "      if y_cate:\n",
        "        y = utils.to_categorical(y, num_classes=4)\n",
        "      \n",
        "      if generator == 1:\n",
        "        set_length = int(y.shape[0]/batch_size) * batch_size\n",
        "        X = X[:set_length]\n",
        "        y = y[:set_length]\n",
        "        sample_weight = sample_weight[:set_length]        \n",
        "        for ii in range(repeat_time):\n",
        "          for i in range(0, set_length, batch_size):\n",
        "            pass\n",
        "            # yield (X[i : i + batch_size], y[i : i + batch_size]\\\n",
        "            #       # , sample_weight[i : i + batch_size]\\\n",
        "            #       # )        \n",
        "      else:\n",
        "        print(\"0\")\n",
        "        return X, y, sample_weight\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeVRSkylh3Ve"
      },
      "source": [
        "#test model are at below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQNsLIzGjMhB"
      },
      "source": [
        "##Model 4-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWVVNt95jbHl"
      },
      "source": [
        "def make_model(summary = False):\n",
        "  K.clear_session()\n",
        "  # tf.keras.backend.clear_session()\n",
        "  p_lstm = 64\n",
        "  p_conv = 256\n",
        "\n",
        "  input = layers.Input(shape = (window_size, factor_num, 1))\n",
        "\n",
        "\n",
        "  model_1 = layers.Conv2D(p_conv, kernel_size =(1,factor_num))(input)\n",
        "  model_1 = layers.Activation(\"relu\")(model_1)\n",
        "  model_1 = layers.BatchNormalization()(model_1)\n",
        "  model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "  model_1 = layers.Reshape((-1,64))(model_1)\n",
        "\n",
        "  model_1 = layers.LSTM(p_lstm)(model_1)\n",
        "\n",
        "\n",
        "  model_5 = layers.Conv2D(p_conv, kernel_size =(5,factor_num))(input)\n",
        "  model_5 = layers.Activation(\"relu\")(model_5)\n",
        "  model_5 = layers.BatchNormalization()(model_5)\n",
        "  model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "  model_5 = layers.Reshape((-1,128))(model_5)\n",
        "\n",
        "  model_5 = layers.LSTM(p_lstm)(model_5)\n",
        "\n",
        "  model_10 = layers.Conv2D(p_conv, kernel_size =(10,factor_num))(input)\n",
        "  model_10 = layers.Activation(\"relu\")(model_10)\n",
        "  model_10 = layers.BatchNormalization()(model_10)\n",
        "  model_10 = layers.Dropout(0.5)(model_10)\n",
        "\n",
        "  model_10 = layers.Reshape((-1,256))(model_10)\n",
        "\n",
        "  model_10 = layers.LSTM(p_lstm)(model_10)\n",
        "\n",
        "  model_15 = layers.Conv2D(p_conv, kernel_size =(15,factor_num))(input)\n",
        "  model_15 = layers.Activation(\"relu\")(model_15)\n",
        "  model_15 = layers.BatchNormalization()(model_15)\n",
        "  model_15 = layers.Dropout(0.5)(model_15)\n",
        "\n",
        "  model_15 = layers.Reshape((-1,256))(model_15)\n",
        "\n",
        "  model_15 = layers.LSTM(p_lstm)(model_15)\n",
        "\n",
        "\n",
        "  model_20 = layers.Conv2D(p_conv, kernel_size =(20,factor_num))(input)\n",
        "  model_20 = layers.Activation(\"relu\")(model_20)\n",
        "  model_20 = layers.BatchNormalization()(model_20)\n",
        "  model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "  model_20 = layers.Reshape((-1,p_conv))(model_20)\n",
        "\n",
        "  model_20 = layers.LSTM(p_lstm)(model_20)\n",
        "\n",
        "\n",
        "  model_30 = layers.Conv2D(p_conv, kernel_size =(30,factor_num))(input)\n",
        "  model_30 = layers.Activation(\"relu\")(model_30)\n",
        "  model_30 = layers.BatchNormalization()(model_30)\n",
        "  model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "  model_30 = layers.Reshape((-1,p_conv))(model_30)\n",
        "\n",
        "  model_30 = layers.LSTM(p_lstm)(model_30)\n",
        "\n",
        "  model_45 = layers.Conv2D(p_conv, kernel_size =(45,factor_num))(input)\n",
        "  model_45 = layers.Activation(\"relu\")(model_45)\n",
        "  model_45 = layers.BatchNormalization()(model_45)\n",
        "  model_45 = layers.Dropout(0.5)(model_45)\n",
        "\n",
        "  model_45 = layers.Reshape((-1,p_conv))(model_45)\n",
        "\n",
        "  model_45 = layers.LSTM(p_lstm)(model_45)\n",
        "\n",
        "\n",
        "  model = layers.concatenate([model_1, model_5, model_10,\\\n",
        "              model_15, model_20, model_30, model_45], axis = -1)\n",
        "\n",
        "\n",
        "  model = layers.Dense(2048)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "\n",
        "  model = layers.Dense(1024)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(512)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(256)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(128)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(64)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(32)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(4)(model)\n",
        "  model = layers.Activation(\"softmax\")(model)\n",
        "\n",
        "  model = Model(inputs = input, outputs = model)\n",
        "\n",
        "  # opt = optimizers.RMSprop(lr = 0.0005, decay = 0.001)\n",
        "  opt = optimizers.Adam(lr = 0.0005, decay = 0.01)\n",
        "\n",
        "  model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics = [\"acc\"])\n",
        "\n",
        "  if summary:\n",
        "    model.summary()\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4ngNCbgjo-K"
      },
      "source": [
        "###training 4-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTiHoClHj1QF"
      },
      "source": [
        "batch_size = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaNA166bhTuz"
      },
      "source": [
        "xxx = G2(ts_data, ts_code, market_index_data,\\\n",
        "                    pct_of_stock=0.1, edge=10.0,\\\n",
        "                    shuffle = 0, y_cate = 1,\\\n",
        "                    batch_size=batch_size, generator = 0, repeat_time = 1)\n",
        "print(xxx[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O1xCNTKqati",
        "outputId": "66119cba-de9d-4d9f-ddc3-9176395c0aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "xxx[2].min()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4427645788336933"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FegrRqvTkDTm",
        "outputId": "65f28af5-45a7-4f75-de89-088b2b130513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "chk_point = ModelCheckpoint(filepath='stock_predict_4-1-set.h5')\n",
        "\n",
        "for i in range(8):\n",
        "\n",
        "  print(datetime.datetime.today())\n",
        "  model = make_model()\n",
        "  model.load_weights(\"stock_predict_4-1-set.h5\") \n",
        "\n",
        "  #\n",
        "  tracemalloc.start()\n",
        "\n",
        "  for l in range(2):\n",
        "    history = model.fit(G2(ts_data, ts_code, market_index_data,\\\n",
        "                    pct_of_stock=0.1, edge=10.0,\\\n",
        "                    shuffle = 0, y_cate = 1, batch_size=batch_size, repeat_time = 2),\\\n",
        "                    steps_per_epoch = batch_size,\\\n",
        "                    epochs = 10,\\\n",
        "                    callbacks = [chk_point],\\\n",
        "                    # validation_data = G1(ts_data, ts_code, mark_index_data, pct_of_stock=0.1, edge=10.0,\\\n",
        "                    # shuffle = 0, y_cate = 1, batchsize=batch_size)\n",
        "                    )\n",
        "    # history = model.fit(X_t, y_t, batch_size = 1024, epochs = '20,\\\n",
        "    #             # validation_split = 0.1, shuffle = True, \\\n",
        "    #             class_weight = cls_weight,\\\n",
        "    #             validation_data = (X_v ,y_v),\\\n",
        "    #             # callbacks = [tbCallBack]\\\n",
        "    #             callbacks = [chk_point],\\\n",
        "    #             # initial_epoch = 5 \n",
        "    #             ) \n",
        "\n",
        "    model.save_weights(\"stock_predict_4-1-set.h5\")\n",
        "  model.save_weights(\"stock_predict_4-1-set_a.h5\")\n",
        "\n",
        "  del model\n",
        "  gc.collect()\n",
        "  \n",
        "  #\n",
        "  snapshot = tracemalloc.take_snapshot()\n",
        "  top_stats = snapshot.statistics('lineno')\n",
        "  print(\"[ Top 10 ]\")\n",
        "  for stat in top_stats[:10]:\n",
        "    print(stat)\n",
        "    \n",
        "\n",
        "plt.plot(history.history[\"acc\"]) \n",
        "print(datetime.datetime.today())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-85d4d482bd2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchk_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stock_predict_4-1-set.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ModelCheckpoint' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnRHfPlDi-wv",
        "outputId": "3776b44d-880d-41f0-fed2-dd54932ec4c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "show_mem()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 7.3 GB  | Proc size: 24.0 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyavqb98yMQP",
        "outputId": "8c0ccf48-bfeb-437e-c5b2-dadbc3420ec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(sys.getsizeof(G2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kjyFdwcJ1Br"
      },
      "source": [
        "K.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9IE74qsKF_n",
        "outputId": "42b528e6-7706-44da-defe-6beec34cec6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# del x\n",
        "# del y\n",
        "# del X\n",
        "# del sample_weight\n",
        "# del stk_data\n",
        "# del model\n",
        "del history\n",
        "print(\"collect\",gc.collect())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "collect 296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGans-FU6GvB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wopchxFu6IZo"
      },
      "source": [
        "###training 4-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjX5KhEH6IZr"
      },
      "source": [
        "batch_size = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V4qlWxb6IZy"
      },
      "source": [
        "xxx = G2(ts_data, ts_code, market_index_data,\\\n",
        "                    pct_of_stock=0.1, edge=10.0,\\\n",
        "                    shuffle = 0, y_cate = 1,\\\n",
        "                    batch_size=batch_size, generator = 0, repeat_time = 1)\n",
        "print(xxx[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YohJHw66IZ4",
        "outputId": "66119cba-de9d-4d9f-ddc3-9176395c0aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "xxx[2].min()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4427645788336933"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkDHmsi76IZ-",
        "outputId": "282b6ab3-5dd3-4839-9eb2-477f07e086b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "chk_point = ModelCheckpoint(filepath='stock_predict_4-1-set.h5')\n",
        "csv_logger = CSVLogger(filename='stock_predict_4-1-set.log', separator=',', append=False)\n",
        "running = LambdaCallback(on_train_begin = lambda logs: next(r))\n",
        "\n",
        "print(datetime.datetime.today())\n",
        "model = make_model()\n",
        "model.load_weights(\"stock_predict_4-1-set.h5\") \n",
        "\n",
        "for i in range(8):\n",
        "\n",
        "  # print(datetime.datetime.today())\n",
        "  # model = make_model()\n",
        "  # model.load_weights(\"stock_predict_4-1-set.h5\") \n",
        "\n",
        "  #\n",
        "  tracemalloc.start()\n",
        "\n",
        "  for l in range(2):\n",
        "    X, y, sample_weight = G2_1(ts_data, ts_code, market_index_data,\\\n",
        "                    pct_of_stock=0.1, edge=10.0,\\\n",
        "                    shuffle = 0, y_cate = 1, generator = 0\\\n",
        "                    #  batch_size=batch_size, repeat_time = 2\\\n",
        "                     )\n",
        "    history = model.fit(X, y, sample_weight = sample_weight,\\\n",
        "                    # steps_per_epoch = batch_size,\\\n",
        "                    epochs = 10,\\\n",
        "                    verbose = 2,\\\n",
        "                    callbacks = [chk_point, csv_logger, running],\\\n",
        "                    batch_size = batch_size,\n",
        "                    # validation_data = G1(ts_data, ts_code, mark_index_data, pct_of_stock=0.1, edge=10.0,\\\n",
        "                    # shuffle = 0, y_cate = 1, batchsize=batch_size)\n",
        "                    validation_split = 0.1\n",
        "                    )\n",
        "    # history = model.fit(X_t, y_t, batch_size = 1024, epochs = '20,\\\n",
        "    #             # validation_split = 0.1, shuffle = True, \\\n",
        "    #             class_weight = cls_weight,\\\n",
        "    #             validation_data = (X_v ,y_v),\\\n",
        "    #             # callbacks = [tbCallBack]\\\n",
        "    #             callbacks = [chk_point],\\\n",
        "    #             # initial_epoch = 5 \n",
        "    #             ) \n",
        "\n",
        "    model.save_weights(\"stock_predict_4-1-set.h5\")\n",
        "  model.save_weights(\"stock_predict_4-1-set_a.h5\")\n",
        "\n",
        "  # del model\n",
        "  # gc.collect()\n",
        "  \n",
        "  #\n",
        "  snapshot = tracemalloc.take_snapshot()\n",
        "  top_stats = snapshot.statistics('lineno')\n",
        "  print(\"[ Top 10 ]\")\n",
        "  for stat in top_stats[:10]:\n",
        "    print(stat)\n",
        "    \n",
        "\n",
        "plt.plot(history.history[\"acc\"]) \n",
        "print(datetime.datetime.today())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-03 13:43:34.145935\n",
            "collect 0\n",
            "Gen RAM Free: 25.9 GB  | Proc size: 2.6 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "/352803 2020-10-03 13:49:08.209828\n",
            "58789.0\n",
            "[1.         0.49684344 0.53912605 0.88208198]\n",
            "0\n",
            "-Epoch 1/10\n",
            "1241/1241 - 134s - loss: 0.6351 - acc: 0.5074 - val_loss: 0.6625 - val_acc: 0.4977\n",
            "Epoch 2/10\n",
            "1241/1241 - 129s - loss: 0.6256 - acc: 0.5149 - val_loss: 0.6608 - val_acc: 0.4994\n",
            "Epoch 3/10\n",
            "1241/1241 - 129s - loss: 0.6234 - acc: 0.5185 - val_loss: 0.6616 - val_acc: 0.4994\n",
            "Epoch 4/10\n",
            "1241/1241 - 130s - loss: 0.6226 - acc: 0.5182 - val_loss: 0.6589 - val_acc: 0.5022\n",
            "Epoch 5/10\n",
            "1241/1241 - 129s - loss: 0.6209 - acc: 0.5197 - val_loss: 0.6599 - val_acc: 0.5019\n",
            "Epoch 6/10\n",
            "1241/1241 - 130s - loss: 0.6206 - acc: 0.5203 - val_loss: 0.6605 - val_acc: 0.5008\n",
            "Epoch 7/10\n",
            "1241/1241 - 130s - loss: 0.6201 - acc: 0.5208 - val_loss: 0.6597 - val_acc: 0.5036\n",
            "Epoch 8/10\n",
            "1241/1241 - 130s - loss: 0.6197 - acc: 0.5213 - val_loss: 0.6588 - val_acc: 0.5035\n",
            "Epoch 9/10\n",
            "1241/1241 - 130s - loss: 0.6190 - acc: 0.5220 - val_loss: 0.6584 - val_acc: 0.5037\n",
            "Epoch 10/10\n",
            "1241/1241 - 130s - loss: 0.6188 - acc: 0.5214 - val_loss: 0.6581 - val_acc: 0.5034\n",
            "collect 1605\n",
            "Gen RAM Free: 22.2 GB  | Proc size: 10.9 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "/336062 2020-10-03 14:17:14.240777\n",
            "57059.0\n",
            "[1.         0.51013402 0.5556919  0.88497867]\n",
            "0\n",
            "-Epoch 1/10\n",
            "1182/1182 - 127s - loss: 0.6367 - acc: 0.5209 - val_loss: 0.6744 - val_acc: 0.4903\n",
            "Epoch 2/10\n",
            "1182/1182 - 124s - loss: 0.6348 - acc: 0.5222 - val_loss: 0.6737 - val_acc: 0.4914\n",
            "Epoch 3/10\n",
            "1182/1182 - 123s - loss: 0.6343 - acc: 0.5227 - val_loss: 0.6740 - val_acc: 0.4912\n",
            "Epoch 4/10\n",
            "1182/1182 - 123s - loss: 0.6321 - acc: 0.5252 - val_loss: 0.6734 - val_acc: 0.4915\n",
            "Epoch 5/10\n",
            "1182/1182 - 124s - loss: 0.6326 - acc: 0.5246 - val_loss: 0.6734 - val_acc: 0.4912\n",
            "Epoch 6/10\n",
            "1182/1182 - 123s - loss: 0.6320 - acc: 0.5245 - val_loss: 0.6740 - val_acc: 0.4922\n",
            "Epoch 7/10\n",
            "1182/1182 - 123s - loss: 0.6318 - acc: 0.5256 - val_loss: 0.6730 - val_acc: 0.4930\n",
            "Epoch 8/10\n",
            "1182/1182 - 123s - loss: 0.6313 - acc: 0.5253 - val_loss: 0.6725 - val_acc: 0.4917\n",
            "Epoch 9/10\n",
            "1182/1182 - 123s - loss: 0.6311 - acc: 0.5266 - val_loss: 0.6728 - val_acc: 0.4926\n",
            "Epoch 10/10\n",
            "1182/1182 - 123s - loss: 0.6308 - acc: 0.5264 - val_loss: 0.6728 - val_acc: 0.4920\n",
            "[ Top 10 ]\n",
            "<ipython-input-9-cf33a72c82f4>:117: size=2769 MiB, count=3, average=923 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:98: size=1386 MiB, count=28, average=49.5 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/range.py:164: size=58.0 MiB, count=4, average=14.5 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1983: size=9650 KiB, count=17156, average=576 B\n",
            "<ipython-input-9-cf33a72c82f4>:66: size=9463 KiB, count=220231, average=44 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:382: size=5882 KiB, count=21570, average=279 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/np_utils.py:77: size=5251 KiB, count=2, average=2626 KiB\n",
            "<ipython-input-9-cf33a72c82f4>:132: size=2626 KiB, count=2, average=1313 KiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_stack.py:153: size=2115 KiB, count=35966, average=60 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:5061: size=1727 KiB, count=10526, average=168 B\n",
            "collect 1641\n",
            "Gen RAM Free: 22.3 GB  | Proc size: 13.9 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "/342663 2020-10-03 14:43:38.669660\n",
            "55191.0\n",
            "[1.         0.47383197 0.50914676 0.88165945]\n",
            "0\n",
            "-Epoch 1/10\n",
            "1205/1205 - 129s - loss: 0.6018 - acc: 0.5141 - val_loss: 0.6165 - val_acc: 0.5082\n",
            "Epoch 2/10\n",
            "1205/1205 - 126s - loss: 0.6011 - acc: 0.5148 - val_loss: 0.6179 - val_acc: 0.5059\n",
            "Epoch 3/10\n",
            "1205/1205 - 126s - loss: 0.6000 - acc: 0.5153 - val_loss: 0.6181 - val_acc: 0.5054\n",
            "Epoch 4/10\n",
            "1205/1205 - 125s - loss: 0.5996 - acc: 0.5149 - val_loss: 0.6188 - val_acc: 0.5057\n",
            "Epoch 5/10\n",
            "1205/1205 - 125s - loss: 0.5989 - acc: 0.5157 - val_loss: 0.6191 - val_acc: 0.5049\n",
            "Epoch 6/10\n",
            "1205/1205 - 125s - loss: 0.5986 - acc: 0.5166 - val_loss: 0.6191 - val_acc: 0.5053\n",
            "Epoch 7/10\n",
            "1205/1205 - 125s - loss: 0.5982 - acc: 0.5155 - val_loss: 0.6192 - val_acc: 0.5048\n",
            "Epoch 8/10\n",
            "1205/1205 - 126s - loss: 0.5984 - acc: 0.5169 - val_loss: 0.6193 - val_acc: 0.5044\n",
            "Epoch 9/10\n",
            "1205/1205 - 125s - loss: 0.5976 - acc: 0.5186 - val_loss: 0.6196 - val_acc: 0.5040\n",
            "Epoch 10/10\n",
            "1205/1205 - 125s - loss: 0.5982 - acc: 0.5169 - val_loss: 0.6198 - val_acc: 0.5036\n",
            "collect 1641\n",
            "Gen RAM Free: 25.2 GB  | Proc size: 13.9 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "-332048 2020-10-03 15:10:20.188298\n",
            "54020.0\n",
            "[1.         0.48088307 0.51376182 0.89214051]\n",
            "0\n",
            "\\Epoch 1/10\n",
            "1168/1168 - 125s - loss: 0.6030 - acc: 0.5202 - val_loss: 0.6279 - val_acc: 0.5003\n",
            "Epoch 2/10\n",
            "1168/1168 - 122s - loss: 0.6023 - acc: 0.5206 - val_loss: 0.6275 - val_acc: 0.5003\n",
            "Epoch 3/10\n",
            "1168/1168 - 121s - loss: 0.6015 - acc: 0.5223 - val_loss: 0.6268 - val_acc: 0.5003\n",
            "Epoch 4/10\n",
            "1168/1168 - 122s - loss: 0.6009 - acc: 0.5222 - val_loss: 0.6267 - val_acc: 0.5007\n",
            "Epoch 5/10\n",
            "1168/1168 - 122s - loss: 0.6010 - acc: 0.5220 - val_loss: 0.6264 - val_acc: 0.5008\n",
            "Epoch 6/10\n",
            "1168/1168 - 122s - loss: 0.6005 - acc: 0.5229 - val_loss: 0.6259 - val_acc: 0.5016\n",
            "Epoch 7/10\n",
            "1168/1168 - 122s - loss: 0.6005 - acc: 0.5230 - val_loss: 0.6256 - val_acc: 0.5012\n",
            "Epoch 8/10\n",
            "1168/1168 - 122s - loss: 0.6004 - acc: 0.5235 - val_loss: 0.6257 - val_acc: 0.5010\n",
            "Epoch 9/10\n",
            "1168/1168 - 122s - loss: 0.5998 - acc: 0.5228 - val_loss: 0.6257 - val_acc: 0.5007\n",
            "Epoch 10/10\n",
            "1168/1168 - 121s - loss: 0.5998 - acc: 0.5232 - val_loss: 0.6257 - val_acc: 0.5009\n",
            "[ Top 10 ]\n",
            "<ipython-input-9-cf33a72c82f4>:117: size=2736 MiB, count=3, average=912 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:98: size=1369 MiB, count=28, average=48.9 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/range.py:164: size=58.0 MiB, count=4, average=14.5 MiB\n",
            "<ipython-input-9-cf33a72c82f4>:66: size=10.2 MiB, count=242324, average=44 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1983: size=9650 KiB, count=17156, average=576 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:382: size=5882 KiB, count=21570, average=279 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/np_utils.py:77: size=5188 KiB, count=2, average=2594 KiB\n",
            "<ipython-input-9-cf33a72c82f4>:132: size=2594 KiB, count=2, average=1297 KiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_stack.py:153: size=2115 KiB, count=35966, average=60 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:5061: size=1727 KiB, count=10526, average=168 B\n",
            "collect 1641\n",
            "Gen RAM Free: 25.1 GB  | Proc size: 14.0 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "\\346591 2020-10-03 15:36:27.982647\n",
            "58095.0\n",
            "[1.         0.50464294 0.53760804 0.88943154]\n",
            "0\n",
            "|Epoch 1/10\n",
            "1219/1219 - 130s - loss: 0.6252 - acc: 0.5199 - val_loss: 0.6409 - val_acc: 0.5207\n",
            "Epoch 2/10\n",
            "1219/1219 - 127s - loss: 0.6246 - acc: 0.5204 - val_loss: 0.6412 - val_acc: 0.5201\n",
            "Epoch 3/10\n",
            "1219/1219 - 126s - loss: 0.6249 - acc: 0.5205 - val_loss: 0.6411 - val_acc: 0.5200\n",
            "Epoch 4/10\n",
            "1219/1219 - 127s - loss: 0.6235 - acc: 0.5215 - val_loss: 0.6409 - val_acc: 0.5201\n",
            "Epoch 5/10\n",
            "1219/1219 - 127s - loss: 0.6230 - acc: 0.5211 - val_loss: 0.6411 - val_acc: 0.5201\n",
            "Epoch 6/10\n",
            "1219/1219 - 127s - loss: 0.6232 - acc: 0.5212 - val_loss: 0.6412 - val_acc: 0.5205\n",
            "Epoch 7/10\n",
            "1219/1219 - 127s - loss: 0.6227 - acc: 0.5228 - val_loss: 0.6406 - val_acc: 0.5198\n",
            "Epoch 8/10\n",
            "1219/1219 - 127s - loss: 0.6231 - acc: 0.5220 - val_loss: 0.6406 - val_acc: 0.5199\n",
            "Epoch 9/10\n",
            "1219/1219 - 126s - loss: 0.6226 - acc: 0.5227 - val_loss: 0.6411 - val_acc: 0.5196\n",
            "Epoch 10/10\n",
            "1219/1219 - 127s - loss: 0.6222 - acc: 0.5219 - val_loss: 0.6406 - val_acc: 0.5193\n",
            "collect 1641\n",
            "Gen RAM Free: 22.0 GB  | Proc size: 17.0 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "|358865 2020-10-03 16:03:39.367595\n",
            "60425.0\n",
            "[1.         0.5041172  0.5414184  0.90218884]\n",
            "0\n",
            "/Epoch 1/10\n",
            "1262/1262 - 134s - loss: 0.6263 - acc: 0.5232 - val_loss: 0.6740 - val_acc: 0.4886\n",
            "Epoch 2/10\n",
            "1262/1262 - 130s - loss: 0.6257 - acc: 0.5230 - val_loss: 0.6743 - val_acc: 0.4890\n",
            "Epoch 3/10\n",
            "1262/1262 - 131s - loss: 0.6253 - acc: 0.5245 - val_loss: 0.6741 - val_acc: 0.4894\n",
            "Epoch 4/10\n",
            "1262/1262 - 132s - loss: 0.6252 - acc: 0.5237 - val_loss: 0.6740 - val_acc: 0.4904\n",
            "Epoch 5/10\n",
            "1262/1262 - 132s - loss: 0.6246 - acc: 0.5242 - val_loss: 0.6742 - val_acc: 0.4904\n",
            "Epoch 6/10\n",
            "1262/1262 - 131s - loss: 0.6251 - acc: 0.5242 - val_loss: 0.6744 - val_acc: 0.4899\n",
            "Epoch 7/10\n",
            "1262/1262 - 132s - loss: 0.6251 - acc: 0.5246 - val_loss: 0.6741 - val_acc: 0.4904\n",
            "Epoch 8/10\n",
            "1262/1262 - 132s - loss: 0.6240 - acc: 0.5261 - val_loss: 0.6741 - val_acc: 0.4901\n",
            "Epoch 9/10\n",
            "1262/1262 - 131s - loss: 0.6244 - acc: 0.5249 - val_loss: 0.6743 - val_acc: 0.4906\n",
            "Epoch 10/10\n",
            "1262/1262 - 132s - loss: 0.6244 - acc: 0.5258 - val_loss: 0.6742 - val_acc: 0.4912\n",
            "[ Top 10 ]\n",
            "<ipython-input-9-cf33a72c82f4>:117: size=2957 MiB, count=3, average=986 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:98: size=1480 MiB, count=28, average=52.9 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/range.py:164: size=58.0 MiB, count=4, average=14.5 MiB\n",
            "<ipython-input-9-cf33a72c82f4>:66: size=10.7 MiB, count=255196, average=44 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1983: size=9650 KiB, count=17156, average=576 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:382: size=5882 KiB, count=21570, average=279 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/np_utils.py:77: size=5607 KiB, count=2, average=2804 KiB\n",
            "<ipython-input-9-cf33a72c82f4>:132: size=2804 KiB, count=2, average=1402 KiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_stack.py:153: size=2115 KiB, count=35966, average=60 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:5061: size=1727 KiB, count=10526, average=168 B\n",
            "collect 1641\n",
            "Gen RAM Free: 21.8 GB  | Proc size: 20.0 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "/348912 2020-10-03 16:31:34.543647\n",
            "57327.0\n",
            "[1.         0.49041867 0.52342908 0.87961272]\n",
            "0\n",
            "-Epoch 1/10\n",
            "1227/1227 - 132s - loss: 0.6152 - acc: 0.5158 - val_loss: 0.6256 - val_acc: 0.5217\n",
            "Epoch 2/10\n",
            "1227/1227 - 128s - loss: 0.6150 - acc: 0.5157 - val_loss: 0.6252 - val_acc: 0.5215\n",
            "Epoch 3/10\n",
            "1227/1227 - 128s - loss: 0.6142 - acc: 0.5168 - val_loss: 0.6244 - val_acc: 0.5222\n",
            "Epoch 4/10\n",
            "1227/1227 - 128s - loss: 0.6144 - acc: 0.5167 - val_loss: 0.6239 - val_acc: 0.5226\n",
            "Epoch 5/10\n",
            "1227/1227 - 128s - loss: 0.6138 - acc: 0.5165 - val_loss: 0.6237 - val_acc: 0.5223\n",
            "Epoch 6/10\n",
            "1227/1227 - 128s - loss: 0.6137 - acc: 0.5170 - val_loss: 0.6237 - val_acc: 0.5221\n",
            "Epoch 7/10\n",
            "1227/1227 - 128s - loss: 0.6134 - acc: 0.5174 - val_loss: 0.6228 - val_acc: 0.5226\n",
            "Epoch 8/10\n",
            "1227/1227 - 128s - loss: 0.6137 - acc: 0.5167 - val_loss: 0.6228 - val_acc: 0.5229\n",
            "Epoch 9/10\n",
            "1227/1227 - 128s - loss: 0.6135 - acc: 0.5165 - val_loss: 0.6229 - val_acc: 0.5224\n",
            "Epoch 10/10\n",
            "1227/1227 - 128s - loss: 0.6134 - acc: 0.5165 - val_loss: 0.6229 - val_acc: 0.5225\n",
            "collect 1641\n",
            "Gen RAM Free: 21.8 GB  | Proc size: 22.5 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "/362651 2020-10-03 16:59:14.131332\n",
            "58361.0\n",
            "[1.         0.47510542 0.51056366 0.86912687]\n",
            "0\n",
            "-Epoch 1/10\n",
            "1275/1275 - 136s - loss: 0.5921 - acc: 0.5249 - val_loss: 0.6145 - val_acc: 0.5058\n",
            "Epoch 2/10\n",
            "1275/1275 - 133s - loss: 0.5912 - acc: 0.5250 - val_loss: 0.6145 - val_acc: 0.5045\n",
            "Epoch 3/10\n",
            "1275/1275 - 132s - loss: 0.5920 - acc: 0.5251 - val_loss: 0.6150 - val_acc: 0.5050\n",
            "Epoch 4/10\n",
            "1275/1275 - 132s - loss: 0.5912 - acc: 0.5246 - val_loss: 0.6154 - val_acc: 0.5041\n",
            "Epoch 5/10\n",
            "1275/1275 - 132s - loss: 0.5912 - acc: 0.5253 - val_loss: 0.6158 - val_acc: 0.5040\n",
            "Epoch 6/10\n",
            "1275/1275 - 133s - loss: 0.5905 - acc: 0.5256 - val_loss: 0.6160 - val_acc: 0.5046\n",
            "Epoch 7/10\n",
            "1275/1275 - 133s - loss: 0.5908 - acc: 0.5263 - val_loss: 0.6159 - val_acc: 0.5040\n",
            "Epoch 8/10\n",
            "1275/1275 - 133s - loss: 0.5906 - acc: 0.5261 - val_loss: 0.6161 - val_acc: 0.5041\n",
            "Epoch 9/10\n",
            "1275/1275 - 132s - loss: 0.5900 - acc: 0.5259 - val_loss: 0.6164 - val_acc: 0.5045\n",
            "Epoch 10/10\n",
            "1275/1275 - 132s - loss: 0.5900 - acc: 0.5265 - val_loss: 0.6165 - val_acc: 0.5045\n",
            "[ Top 10 ]\n",
            "<ipython-input-9-cf33a72c82f4>:117: size=2988 MiB, count=3, average=996 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:98: size=1495 MiB, count=28, average=53.4 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/range.py:164: size=58.0 MiB, count=4, average=14.5 MiB\n",
            "<ipython-input-9-cf33a72c82f4>:66: size=10.9 MiB, count=259592, average=44 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1983: size=9650 KiB, count=17156, average=576 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:382: size=5882 KiB, count=21570, average=279 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/np_utils.py:77: size=5666 KiB, count=2, average=2833 KiB\n",
            "<ipython-input-9-cf33a72c82f4>:132: size=2833 KiB, count=2, average=1417 KiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_stack.py:153: size=2115 KiB, count=35966, average=60 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:5061: size=1727 KiB, count=10526, average=168 B\n",
            "collect 1641\n",
            "Gen RAM Free: 21.6 GB  | Proc size: 22.6 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "-343291 2020-10-03 17:27:26.420590\n",
            "56983.0\n",
            "[1.         0.49106343 0.53202932 0.90210078]\n",
            "0\n",
            "|Epoch 1/10\n",
            "1207/1207 - 128s - loss: 0.6131 - acc: 0.5250 - val_loss: 0.6380 - val_acc: 0.5069\n",
            "Epoch 2/10\n",
            "1207/1207 - 125s - loss: 0.6128 - acc: 0.5249 - val_loss: 0.6376 - val_acc: 0.5063\n",
            "Epoch 3/10\n",
            "1207/1207 - 125s - loss: 0.6125 - acc: 0.5248 - val_loss: 0.6370 - val_acc: 0.5077\n",
            "Epoch 4/10\n",
            "1207/1207 - 125s - loss: 0.6118 - acc: 0.5257 - val_loss: 0.6368 - val_acc: 0.5067\n",
            "Epoch 5/10\n",
            "1207/1207 - 125s - loss: 0.6118 - acc: 0.5264 - val_loss: 0.6368 - val_acc: 0.5070\n",
            "Epoch 6/10\n",
            "1207/1207 - 125s - loss: 0.6114 - acc: 0.5259 - val_loss: 0.6370 - val_acc: 0.5076\n",
            "Epoch 7/10\n",
            "1207/1207 - 124s - loss: 0.6118 - acc: 0.5263 - val_loss: 0.6365 - val_acc: 0.5072\n",
            "Epoch 8/10\n",
            "1207/1207 - 126s - loss: 0.6110 - acc: 0.5269 - val_loss: 0.6369 - val_acc: 0.5076\n",
            "Epoch 9/10\n",
            "1207/1207 - 125s - loss: 0.6115 - acc: 0.5266 - val_loss: 0.6365 - val_acc: 0.5073\n",
            "Epoch 10/10\n",
            "1207/1207 - 124s - loss: 0.6110 - acc: 0.5264 - val_loss: 0.6366 - val_acc: 0.5074\n",
            "collect 1641\n",
            "Gen RAM Free: 24.8 GB  | Proc size: 22.6 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "/334815 2020-10-03 17:54:06.405442\n",
            "56315.0\n",
            "[1.         0.50031984 0.5417769  0.9082918 ]\n",
            "0\n",
            "-Epoch 1/10\n",
            "1178/1178 - 125s - loss: 0.6277 - acc: 0.5212 - val_loss: 0.6589 - val_acc: 0.5042\n",
            "Epoch 2/10\n",
            "1178/1178 - 121s - loss: 0.6276 - acc: 0.5218 - val_loss: 0.6586 - val_acc: 0.5041\n",
            "Epoch 3/10\n",
            "1178/1178 - 121s - loss: 0.6267 - acc: 0.5217 - val_loss: 0.6587 - val_acc: 0.5045\n",
            "Epoch 4/10\n",
            "1178/1178 - 122s - loss: 0.6270 - acc: 0.5209 - val_loss: 0.6591 - val_acc: 0.5048\n",
            "Epoch 5/10\n",
            "1178/1178 - 122s - loss: 0.6264 - acc: 0.5213 - val_loss: 0.6592 - val_acc: 0.5046\n",
            "Epoch 6/10\n",
            "1178/1178 - 122s - loss: 0.6261 - acc: 0.5224 - val_loss: 0.6591 - val_acc: 0.5039\n",
            "Epoch 7/10\n",
            "1178/1178 - 122s - loss: 0.6260 - acc: 0.5232 - val_loss: 0.6591 - val_acc: 0.5041\n",
            "Epoch 8/10\n",
            "1178/1178 - 122s - loss: 0.6265 - acc: 0.5221 - val_loss: 0.6593 - val_acc: 0.5035\n",
            "Epoch 9/10\n",
            "1178/1178 - 122s - loss: 0.6260 - acc: 0.5229 - val_loss: 0.6595 - val_acc: 0.5034\n",
            "Epoch 10/10\n",
            "1178/1178 - 123s - loss: 0.6264 - acc: 0.5220 - val_loss: 0.6594 - val_acc: 0.5037\n",
            "[ Top 10 ]\n",
            "<ipython-input-9-cf33a72c82f4>:117: size=2759 MiB, count=3, average=920 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:98: size=1381 MiB, count=28, average=49.3 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/range.py:164: size=58.0 MiB, count=4, average=14.5 MiB\n",
            "<ipython-input-9-cf33a72c82f4>:66: size=11.0 MiB, count=262673, average=44 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1983: size=9650 KiB, count=17156, average=576 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:382: size=5882 KiB, count=21570, average=279 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/np_utils.py:77: size=5232 KiB, count=2, average=2616 KiB\n",
            "<ipython-input-9-cf33a72c82f4>:132: size=2616 KiB, count=2, average=1308 KiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_stack.py:153: size=2115 KiB, count=35966, average=60 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:5061: size=1727 KiB, count=10526, average=168 B\n",
            "collect 1641\n",
            "Gen RAM Free: 20.4 GB  | Proc size: 23.4 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "/334781 2020-10-03 18:20:18.102388\n",
            "55453.0\n",
            "[1.         0.49153925 0.52844591 0.90048879]\n",
            "0\n",
            "-Epoch 1/10\n",
            "1177/1177 - 125s - loss: 0.6116 - acc: 0.5243 - val_loss: 0.6687 - val_acc: 0.4873\n",
            "Epoch 2/10\n",
            "1177/1177 - 121s - loss: 0.6113 - acc: 0.5253 - val_loss: 0.6687 - val_acc: 0.4874\n",
            "Epoch 3/10\n",
            "1177/1177 - 122s - loss: 0.6103 - acc: 0.5246 - val_loss: 0.6684 - val_acc: 0.4881\n",
            "Epoch 4/10\n",
            "1177/1177 - 122s - loss: 0.6100 - acc: 0.5266 - val_loss: 0.6682 - val_acc: 0.4879\n",
            "Epoch 5/10\n",
            "1177/1177 - 122s - loss: 0.6107 - acc: 0.5259 - val_loss: 0.6681 - val_acc: 0.4879\n",
            "Epoch 6/10\n",
            "1177/1177 - 122s - loss: 0.6102 - acc: 0.5255 - val_loss: 0.6681 - val_acc: 0.4882\n",
            "Epoch 7/10\n",
            "1177/1177 - 122s - loss: 0.6100 - acc: 0.5266 - val_loss: 0.6682 - val_acc: 0.4882\n",
            "Epoch 8/10\n",
            "1177/1177 - 122s - loss: 0.6095 - acc: 0.5272 - val_loss: 0.6681 - val_acc: 0.4885\n",
            "Epoch 9/10\n",
            "1177/1177 - 122s - loss: 0.6098 - acc: 0.5271 - val_loss: 0.6679 - val_acc: 0.4888\n",
            "Epoch 10/10\n",
            "1177/1177 - 122s - loss: 0.6099 - acc: 0.5262 - val_loss: 0.6682 - val_acc: 0.4895\n",
            "collect 1641\n",
            "Gen RAM Free: 23.3 GB  | Proc size: 23.3 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "|362052 2020-10-03 18:46:42.369260\n",
            "61815.0\n",
            "[1.         0.50853941 0.55129943 0.92869698]\n",
            "0\n",
            "/Epoch 1/10\n",
            "1273/1273 - 136s - loss: 0.6346 - acc: 0.5232 - val_loss: 0.6326 - val_acc: 0.5408\n",
            "Epoch 2/10\n",
            "1273/1273 - 132s - loss: 0.6338 - acc: 0.5233 - val_loss: 0.6324 - val_acc: 0.5413\n",
            "Epoch 3/10\n",
            "1273/1273 - 132s - loss: 0.6334 - acc: 0.5245 - val_loss: 0.6327 - val_acc: 0.5410\n",
            "Epoch 4/10\n",
            "1273/1273 - 132s - loss: 0.6331 - acc: 0.5240 - val_loss: 0.6326 - val_acc: 0.5416\n",
            "Epoch 5/10\n",
            "1273/1273 - 132s - loss: 0.6329 - acc: 0.5237 - val_loss: 0.6328 - val_acc: 0.5412\n",
            "Epoch 6/10\n",
            "1273/1273 - 132s - loss: 0.6332 - acc: 0.5248 - val_loss: 0.6328 - val_acc: 0.5406\n",
            "Epoch 7/10\n",
            "1273/1273 - 132s - loss: 0.6330 - acc: 0.5249 - val_loss: 0.6326 - val_acc: 0.5420\n",
            "Epoch 8/10\n",
            "1273/1273 - 132s - loss: 0.6331 - acc: 0.5248 - val_loss: 0.6325 - val_acc: 0.5416\n",
            "Epoch 9/10\n",
            "1273/1273 - 131s - loss: 0.6328 - acc: 0.5257 - val_loss: 0.6326 - val_acc: 0.5416\n",
            "Epoch 10/10\n",
            "1273/1273 - 131s - loss: 0.6329 - acc: 0.5250 - val_loss: 0.6330 - val_acc: 0.5414\n",
            "[ Top 10 ]\n",
            "<ipython-input-9-cf33a72c82f4>:117: size=2983 MiB, count=3, average=994 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:98: size=1493 MiB, count=28, average=53.3 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/range.py:164: size=58.0 MiB, count=4, average=14.5 MiB\n",
            "<ipython-input-9-cf33a72c82f4>:66: size=11.1 MiB, count=265682, average=44 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1983: size=9650 KiB, count=17156, average=576 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:382: size=5882 KiB, count=21570, average=279 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/np_utils.py:77: size=5657 KiB, count=2, average=2829 KiB\n",
            "<ipython-input-9-cf33a72c82f4>:132: size=2829 KiB, count=2, average=1414 KiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_stack.py:153: size=2115 KiB, count=35966, average=60 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:5061: size=1727 KiB, count=10526, average=168 B\n",
            "collect 1641\n",
            "Gen RAM Free: 23.1 GB  | Proc size: 22.9 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "-345209 2020-10-03 19:14:37.892137\n",
            "57126.0\n",
            "[1.         0.49121208 0.52754255 0.89956538]\n",
            "0\n",
            "\\Epoch 1/10\n",
            "1214/1214 - 129s - loss: 0.6112 - acc: 0.5239 - val_loss: 0.6453 - val_acc: 0.5029\n",
            "Epoch 2/10\n",
            "1214/1214 - 125s - loss: 0.6109 - acc: 0.5255 - val_loss: 0.6451 - val_acc: 0.5033\n",
            "Epoch 3/10\n",
            "1214/1214 - 126s - loss: 0.6107 - acc: 0.5238 - val_loss: 0.6455 - val_acc: 0.5035\n",
            "Epoch 4/10\n",
            "1214/1214 - 125s - loss: 0.6107 - acc: 0.5252 - val_loss: 0.6452 - val_acc: 0.5038\n",
            "Epoch 5/10\n",
            "1214/1214 - 126s - loss: 0.6099 - acc: 0.5254 - val_loss: 0.6454 - val_acc: 0.5044\n",
            "Epoch 6/10\n",
            "1214/1214 - 126s - loss: 0.6102 - acc: 0.5264 - val_loss: 0.6452 - val_acc: 0.5044\n",
            "Epoch 7/10\n",
            "1214/1214 - 126s - loss: 0.6102 - acc: 0.5258 - val_loss: 0.6450 - val_acc: 0.5038\n",
            "Epoch 8/10\n",
            "1214/1214 - 125s - loss: 0.6098 - acc: 0.5259 - val_loss: 0.6450 - val_acc: 0.5042\n",
            "Epoch 9/10\n",
            "1214/1214 - 125s - loss: 0.6104 - acc: 0.5252 - val_loss: 0.6451 - val_acc: 0.5039\n",
            "Epoch 10/10\n",
            "1214/1214 - 126s - loss: 0.6097 - acc: 0.5256 - val_loss: 0.6450 - val_acc: 0.5041\n",
            "collect 1641\n",
            "Gen RAM Free: 22.2 GB  | Proc size: 21.4 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "|338388 2020-10-03 19:41:27.765419\n",
            "55732.0\n",
            "[1.         0.48540273 0.53262739 0.88172383]\n",
            "0\n",
            "/Epoch 1/10\n",
            "1190/1190 - 126s - loss: 0.6100 - acc: 0.5223 - val_loss: 0.6387 - val_acc: 0.5066\n",
            "Epoch 2/10\n",
            "1190/1190 - 124s - loss: 0.6097 - acc: 0.5218 - val_loss: 0.6384 - val_acc: 0.5066\n",
            "Epoch 3/10\n",
            "1190/1190 - 123s - loss: 0.6100 - acc: 0.5222 - val_loss: 0.6382 - val_acc: 0.5068\n",
            "Epoch 4/10\n",
            "1190/1190 - 124s - loss: 0.6097 - acc: 0.5221 - val_loss: 0.6379 - val_acc: 0.5075\n",
            "Epoch 5/10\n",
            "1190/1190 - 123s - loss: 0.6098 - acc: 0.5211 - val_loss: 0.6384 - val_acc: 0.5069\n",
            "Epoch 6/10\n",
            "1190/1190 - 124s - loss: 0.6096 - acc: 0.5227 - val_loss: 0.6376 - val_acc: 0.5076\n",
            "Epoch 7/10\n",
            "1190/1190 - 124s - loss: 0.6100 - acc: 0.5217 - val_loss: 0.6376 - val_acc: 0.5076\n",
            "Epoch 8/10\n",
            "1190/1190 - 123s - loss: 0.6094 - acc: 0.5229 - val_loss: 0.6378 - val_acc: 0.5071\n",
            "Epoch 9/10\n",
            "1190/1190 - 124s - loss: 0.6088 - acc: 0.5230 - val_loss: 0.6376 - val_acc: 0.5072\n",
            "Epoch 10/10\n",
            "1190/1190 - 123s - loss: 0.6089 - acc: 0.5227 - val_loss: 0.6377 - val_acc: 0.5071\n",
            "[ Top 10 ]\n",
            "<ipython-input-9-cf33a72c82f4>:117: size=2788 MiB, count=3, average=929 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:98: size=1395 MiB, count=28, average=49.8 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/range.py:164: size=58.0 MiB, count=4, average=14.5 MiB\n",
            "<ipython-input-9-cf33a72c82f4>:66: size=11.2 MiB, count=266807, average=44 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1983: size=9650 KiB, count=17156, average=576 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:382: size=5882 KiB, count=21570, average=279 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/np_utils.py:77: size=5287 KiB, count=2, average=2644 KiB\n",
            "<ipython-input-9-cf33a72c82f4>:132: size=2644 KiB, count=2, average=1322 KiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_stack.py:153: size=2115 KiB, count=35966, average=60 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:5061: size=1727 KiB, count=10526, average=168 B\n",
            "collect 1641\n",
            "Gen RAM Free: 23.1 GB  | Proc size: 21.2 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "/347546 2020-10-03 20:08:00.894219\n",
            "57426.0\n",
            "[1.         0.49472333 0.52801147 0.87957971]\n",
            "0\n",
            "-Epoch 1/10\n",
            "1222/1222 - 130s - loss: 0.6152 - acc: 0.5209 - val_loss: 0.6428 - val_acc: 0.5026\n",
            "Epoch 2/10\n",
            "1222/1222 - 127s - loss: 0.6146 - acc: 0.5217 - val_loss: 0.6431 - val_acc: 0.5029\n",
            "Epoch 3/10\n",
            "1222/1222 - 127s - loss: 0.6141 - acc: 0.5218 - val_loss: 0.6431 - val_acc: 0.5021\n",
            "Epoch 4/10\n",
            "1222/1222 - 127s - loss: 0.6145 - acc: 0.5212 - val_loss: 0.6432 - val_acc: 0.5024\n",
            "Epoch 5/10\n",
            "1222/1222 - 127s - loss: 0.6142 - acc: 0.5216 - val_loss: 0.6430 - val_acc: 0.5018\n",
            "Epoch 6/10\n",
            "1222/1222 - 127s - loss: 0.6143 - acc: 0.5213 - val_loss: 0.6429 - val_acc: 0.5021\n",
            "Epoch 7/10\n",
            "1222/1222 - 126s - loss: 0.6139 - acc: 0.5220 - val_loss: 0.6430 - val_acc: 0.5026\n",
            "Epoch 8/10\n",
            "1222/1222 - 127s - loss: 0.6147 - acc: 0.5213 - val_loss: 0.6430 - val_acc: 0.5020\n",
            "Epoch 9/10\n",
            "1222/1222 - 126s - loss: 0.6146 - acc: 0.5215 - val_loss: 0.6431 - val_acc: 0.5023\n",
            "Epoch 10/10\n",
            "1222/1222 - 127s - loss: 0.6141 - acc: 0.5207 - val_loss: 0.6432 - val_acc: 0.5022\n",
            "collect 1641\n",
            "Gen RAM Free: 23.2 GB  | Proc size: 21.3 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "-335388 2020-10-03 20:35:01.855238\n",
            "55992.0\n",
            "[1.         0.50436428 0.53440739 0.88022512]\n",
            "0\n",
            "|Epoch 1/10\n",
            "1180/1180 - 126s - loss: 0.6238 - acc: 0.5214 - val_loss: 0.6404 - val_acc: 0.5164\n",
            "Epoch 2/10\n",
            "1180/1180 - 123s - loss: 0.6230 - acc: 0.5223 - val_loss: 0.6406 - val_acc: 0.5167\n",
            "Epoch 3/10\n",
            "1180/1180 - 123s - loss: 0.6229 - acc: 0.5228 - val_loss: 0.6405 - val_acc: 0.5167\n",
            "Epoch 4/10\n",
            "1180/1180 - 123s - loss: 0.6227 - acc: 0.5214 - val_loss: 0.6404 - val_acc: 0.5167\n",
            "Epoch 5/10\n",
            "1180/1180 - 123s - loss: 0.6225 - acc: 0.5223 - val_loss: 0.6403 - val_acc: 0.5166\n",
            "Epoch 6/10\n",
            "1180/1180 - 122s - loss: 0.6230 - acc: 0.5215 - val_loss: 0.6403 - val_acc: 0.5165\n",
            "Epoch 7/10\n",
            "1180/1180 - 123s - loss: 0.6232 - acc: 0.5210 - val_loss: 0.6404 - val_acc: 0.5164\n",
            "Epoch 8/10\n",
            "1180/1180 - 123s - loss: 0.6223 - acc: 0.5215 - val_loss: 0.6400 - val_acc: 0.5166\n",
            "Epoch 9/10\n",
            "1180/1180 - 122s - loss: 0.6226 - acc: 0.5213 - val_loss: 0.6404 - val_acc: 0.5160\n",
            "Epoch 10/10\n",
            "1180/1180 - 122s - loss: 0.6222 - acc: 0.5217 - val_loss: 0.6399 - val_acc: 0.5168\n",
            "[ Top 10 ]\n",
            "<ipython-input-9-cf33a72c82f4>:117: size=2764 MiB, count=3, average=921 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:98: size=1383 MiB, count=28, average=49.4 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/range.py:164: size=58.0 MiB, count=4, average=14.5 MiB\n",
            "<ipython-input-9-cf33a72c82f4>:66: size=11.2 MiB, count=267780, average=44 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1983: size=9650 KiB, count=17156, average=576 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:382: size=5882 KiB, count=21570, average=279 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/np_utils.py:77: size=5241 KiB, count=2, average=2620 KiB\n",
            "<ipython-input-9-cf33a72c82f4>:132: size=2620 KiB, count=2, average=1310 KiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_stack.py:153: size=2115 KiB, count=35966, average=60 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:5061: size=1727 KiB, count=10526, average=168 B\n",
            "2020-10-03 20:56:05.043981\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y74_fyv6IaH",
        "outputId": "3776b44d-880d-41f0-fed2-dd54932ec4c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "show_mem()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 7.3 GB  | Proc size: 24.0 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3iBNhnu6IaN",
        "outputId": "8c0ccf48-bfeb-437e-c5b2-dadbc3420ec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(sys.getsizeof(G2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0ySLFLn6IaR"
      },
      "source": [
        "K.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMr60z-I6IaV",
        "outputId": "42b528e6-7706-44da-defe-6beec34cec6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# del x\n",
        "# del y\n",
        "# del X\n",
        "# del sample_weight\n",
        "# del stk_data\n",
        "# del model\n",
        "del history\n",
        "print(\"collect\",gc.collect())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "collect 296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxNoc_9giERB"
      },
      "source": [
        "#Model 1: CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p6l2wfbHXZB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPFa4FEWkCFH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7l4RbsFikd4"
      },
      "source": [
        "##model 1-1: CNN + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1lW5hjDiTBy",
        "outputId": "bbc51a7d-70e2-4137-a1af-c16576ee1336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "input = layers.Input(shape = (window_size, factor_num - 2 ,1))\n",
        "\n",
        "model = layers.Conv2D(32, kernel_size = 4)(input)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Reshape((-1, 32))(model)\n",
        "\n",
        "model = layers.Conv1D(32, kernel_size = 5)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Conv1D(64, kernel_size = 5)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Conv1D(256, kernel_size = 5)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Conv1D(512, kernel_size = 5)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.LSTM(64)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(32)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(1)(model)\n",
        "\n",
        "model = Model(inputs = input, outputs = model)\n",
        "\n",
        "opt = optimizers.RMSprop(lr = 0.001, decay = 0.001)\n",
        "\n",
        "model.compile(loss = \"mse\", optimizer = opt)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-965e3ecc17cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor_num\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'K' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x17JDv8Wi12w"
      },
      "source": [
        "###dataset 1-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvM6qL0EiQln"
      },
      "source": [
        "\n",
        "pct_of_stock  = 0.004\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "\n",
        "print(np.sort(index))\n",
        "                      \n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in index:\n",
        "  stk_data = result[result.code == ts_code.iloc[i][\"code\"]]\n",
        "  length = len(stk_data)\n",
        "  if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "    for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "      ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "      if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-1,:-2].max()\n",
        "        min_price = ohlct[:-1,:-2].min()\n",
        "        scale = max_price - min_price\n",
        "        ohlct[:-1,:-2] = ([ohlct[:-1,:-2] - min_price]) /scale\n",
        "\n",
        "        x.extend([np.expand_dims(ohlct[:-predict_next_days,:-2], axis=-1)])\n",
        "\n",
        "        # y.extend([utils.to_categorical(ohlct[-1,-1], num_classes=201)])\n",
        "        ohlct[-1,-1] = (ohlct[-1,-1] + 10.0) / 20.0\n",
        "        y.extend([ohlct[-1,-1]])\n",
        "\n",
        "\n",
        "\n",
        "X = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "dis_count, dis_edge = np.histogram(y, bins = 20)\n",
        "\n",
        "dis_count_max = dis_count.max() * 4\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[(d*100//5).astype(int)]), \\\n",
        "                                0 , y)\n",
        "\n",
        "X = np.repeat(X, time_count, axis = 0)\n",
        "y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "length = X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "print(X.shape)\n",
        "print(X[1])\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUmQNnUyjYPO"
      },
      "source": [
        "###draw data distribution 1-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDQEhrWGjW0p"
      },
      "source": [
        "plt.hist(y, bins = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llpn-tNzjwO3"
      },
      "source": [
        "###training 1-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QIWpgbdj797"
      },
      "source": [
        "# model.load_weights(\"stock_predict_3_1-1.h5\")\n",
        "\n",
        "history = model.fit(X, y, batch_size = 1024, epochs = 240,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "# plt.plot(history.history[\"acc\"])\n",
        "plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(\"stock_predict_3_1-1.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzFRlwZzkAfw"
      },
      "source": [
        "###drawing validation 1-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v9SqPo8kIxb"
      },
      "source": [
        "# model.load_weights(\"stock_predict_3_1-1.h5\")\n",
        "\n",
        "history = model.fit(X, y, batch_size = 1024, epochs = 240,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "# plt.plot(history.history[\"acc\"])\n",
        "plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(\"stock_predict_3_1-1.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLL6GnFjyQW-"
      },
      "source": [
        "##model 1-2: CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCtdPQjVyEzG"
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "input = layers.Input(shape = (window_size, factor_num - 1 ,1))\n",
        "\n",
        "\n",
        "model_1 = layers.Conv2D(32, kernel_size =(1,4))(input)\n",
        "model_1 = layers.Activation(\"relu\")(model_1)\n",
        "model_1 = layers.BatchNormalization()(model_1)\n",
        "model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "model_1 = layers.Reshape((-1, 32))(model_1)\n",
        "\n",
        "# model_1 = layers.Conv1D(32, kernel_size = 5)(model_1)\n",
        "# model_1 = layers.Activation(\"relu\")(model_1)\n",
        "# model_1 = layers.BatchNormalization()(model_1)\n",
        "# model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "# model_1 = layers.Conv1D(64, kernel_size = 5)(model_1)\n",
        "# model_1 = layers.Activation(\"relu\")(model_1)\n",
        "# model_1 = layers.BatchNormalization()(model_1)\n",
        "# model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "# model_1 = layers.Conv1D(128, kernel_size = 5)(model_1)\n",
        "# model_1 = layers.Activation(\"relu\")(model_1)\n",
        "# model_1 = layers.BatchNormalization()(model_1)\n",
        "# model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "\n",
        "model_5 = layers.Conv2D(32, kernel_size =(5,4))(input)\n",
        "model_5 = layers.Activation(\"relu\")(model_5)\n",
        "model_5 = layers.BatchNormalization()(model_5)\n",
        "model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "model_5 = layers.Reshape((-1, 32))(model_5)\n",
        "\n",
        "# model_5 = layers.Conv1D(64, kernel_size = 5)(model_5)\n",
        "# model_5 = layers.Activation(\"relu\")(model_5)\n",
        "# model_5 = layers.BatchNormalization()(model_5)\n",
        "# model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "# model_5 = layers.Conv1D(128, kernel_size = 5)(model_5)\n",
        "# model_5 = layers.Activation(\"relu\")(model_5)\n",
        "# model_5 = layers.BatchNormalization()(model_5)\n",
        "# model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "# model_5 = layers.Conv1D(256, kernel_size = 5)(model_5)\n",
        "# model_5 = layers.Activation(\"relu\")(model_5)\n",
        "# model_5 = layers.BatchNormalization()(model_5)\n",
        "# model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "\n",
        "model_20 = layers.Conv2D(32, kernel_size =(20,4))(input)\n",
        "model_20 = layers.Activation(\"relu\")(model_20)\n",
        "model_20 = layers.BatchNormalization()(model_20)\n",
        "model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "model_20 = layers.Reshape((-1, 32))(model_20)\n",
        "\n",
        "# model_20 = layers.Conv1D(64, kernel_size = 20)(model_20)\n",
        "# model_20 = layers.Activation(\"relu\")(model_20)\n",
        "# model_20 = layers.BatchNormalization()(model_20)\n",
        "# model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "# model_20 = layers.Conv1D(128, kernel_size = 20)(model_20)\n",
        "# model_20 = layers.Activation(\"relu\")(model_20)\n",
        "# model_20 = layers.BatchNormalization()(model_20)\n",
        "# model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "# model_20 = layers.Conv1D(256, kernel_size = 20)(model_20)\n",
        "# model_20 = layers.Activation(\"relu\")(model_20)\n",
        "# model_20 = layers.BatchNormalization()(model_20)\n",
        "# model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "\n",
        "model_30 = layers.Conv2D(32, kernel_size =(30,4))(input)\n",
        "model_30 = layers.Activation(\"relu\")(model_30)\n",
        "model_30 = layers.BatchNormalization()(model_30)\n",
        "model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "model_30 = layers.Reshape((-1, 32))(model_30)\n",
        "\n",
        "# model_30 = layers.Conv1D(64, kernel_size = 30)(model_30)\n",
        "# model_30 = layers.Activation(\"relu\")(model_30)\n",
        "# model_30 = layers.BatchNormalization()(model_30)\n",
        "# model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "# # model_30 = layers.Conv1D(64, kernel_size = 30)(model_30)\n",
        "# # model_30 = layers.Activation(\"relu\")(model_30)\n",
        "# # model_30 = layers.BatchNormalization()(model_30)\n",
        "# # model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "# model_30 = layers.Conv1D(256, kernel_size = 30)(model_30)\n",
        "# model_30 = layers.Activation(\"relu\")(model_30)\n",
        "# model_30 = layers.BatchNormalization()(model_30)\n",
        "# model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "\n",
        "\n",
        "model = layers.concatenate([model_1, model_5, model_20, model_30], axis = -2)\n",
        "\n",
        "model = layers.Flatten()(model)\n",
        "\n",
        "model = layers.Dense(64)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(32)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(1)(model)\n",
        "\n",
        "model = Model(inputs = input, outputs = model)\n",
        "\n",
        "opt = optimizers.RMSprop(lr = 0.001, decay = 0.001)\n",
        "\n",
        "model.compile(loss = \"mse\", optimizer = opt)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHbZzoodbpC"
      },
      "source": [
        "###dataset 1-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAF9GL6Ydnas"
      },
      "source": [
        "pct_of_stock  = 0.004\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "\n",
        "print(np.sort(index))\n",
        "                      \n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in index:\n",
        "  stk_data = result[result.code == ts_code.iloc[i][\"code\"]]\n",
        "  length = len(stk_data)\n",
        "  if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "    for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "      ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "      if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "        min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "        scale = max_price - min_price\n",
        "        ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "        x.append(np.expand_dims(ohlct[:-predict_next_days,:-2], axis=-1))\n",
        "\n",
        "        # y.extend([utils.to_categorical(ohlct[-1,-1], num_classes=201)])\n",
        "        ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "        y.append(ohlct[-predict_next_days,-1])\n",
        "\n",
        "\n",
        "\n",
        "X = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "dis_count, dis_edge = np.histogram(y, bins = 20)\n",
        "\n",
        "dis_count_max = dis_count.max() * 4\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[(d*100//5).astype(int)]), \\\n",
        "                                0 , y)\n",
        "\n",
        "X = np.repeat(X, time_count, axis = 0)\n",
        "y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "length = X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "print(X.shape)\n",
        "print(X[1])\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgdFoC0ZqnOx"
      },
      "source": [
        "###draw data distribution 1-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMVjP76Mqonw"
      },
      "source": [
        "plt.hist(y, bins = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycLBn2g9eJoz"
      },
      "source": [
        "###training 1-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqCxwMbQeMqw"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-2.h5\")\n",
        "\n",
        "history = model.fit(X, y, batch_size = 1024, epochs = 16,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "# plt.plot(history.history[\"acc\"])\n",
        "plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(\"stock_predict_3_1-2.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMaIpUCXdKAd"
      },
      "source": [
        "###drawing validation 1-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N17j1OcleJko"
      },
      "source": [
        "\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = np.random.randint(0, stock_qty)\n",
        "\n",
        "                      \n",
        "x_plot = []\n",
        "y_plot = []\n",
        "yy_plot = []\n",
        "\n",
        "stk_data = result[result.code == ts_code.iloc[index][\"code\"]]\n",
        "length = len(stk_data)\n",
        "if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "  for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "    ohlct = stk_data.iloc[l : l + window_size + predict_next_days] \\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"isST\",\"pctChg\"]].values\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "      min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "      max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "      scale = (max_price - min_price)\n",
        "      ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:-predict_next_days,:-2], axis = -1)])\n",
        "\n",
        "      # ohlct[-1,-1] = (ohlct[-1,-1] + 10.0) / 20.0\n",
        "      y_plot.extend([ohlct[-predict_next_days,-1]])\n",
        "\n",
        "  X_plot = np.array(x_plot)\n",
        "  y_plot = np.array(y_plot)\n",
        "  # yy_plot = np.array(yy_plot)\n",
        "  print(ts_code.iloc[index])    \n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Kbry9q0dOgQ"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-2.h5\")\n",
        "p = model.predict(X_plot).reshape(-1)\n",
        "\n",
        "p = p * 20.0 - 10.0\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot[-100:], color=\"blue\")\n",
        "plt.plot(p[-100:], color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpCX5_PRvWDq"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-2.h5\")\n",
        "model.evaluate(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDVfkKLHxD1a"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-2.h5\")\n",
        "model.evaluate(X_plot, y_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6hsUOdJDczM"
      },
      "source": [
        "##model 1-3: CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5BAsBF6DczY"
      },
      "source": [
        "def make_model(summary = False):\n",
        "  K.clear_session()\n",
        "\n",
        "  input = layers.Input(shape = (window_size, factor_num - 1 ,1))\n",
        "\n",
        "\n",
        "  model_1 = layers.Conv2D(256, kernel_size =(1,4))(input)\n",
        "  model_1 = layers.Activation(\"relu\")(model_1)\n",
        "  model_1 = layers.BatchNormalization()(model_1)\n",
        "  model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "  model_1 = layers.Reshape((-1, 256))(model_1)\n",
        "\n",
        "\n",
        "\n",
        "  model_5 = layers.Conv2D(256, kernel_size =(5,4))(input)\n",
        "  model_5 = layers.Activation(\"relu\")(model_5)\n",
        "  model_5 = layers.BatchNormalization()(model_5)\n",
        "  model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "  model_5 = layers.Reshape((-1, 256))(model_5)\n",
        "\n",
        "\n",
        "\n",
        "  model_20 = layers.Conv2D(256, kernel_size =(20,4))(input)\n",
        "  model_20 = layers.Activation(\"relu\")(model_20)\n",
        "  model_20 = layers.BatchNormalization()(model_20)\n",
        "  model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "  model_20 = layers.Reshape((-1, 256))(model_20)\n",
        "\n",
        "\n",
        "\n",
        "  model_30 = layers.Conv2D(256, kernel_size =(30,4))(input)\n",
        "  model_30 = layers.Activation(\"relu\")(model_30)\n",
        "  model_30 = layers.BatchNormalization()(model_30)\n",
        "  model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "  model_30 = layers.Reshape((-1, 256))(model_30)\n",
        "\n",
        "\n",
        "  model_60 = layers.Conv2D(256, kernel_size =(60,4))(input)\n",
        "  model_60 = layers.Activation(\"relu\")(model_60)\n",
        "  model_60 = layers.BatchNormalization()(model_60)\n",
        "  model_60 = layers.Dropout(0.5)(model_60)\n",
        "\n",
        "  model_60 = layers.Reshape((-1, 256))(model_60)\n",
        "\n",
        "\n",
        "  model_90 = layers.Conv2D(256, kernel_size =(90,4))(input)\n",
        "  model_90 = layers.Activation(\"relu\")(model_90)\n",
        "  model_90 = layers.BatchNormalization()(model_90)\n",
        "  model_90 = layers.Dropout(0.5)(model_90)\n",
        "\n",
        "  model_90 = layers.Reshape((-1, 256))(model_90)\n",
        "\n",
        "\n",
        "\n",
        "  model = layers.concatenate([model_1, model_5, model_20, model_30, model_60, model_90], axis = -2)\n",
        "\n",
        "  model = layers.Flatten()(model)\n",
        "\n",
        "  # model = layers.Dense(64)(model)\n",
        "  # model = layers.Activation(\"relu\")(model)\n",
        "  # model = layers.BatchNormalization()(model)\n",
        "  # model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(32)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "\n",
        "  model = layers.Dense(16)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  # model = layers.Dense(1)(model)\n",
        "\n",
        "  model = layers.Dense(4)(model)\n",
        "  model = layers.Activation(\"softmax\")(model)\n",
        "\n",
        "  model = Model(inputs = input, outputs = model)\n",
        "\n",
        "  # opt = optimizers.RMSprop(lr = 0.001, decay = 0.001)\n",
        "  opt = optimizers.Adam(lr = 0.005, decay = 0.001)\n",
        "\n",
        "  # model.compile(loss = \"mse\", optimizer = opt)\n",
        "  model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics = [\"acc\"])\n",
        "\n",
        "  if(summary == True):\n",
        "    model.summary()\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3AUWCwiDczo"
      },
      "source": [
        "###dataset 1-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0xCITARDczr"
      },
      "source": [
        "pct_of_stock  = 0.004\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "\n",
        "print(np.sort(index))\n",
        "                      \n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in index:\n",
        "  stk_data = result[result.code == ts_code.iloc[i][\"code\"]]\n",
        "  length = len(stk_data)\n",
        "  if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "    for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "      ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "      if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "        min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "        scale = max_price - min_price\n",
        "        ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "        x.append(np.expand_dims(ohlct[:-predict_next_days,:-2], axis=-1))\n",
        "\n",
        "        # y.extend([utils.to_categorical(ohlct[-1,-1], num_classes=201)])\n",
        "        ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "        y.append(ohlct[-predict_next_days,-1])\n",
        "\n",
        "\n",
        "\n",
        "X = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "dis_count, dis_edge = np.histogram(y, bins = 20)\n",
        "\n",
        "dis_count_max = dis_count.max() * 4\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[(d*100//5).astype(int)]), \\\n",
        "                                0 , y)\n",
        "\n",
        "X = np.repeat(X, time_count, axis = 0)\n",
        "y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "length = X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "print(X.shape)\n",
        "print(X[1])\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc1VTJ4WfPtd"
      },
      "source": [
        "#20200824\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtE3stnXDcz1"
      },
      "source": [
        "###draw data distribution 1-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tq0EZ8uDcz3"
      },
      "source": [
        "plt.hist(y, bins = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYp2sAQEDcz7"
      },
      "source": [
        "###training 1-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx9Ps7PODcz9"
      },
      "source": [
        "# model.load_weights(\"stock_predict_3_1-3.h5\")\n",
        "\n",
        "history = model.fit(X, y, batch_size = 256, epochs = 64,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "# plt.plot(history.history[\"acc\"])\n",
        "plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(\"stock_predict_3_1-3.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga5h6zWQUi-J"
      },
      "source": [
        "#reset model params file\n",
        "model = make_model()\n",
        "model.save_weights(\"stock_predict_3_1-3-set.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azBWGp6TwroY"
      },
      "source": [
        "#20200824\n",
        "\n",
        "chk_point = ModelCheckpoint(filepath='stock_predict_3_1-3-set.h5')\n",
        "# model.load_weights(\"stock_predict_3_1-3-set.h5\")\n",
        "\n",
        "\n",
        "for i in range(4):\n",
        "  print(datetime.datetime.today())\n",
        "  model = make_model()\n",
        "  model.load_weights(\"stock_predict_3_1-3-set.h5\")  \n",
        "\n",
        "  for l in range(2):\n",
        "    history = model.fit(X_t, y_t, batch_size = 1024, epochs = 10,\\\n",
        "                # validation_split = 0.1, shuffle = True, \\\n",
        "                class_weight = cls_weight,\\\n",
        "                validation_data = (X_v ,y_v),\\\n",
        "                # callbacks = [tbCallBack]\\\n",
        "                callbacks = [chk_point],\\\n",
        "                # initial_epoch = 5 \n",
        "                ) \n",
        "\n",
        "    model.save_weights(\"stock_predict_3_1-3-set.h5\")\n",
        "\n",
        "  model.save_weights(\"stock_predict_3_1-3-set_a.h5\")\n",
        "  del model\n",
        "  gc.collect()\n",
        "plt.plot(history.history[\"acc\"])\n",
        "print(datetime.datetime.today())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH6D_yy4Dc0K"
      },
      "source": [
        "###drawing validation 1-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGJySR5VDc0L"
      },
      "source": [
        "\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = np.random.randint(0, stock_qty)\n",
        "\n",
        "                      \n",
        "x_plot = []\n",
        "y_plot = []\n",
        "yy_plot = []\n",
        "\n",
        "stk_data = result[result.code == ts_code.iloc[index][\"code\"]]\n",
        "length = len(stk_data)\n",
        "if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "  for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "    ohlct = stk_data.iloc[l : l + window_size + predict_next_days] \\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"isST\",\"pctChg\"]].values\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "      min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "      max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "      scale = (max_price - min_price)\n",
        "      ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:-predict_next_days,:-2], axis = -1)])\n",
        "\n",
        "      # ohlct[-1,-1] = (ohlct[-1,-1] + 10.0) / 20.0\n",
        "      y_plot.extend([ohlct[-predict_next_days,-1]])\n",
        "\n",
        "  X_plot = np.array(x_plot)\n",
        "  y_plot = np.array(y_plot)\n",
        "  # yy_plot = np.array(yy_plot)\n",
        "  print(ts_code.iloc[index])    \n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjSK3PNgDc0S"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-3.h5\")\n",
        "p = model.predict(X_plot).reshape(-1)\n",
        "\n",
        "p = p * 20.0 - 10.0\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot[-100:], color=\"blue\")\n",
        "plt.plot(p[-100:], color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRnEIxZrDc0c"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-3.h5\")\n",
        "model.evaluate(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdLHMEpQDc0g"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-3.h5\")\n",
        "model.evaluate(X_plot, y_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz7H77AjNK9p"
      },
      "source": [
        "##model 1-4: CNN + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0zrUfHFNK9t"
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "input = layers.Input(shape = (window_size, factor_num - 1 ,1))\n",
        "\n",
        "\n",
        "model_1 = layers.Conv2D(256, kernel_size =(1,4))(input)\n",
        "model_1 = layers.Activation(\"relu\")(model_1)\n",
        "model_1 = layers.BatchNormalization()(model_1)\n",
        "model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "model_1 = layers.Reshape((-1,256))(model_1)\n",
        "\n",
        "model_1 = layers.LSTM(256)(model_1)\n",
        "\n",
        "\n",
        "\n",
        "model_5 = layers.Conv2D(256, kernel_size =(5,4))(input)\n",
        "model_5 = layers.Activation(\"relu\")(model_5)\n",
        "model_5 = layers.BatchNormalization()(model_5)\n",
        "model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "model_5 = layers.Reshape((-1,256))(model_5)\n",
        "\n",
        "model_5 = layers.LSTM(256)(model_5)\n",
        "\n",
        "model_10 = layers.Conv2D(256, kernel_size =(5,4))(input)\n",
        "model_10 = layers.Activation(\"relu\")(model_10)\n",
        "model_10 = layers.BatchNormalization()(model_10)\n",
        "model_10 = layers.Dropout(0.5)(model_10)\n",
        "\n",
        "model_10 = layers.Reshape((-1,256))(model_10)\n",
        "\n",
        "model_10 = layers.LSTM(256)(model_10)\n",
        "\n",
        "model_20 = layers.Conv2D(256, kernel_size =(20,4))(input)\n",
        "model_20 = layers.Activation(\"relu\")(model_20)\n",
        "model_20 = layers.BatchNormalization()(model_20)\n",
        "model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "model_20 = layers.Reshape((-1,256))(model_20)\n",
        "\n",
        "model_20 = layers.LSTM(256)(model_20)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_30 = layers.Conv2D(256, kernel_size =(30,4))(input)\n",
        "model_30 = layers.Activation(\"relu\")(model_30)\n",
        "model_30 = layers.BatchNormalization()(model_30)\n",
        "model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "model_30 = layers.Reshape((-1,256))(model_30)\n",
        "\n",
        "model_30 = layers.LSTM(256)(model_30)\n",
        "\n",
        "\n",
        "\n",
        "model = layers.concatenate([model_1, model_5, model_10, model_20, model_30], axis = -1)\n",
        "\n",
        "model = layers.Dense(256)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(128)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(64)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(32)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(1)(model)\n",
        "\n",
        "model = Model(inputs = input, outputs = model)\n",
        "\n",
        "opt = optimizers.RMSprop(lr = 0.001, decay = 0.001)\n",
        "\n",
        "model.compile(loss = \"mse\", optimizer = opt)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ji2nW0xNK9-"
      },
      "source": [
        "###dataset 1-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8tzPppANK-B"
      },
      "source": [
        "pct_of_stock  = 0.01\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "\n",
        "print(np.sort(index))\n",
        "                      \n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in index:\n",
        "  stk_data = result[result.code == ts_code.iloc[i][\"code\"]]\n",
        "  length = len(stk_data)\n",
        "  if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "    for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "      ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "      if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "        min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "        scale = max_price - min_price\n",
        "        ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "        x.append(np.expand_dims(ohlct[:-predict_next_days,:-2], axis=-1))\n",
        "\n",
        "        # y.extend([utils.to_categorical(ohlct[-1,-1], num_classes=201)])\n",
        "        ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "        y.append(ohlct[-predict_next_days,-1])\n",
        "\n",
        "\n",
        "\n",
        "X = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "dis_count, dis_edge = np.histogram(y, bins = 20)\n",
        "\n",
        "dis_count_max = dis_count.max() * 4\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[(d*100//5).astype(int)]), \\\n",
        "                                0 , y)\n",
        "\n",
        "X = np.repeat(X, time_count, axis = 0)\n",
        "y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "length = X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "print(X.shape)\n",
        "print(X[1])\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n5kGdKdNK-L"
      },
      "source": [
        "###draw data distribution 1-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jkxO4iBNK-O"
      },
      "source": [
        "plt.hist(y, bins = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh9nRcdaNK-U"
      },
      "source": [
        "###training 1-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6DkXeRWNK-V"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-4.h5\")\n",
        "\n",
        "history = model.fit(X, y, batch_size = 1024, epochs = 24,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "# plt.plot(history.history[\"acc\"])\n",
        "plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(\"stock_predict_3_1-4.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XitBIfkFNK-d"
      },
      "source": [
        "###drawing validation 1-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMF-aFYHNK-g"
      },
      "source": [
        "\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = np.random.randint(0, stock_qty)\n",
        "\n",
        "                      \n",
        "x_plot = []\n",
        "y_plot = []\n",
        "yy_plot = []\n",
        "\n",
        "stk_data = result[result.code == ts_code.iloc[index][\"code\"]]\n",
        "length = len(stk_data)\n",
        "if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "  for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "    ohlct = stk_data.iloc[l : l + window_size + predict_next_days] \\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"isST\",\"pctChg\"]].values\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "      min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "      max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "      scale = (max_price - min_price)\n",
        "      ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:-predict_next_days,:-2], axis = -1)])\n",
        "\n",
        "      # ohlct[-1,-1] = (ohlct[-1,-1] + 10.0) / 20.0\n",
        "      y_plot.extend([ohlct[-predict_next_days,-1]])\n",
        "\n",
        "  X_plot = np.array(x_plot)\n",
        "  y_plot = np.array(y_plot)\n",
        "  # yy_plot = np.array(yy_plot)\n",
        "  print(ts_code.iloc[index])    \n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMwImG5KNK-m"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-4.h5\")\n",
        "p = model.predict(X_plot).reshape(-1)\n",
        "\n",
        "p = p * 20.0 - 10.0\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot[-100:], color=\"blue\")\n",
        "plt.plot(p[-100:], color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG3zMp0SNK-r"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-4.h5\")\n",
        "model.evaluate(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJUgwzrjNK_J"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-4.h5\")\n",
        "model.evaluate(X_plot, y_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeK-E3zCsBub"
      },
      "source": [
        "##model 1-5: CNN + LSTM with dif params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDadEhEQsBun"
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "input = layers.Input(shape = (window_size, factor_num - 1 ,1))\n",
        "\n",
        "\n",
        "model_1 = layers.Conv2D(64, kernel_size =(1,4))(input)\n",
        "model_1 = layers.Activation(\"relu\")(model_1)\n",
        "model_1 = layers.BatchNormalization()(model_1)\n",
        "model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "model_1 = layers.Reshape((-1,64))(model_1)\n",
        "\n",
        "model_1 = layers.LSTM(256)(model_1)\n",
        "\n",
        "\n",
        "\n",
        "model_5 = layers.Conv2D(128, kernel_size =(5,4))(input)\n",
        "model_5 = layers.Activation(\"relu\")(model_5)\n",
        "model_5 = layers.BatchNormalization()(model_5)\n",
        "model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "model_5 = layers.Reshape((-1,128))(model_5)\n",
        "\n",
        "model_5 = layers.LSTM(256)(model_5)\n",
        "\n",
        "model_10 = layers.Conv2D(256, kernel_size =(5,4))(input)\n",
        "model_10 = layers.Activation(\"relu\")(model_10)\n",
        "model_10 = layers.BatchNormalization()(model_10)\n",
        "model_10 = layers.Dropout(0.5)(model_10)\n",
        "\n",
        "model_10 = layers.Reshape((-1,256))(model_10)\n",
        "\n",
        "model_10 = layers.LSTM(256)(model_10)\n",
        "\n",
        "model_20 = layers.Conv2D(512, kernel_size =(20,4))(input)\n",
        "model_20 = layers.Activation(\"relu\")(model_20)\n",
        "model_20 = layers.BatchNormalization()(model_20)\n",
        "model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "model_20 = layers.Reshape((-1,512))(model_20)\n",
        "\n",
        "model_20 = layers.LSTM(256)(model_20)\n",
        "\n",
        "\n",
        "\n",
        "model_30 = layers.Conv2D(1024, kernel_size =(30,4))(input)\n",
        "model_30 = layers.Activation(\"relu\")(model_30)\n",
        "model_30 = layers.BatchNormalization()(model_30)\n",
        "model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "model_30 = layers.Reshape((-1,1024))(model_30)\n",
        "\n",
        "model_30 = layers.LSTM(256)(model_30)\n",
        "\n",
        "\n",
        "\n",
        "model = layers.concatenate([model_1, model_5, model_10, model_20, model_30], axis = -1)\n",
        "\n",
        "model = layers.Dense(256)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(128)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(64)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(32)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(1)(model)\n",
        "\n",
        "model = Model(inputs = input, outputs = model)\n",
        "\n",
        "opt = optimizers.RMSprop(lr = 0.001, decay = 0.001)\n",
        "\n",
        "model.compile(loss = \"mse\", optimizer = opt)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCpq_WYJsBux"
      },
      "source": [
        "###dataset 1-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vhbfyRhsBu0"
      },
      "source": [
        "pct_of_stock  = 0.01\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "\n",
        "print(np.sort(index))\n",
        "                      \n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in index:\n",
        "  stk_data = result[result.code == ts_code.iloc[i][\"code\"]]\n",
        "  length = len(stk_data)\n",
        "  if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "    for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "      ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "      if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "        min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "        scale = max_price - min_price\n",
        "        ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "        x.append(np.expand_dims(ohlct[:-predict_next_days,:-2], axis=-1))\n",
        "\n",
        "        # y.extend([utils.to_categorical(ohlct[-1,-1], num_classes=201)])\n",
        "        ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "        y.append(ohlct[-predict_next_days,-1])\n",
        "\n",
        "\n",
        "\n",
        "X = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "dis_count, dis_edge = np.histogram(y, bins = 20)\n",
        "\n",
        "dis_count_max = dis_count.max() * 4\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[(d*100//5).astype(int)]), \\\n",
        "                                0 , y)\n",
        "\n",
        "X = np.repeat(X, time_count, axis = 0)\n",
        "y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "length = X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "print(X.shape)\n",
        "print(X[1])\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOpu9aoFsBu-"
      },
      "source": [
        "###draw data distribution 1-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G94R0K9vsBvB"
      },
      "source": [
        "plt.hist(y, bins = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFHZhz3NsBvU"
      },
      "source": [
        "###training 1-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktctvB8lsBvY"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-5.h5\")\n",
        "\n",
        "history = model.fit(X, y, batch_size = 1024, epochs = 24,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "# plt.plot(history.history[\"acc\"])\n",
        "plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(\"stock_predict_3_1-5.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FOqy4bPDjgz"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-5.h5\")\n",
        "\n",
        "history = model.fit(X, y, batch_size = 1024, epochs = 64,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "# plt.plot(history.history[\"acc\"])\n",
        "plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(\"stock_predict_3_1-5.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WUuXH9nsBvj"
      },
      "source": [
        "###drawing validation 1-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82V0HmWFsBvl"
      },
      "source": [
        "\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = np.random.randint(0, stock_qty)\n",
        "\n",
        "                      \n",
        "x_plot = []\n",
        "y_plot = []\n",
        "yy_plot = []\n",
        "\n",
        "stk_data = result[result.code == ts_code.iloc[index][\"code\"]]\n",
        "length = len(stk_data)\n",
        "if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "  for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "    ohlct = stk_data.iloc[l : l + window_size + predict_next_days] \\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"isST\",\"pctChg\"]].values\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "      min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "      max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "      scale = (max_price - min_price)\n",
        "      ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:-predict_next_days,:-2], axis = -1)])\n",
        "\n",
        "      # ohlct[-1,-1] = (ohlct[-1,-1] + 10.0) / 20.0\n",
        "      y_plot.extend([ohlct[-predict_next_days,-1]])\n",
        "\n",
        "  X_plot = np.array(x_plot)\n",
        "  y_plot = np.array(y_plot)\n",
        "  # yy_plot = np.array(yy_plot)\n",
        "  print(ts_code.iloc[index])    \n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U-DxIc_sBvt"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-5.h5\")\n",
        "p = model.predict(X_plot).reshape(-1)\n",
        "\n",
        "p = p * 20.0 - 10.0\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot[-100:], color=\"blue\")\n",
        "plt.plot(p[-100:], color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHAjTtQVsBvx"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-5.h5\")\n",
        "model.evaluate(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZghInDdrsBv0"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-5.h5\")\n",
        "model.evaluate(X_plot, y_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1_k6_jXNQx6"
      },
      "source": [
        "###get data of specific code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t3nvTRA5asI"
      },
      "source": [
        "s_code = \"sh.000016\"\n",
        "\n",
        "s_data_file = s_code + \".csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9aqqD2EfEm0"
      },
      "source": [
        "\n",
        "s_code = \"sh.000016\"\n",
        "\n",
        "s_data_file = s_code + \".csv\"\n",
        "\n",
        "bs.login()\n",
        "\n",
        "rs = bs.query_history_k_data_plus(s_code, \",\".join(data_fields),\\\n",
        "                          start_date = start_date, end_date = end_date,\\\n",
        "                            frequency = \"d\", adjustflag = adjustflag)\n",
        "\n",
        "data_list = []\n",
        "while (rs.error_code == \"0\") and rs.next():\n",
        "    row_data = rs.get_row_data()\n",
        "    if(row_data[6] == \"1\"):         \n",
        "        data_list.append(row_data)\n",
        "s_result = pd.DataFrame(data_list, columns = rs.fields)\n",
        "s_result.to_csv(s_data_file)\n",
        "print(s_result.head())\n",
        "\n",
        "bs.logout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qYIH7kzfsNh"
      },
      "source": [
        "###dataset of specific code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh0IgWV-fHcE"
      },
      "source": [
        "x = []\n",
        "y = []\n",
        "\n",
        "stk_data = pd.read_csv(s_data_file)\n",
        "length = len(stk_data)\n",
        "if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        " for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "  ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "  if np.all(ohlct[:, -2] != 1) and \\\n",
        "     np.all(ohlct[:,-1] > -10.0) and \\\n",
        "      np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "  min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "  scale = max_price - min_price\n",
        "  ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "  x.append(np.expand_dims(ohlct[:-predict_next_days,:-2], axis=-1))\n",
        "\n",
        "  # y.extend([utils.to_categorical(ohlct[-1,-1], num_classes=201)])\n",
        "  ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "  y.append(ohlct[-predict_next_days,-1])\n",
        "\n",
        "\n",
        "s_X = np.array(x)\n",
        "s_y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "dis_count, dis_edge = np.histogram(s_y, bins = 20)\n",
        "\n",
        "dis_count_max = dis_count.max() * 4\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[(d*100//5).astype(int)]), \\\n",
        "                                0 , s_y)\n",
        "\n",
        "s_X = np.repeat(s_X, time_count, axis = 0)\n",
        "s_y = np.repeat(s_y, time_count, axis = 0)\n",
        "\n",
        "length = s_X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "s_X = s_X[index]\n",
        "s_y = s_y[index]\n",
        "\n",
        "index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "s_X = s_X[index]\n",
        "s_y = s_y[index]\n",
        "\n",
        "print(s_X.shape)\n",
        "print(s_X[1])\n",
        "print(s_y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj_SJEQ_f2-h"
      },
      "source": [
        "###training basing on model1-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3fEb2kdf-Hc"
      },
      "source": [
        "# model.load_weights(\"stock_predict_3_1-5.h5\")\n",
        "model.load_weights(s_data_file + \".h5\")\n",
        "\n",
        "history = model.fit(s_X, s_y, batch_size = 128, epochs = 24,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "# plt.plot(history.history[\"acc\"])\n",
        "plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(s_data_file + \".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyRaLilwhXDe"
      },
      "source": [
        "###get more data for specific code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brNM09tUhc3W"
      },
      "source": [
        "bs.login()\n",
        "\n",
        "s_date = \"2018-06-01\"\n",
        "e_date = \"2019-09-30\"\n",
        "\n",
        "rs = bs.query_history_k_data_plus(s_code, \",\".join(data_fields),\\\n",
        "                          start_date = s_date, end_date = e_date,\\\n",
        "                            frequency = \"d\", adjustflag = adjustflag)\n",
        "\n",
        "data_list = []\n",
        "while (rs.error_code == \"0\") and rs.next():\n",
        "    row_data = rs.get_row_data()\n",
        "    if(row_data[6] == \"1\"):         \n",
        "        data_list.append(row_data)\n",
        "p_result = pd.DataFrame(data_list, columns = rs.fields)\n",
        "\n",
        "print(p_result.head())\n",
        "\n",
        "bs.logout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unLFYTKkDXEo"
      },
      "source": [
        "print(p_result.loc[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs-RKeyFgyRP"
      },
      "source": [
        "print(p_result.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51tkVCF26Agg"
      },
      "source": [
        "###predict for specific code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q90PLdFU6eYc"
      },
      "source": [
        "                      \n",
        "x_plot = []\n",
        "y_plot = []\n",
        "\n",
        "stk_data = p_result\n",
        "length = len(stk_data)\n",
        "if ((length - predict_next_days) > window_size):\n",
        "  for l in range(length - window_size - predict_next_days):\n",
        "    ohlct = stk_data.iloc[l : l + window_size + predict_next_days] \\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"isST\",\"pctChg\"]].values\n",
        "    ohlct = ohlct.astype(\"float\")\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "      min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "      max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "      scale = (max_price - min_price)\n",
        "      ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:-predict_next_days,:-2], axis = -1)])\n",
        "\n",
        "      # ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "      y_plot.extend([ohlct[-predict_next_days,-1]])\n",
        "\n",
        "  X_plot = np.array(x_plot)\n",
        "  y_plot = np.array(y_plot)\n",
        "  # yy_plot = np.array(yy_plot)  \n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc8MyjZu6Lgo"
      },
      "source": [
        "model.load_weights(\"sh.000016.csv.h5\")\n",
        "# model.load_weights(\"stock_predict_3_1-5.h5\")\n",
        "\n",
        "p = model.predict(X_plot).reshape(-1)\n",
        "\n",
        "p = p * 20.0 - 10.0\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot, color=\"blue\")\n",
        "plt.plot(p, color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBQKvnNyvv76"
      },
      "source": [
        "##model 1-6: CNN + LSTM with dif params by crossentrpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzcJ4msjvv79"
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "input = layers.Input(shape = (window_size, factor_num - 1 ,1))\n",
        "\n",
        "\n",
        "model_1 = layers.Conv2D(64, kernel_size =(1,4))(input)\n",
        "model_1 = layers.Activation(\"relu\")(model_1)\n",
        "model_1 = layers.BatchNormalization()(model_1)\n",
        "model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "model_1 = layers.Reshape((-1,64))(model_1)\n",
        "\n",
        "model_1 = layers.LSTM(256)(model_1)\n",
        "\n",
        "\n",
        "\n",
        "model_5 = layers.Conv2D(128, kernel_size =(5,4))(input)\n",
        "model_5 = layers.Activation(\"relu\")(model_5)\n",
        "model_5 = layers.BatchNormalization()(model_5)\n",
        "model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "model_5 = layers.Reshape((-1,128))(model_5)\n",
        "\n",
        "model_5 = layers.LSTM(256)(model_5)\n",
        "\n",
        "model_10 = layers.Conv2D(256, kernel_size =(5,4))(input)\n",
        "model_10 = layers.Activation(\"relu\")(model_10)\n",
        "model_10 = layers.BatchNormalization()(model_10)\n",
        "model_10 = layers.Dropout(0.5)(model_10)\n",
        "\n",
        "model_10 = layers.Reshape((-1,256))(model_10)\n",
        "\n",
        "model_10 = layers.LSTM(256)(model_10)\n",
        "\n",
        "model_20 = layers.Conv2D(512, kernel_size =(20,4))(input)\n",
        "model_20 = layers.Activation(\"relu\")(model_20)\n",
        "model_20 = layers.BatchNormalization()(model_20)\n",
        "model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "model_20 = layers.Reshape((-1,512))(model_20)\n",
        "\n",
        "model_20 = layers.LSTM(256)(model_20)\n",
        "\n",
        "\n",
        "\n",
        "model_30 = layers.Conv2D(1024, kernel_size =(30,4))(input)\n",
        "model_30 = layers.Activation(\"relu\")(model_30)\n",
        "model_30 = layers.BatchNormalization()(model_30)\n",
        "model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "model_30 = layers.Reshape((-1,1024))(model_30)\n",
        "\n",
        "model_30 = layers.LSTM(256)(model_30)\n",
        "\n",
        "\n",
        "\n",
        "model = layers.concatenate([model_1, model_5, model_10, model_20, model_30], axis = -1)\n",
        "\n",
        "model = layers.Dense(256)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(128)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(64)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(32)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(4)(model)\n",
        "model = layers.Activation(\"softmax\")(model)\n",
        "\n",
        "model = Model(inputs = input, outputs = model)\n",
        "\n",
        "opt = optimizers.RMSprop(lr = 0.001, decay = 0.001)\n",
        "\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics = [\"acc\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7ibvQRGvv8G"
      },
      "source": [
        "###dataset 1-6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHX8zzMZvv8H"
      },
      "source": [
        "pct_of_stock  = 0.04\n",
        "stock_qty = len(ts_code)\n",
        "edge = 3.0\n",
        "\n",
        "index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "\n",
        "print(np.sort(index))\n",
        "                      \n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in index:\n",
        "  stk_data = result[result.code == ts_code.iloc[i][\"code\"]]\n",
        "  length = len(stk_data)\n",
        "  if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "    for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "      ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "      if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "        min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "        scale = max_price - min_price\n",
        "        ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "        x.append(np.expand_dims(ohlct[:-predict_next_days,:-2], axis=-1))\n",
        "\n",
        "        # y.extend([utils.to_categorical(ohlct[-1,-1], num_classes=201)])\n",
        "        if ohlct[-predict_next_days,-1] <= -edge:\n",
        "          y.append(0)\n",
        "        elif ohlct[-predict_next_days,-1] > -edge and \\\n",
        "          ohlct[-predict_next_days,-1] <= 0:\n",
        "          y.append(1)\n",
        "        elif ohlct[-predict_next_days,-1] > 0 and \\\n",
        "          ohlct[-predict_next_days,-1] <= edge:\n",
        "          y.append(2)\n",
        "        else:\n",
        "          y.append(3)\n",
        "\n",
        "\n",
        "X = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "\n",
        "dis_count, dis_edge = np.histogram(y, bins = 4)\n",
        "\n",
        "dis_count_max = dis_count.max()\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[d.astype(int)]), \\\n",
        "                                0 , y)\n",
        "\n",
        "X = np.repeat(X, time_count, axis = 0)\n",
        "y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "# shuffle dataset\n",
        "\n",
        "length = X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "# index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "y = utils.to_categorical(y)\n",
        "\n",
        "print(X.shape)\n",
        "print(X[1])\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sbAduiZryBx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQeHx4HBvv8O"
      },
      "source": [
        "###draw data distribution 1-6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN1RgSLWvv8R"
      },
      "source": [
        "plt.hist(y, bins = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJUabvmyvv8e"
      },
      "source": [
        "###training 1-6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qahrtZ2yvv8i"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-6.h5\")\n",
        "\n",
        "history = model.fit(X, y, batch_size = 1024, epochs = 64,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "plt.plot(history.history[\"acc\"])\n",
        "# plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(\"stock_predict_3_1-6.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCk527zzvv85"
      },
      "source": [
        "###drawing validation 1-6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRPaKp41vv87"
      },
      "source": [
        "\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = np.random.randint(0, stock_qty)\n",
        "\n",
        "                      \n",
        "x_plot = []\n",
        "y_plot = []\n",
        "yy_plot = []\n",
        "\n",
        "stk_data = result[result.code == ts_code.iloc[index][\"code\"]]\n",
        "length = len(stk_data)\n",
        "if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "  for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "    ohlct = stk_data.iloc[l : l + window_size + predict_next_days] \\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"isST\",\"pctChg\"]].values\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "      min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "      max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "      scale = (max_price - min_price)\n",
        "      ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:-predict_next_days,:-2], axis = -1)])\n",
        "\n",
        "      # ohlct[-1,-1] = (ohlct[-1,-1] + 10.0) / 20.0\n",
        "      if ohlct[-predict_next_days,-1] <= -edge:\n",
        "        y_plot.append(0)\n",
        "      elif ohlct[-predict_next_days,-1] > -edge and \\\n",
        "        ohlct[-predict_next_days,-1] <= 0:\n",
        "        y_plot.append(1)\n",
        "      elif ohlct[-predict_next_days,-1] > 0 and \\\n",
        "        ohlct[-predict_next_days,-1] <= edge:\n",
        "        y_plot.append(2)\n",
        "      else:\n",
        "        y_plot.append(3)\n",
        "\n",
        "\n",
        "  X_plot = np.array(x_plot)\n",
        "  y_plot = np.array(y_plot)\n",
        "\n",
        "  # yy_plot = np.array(yy_plot)\n",
        "  print(ts_code.iloc[index])    \n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWtYheSQvv9F"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-6.h5\")\n",
        "p = model.predict(X_plot)\n",
        "p = np.apply_along_axis(lambda d: d.argmax() + 1, 1, p)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot[-100:], color=\"blue\")\n",
        "plt.plot(p[-100:], color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nenRGirXVbjw"
      },
      "source": [
        "np.apply_along_axis(lambda d: d.argmax(), 1, p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeq1O0RHvv9b"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-6.h5\")\n",
        "model.evaluate(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ouV4-hQvv9o"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-6.h5\")\n",
        "\n",
        "model.evaluate(X_plot, utils.to_categorical(y_plot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKXBcctzvv95"
      },
      "source": [
        "###get data of specific code 1-6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvEuGsM0vv98"
      },
      "source": [
        "s_code = \"sh.000016\"\n",
        "\n",
        "s_data_file = s_code + \"_1-6.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q46lAJ8Jvv-H"
      },
      "source": [
        "\n",
        "s_code = \"sh.000016\"\n",
        "\n",
        "s_data_file = s_code + \"_1-6.csv\"\n",
        "\n",
        "bs.login()\n",
        "\n",
        "rs = bs.query_history_k_data_plus(s_code, \",\".join(data_fields),\\\n",
        "                          start_date = start_date, end_date = end_date,\\\n",
        "                            frequency = \"d\", adjustflag = adjustflag)\n",
        "\n",
        "data_list = []\n",
        "while (rs.error_code == \"0\") and rs.next():\n",
        "    row_data = rs.get_row_data()\n",
        "    if(row_data[6] == \"1\"):         \n",
        "        data_list.append(row_data)\n",
        "s_result = pd.DataFrame(data_list, columns = rs.fields)\n",
        "s_result.to_csv(s_data_file)\n",
        "print(s_result.head())\n",
        "\n",
        "bs.logout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8iDRzhCvv-S"
      },
      "source": [
        "###dataset of specific code 1-6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vdO3VPbvv-W"
      },
      "source": [
        "edge = 3.0\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "stk_data = pd.read_csv(s_data_file)\n",
        "length = len(stk_data)\n",
        "if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        " for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "  ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "  if np.all(ohlct[:, -2] != 1) and \\\n",
        "     np.all(ohlct[:,-1] > -10.0) and \\\n",
        "      np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "  min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "  scale = max_price - min_price\n",
        "  ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "  x.append(np.expand_dims(ohlct[:-predict_next_days,:-2], axis=-1))\n",
        "\n",
        "  # y.extend([utils.to_categorical(ohlct[-1,-1], num_classes=201)])\n",
        "  # ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "  if ohlct[-predict_next_days,-1] <= -edge:\n",
        "    y.append(0)\n",
        "  elif ohlct[-predict_next_days,-1] > -edge and \\\n",
        "    ohlct[-predict_next_days,-1] <= 0:\n",
        "    y.append(1)\n",
        "  elif ohlct[-predict_next_days,-1] > 0 and \\\n",
        "    ohlct[-predict_next_days,-1] <= edge:\n",
        "    y.append(2)\n",
        "  else:\n",
        "    y.append(3)\n",
        "\n",
        "\n",
        "\n",
        "s_X = np.array(x)\n",
        "s_y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "\n",
        "dis_count, dis_edge = np.histogram(y, bins = 4)\n",
        "\n",
        "dis_count_max = dis_count.max()\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[d.astype(int)]), \\\n",
        "                                0 , s_y)\n",
        "\n",
        "s_X = np.repeat(s_X, time_count, axis = 0)\n",
        "s_y = np.repeat(s_y, time_count, axis = 0)\n",
        "\n",
        "\n",
        "#shuffle\n",
        "length = s_X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "s_X = s_X[index]\n",
        "s_y = s_y[index]\n",
        "\n",
        "# index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "s_X = s_X[index]\n",
        "s_y = s_y[index]\n",
        "\n",
        "s_y = utils.to_categorical(s_y)\n",
        "\n",
        "print(s_X.shape)\n",
        "print(s_X[1])\n",
        "print(s_y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKV8H5ywvv-m"
      },
      "source": [
        "###training basing on model1-6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCWut5rVvv-x"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-6.h5\")\n",
        "# model.load_weights(s_data_file + \".h5\")\n",
        "\n",
        "history = model.fit(s_X, s_y, batch_size = 128, epochs = 64,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "plt.plot(history.history[\"acc\"])\n",
        "# plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(s_data_file + \".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TdN1QEEvv-5"
      },
      "source": [
        "###get more data for specific code 1-6\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG-1xYUVvv-7"
      },
      "source": [
        "bs.login()\n",
        "\n",
        "s_date = \"2018-06-01\"\n",
        "e_date = \"2019-11-13\"\n",
        "\n",
        "rs = bs.query_history_k_data_plus(s_code, \",\".join(data_fields),\\\n",
        "                          start_date = s_date, end_date = e_date,\\\n",
        "                            frequency = \"d\", adjustflag = adjustflag)\n",
        "\n",
        "data_list = []\n",
        "while (rs.error_code == \"0\") and rs.next():\n",
        "    row_data = rs.get_row_data()\n",
        "    if(row_data[6] == \"1\"):         \n",
        "        data_list.append(row_data)\n",
        "p_result = pd.DataFrame(data_list, columns = rs.fields)\n",
        "\n",
        "print(p_result.head())\n",
        "\n",
        "bs.logout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW9oeSHVvv_B"
      },
      "source": [
        "print(p_result.loc[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wauhK0Ivvv_Q"
      },
      "source": [
        "print(p_result.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo_Nwseevv_d"
      },
      "source": [
        "###predict for specific code 1-6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDITjuEOvv_f"
      },
      "source": [
        "                      \n",
        "x_plot = []\n",
        "y_plot = []\n",
        "\n",
        "stk_data = p_result\n",
        "length = len(stk_data)\n",
        "if ((length - predict_next_days) > window_size):\n",
        "  for l in range(length - window_size - predict_next_days):\n",
        "    ohlct = stk_data.iloc[l : l + window_size + predict_next_days] \\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"isST\",\"pctChg\"]].values\n",
        "    ohlct = ohlct.astype(\"float\")\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "      min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "      max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "      scale = (max_price - min_price)\n",
        "      ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:-predict_next_days,:-2], axis = -1)])\n",
        "\n",
        "      # ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "      if ohlct[-predict_next_days,-1] <= -edge:\n",
        "        y_plot.append(0)\n",
        "      elif ohlct[-predict_next_days,-1] > -edge and \\\n",
        "        ohlct[-predict_next_days,-1] <= 0:\n",
        "        y_plot.append(1)\n",
        "      elif ohlct[-predict_next_days,-1] > 0 and \\\n",
        "        ohlct[-predict_next_days,-1] <= edge:\n",
        "        y_plot.append(2)\n",
        "      else:\n",
        "        y_plot.append(3)\n",
        "\n",
        "\n",
        "  X_plot = np.array(x_plot)\n",
        "  y_plot = np.array(y_plot)\n",
        "  # yy_plot = np.array(yy_plot)  \n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz4sXwWZvv_n"
      },
      "source": [
        "model.load_weights(s_data_file + \".h5\")\n",
        "# model.load_weights(\"stock_predict_3_1-6.h5\")\n",
        "\n",
        "p = model.predict(X_plot)\n",
        "p = np.apply_along_axis(lambda d: d.argmax() + 1, 1, p)\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot, color=\"blue\")\n",
        "plt.plot(p, color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVaJgFYXjOtL"
      },
      "source": [
        "model.load_weights(s_data_file + \".h5\")\n",
        "model.evaluate(X_plot, utils.to_categorical(y_plot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fWGVoHUha_5"
      },
      "source": [
        "##model 1-7: CNN + LSTM with dif params by crossentrpy for next few days"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRxx0fuVixHa"
      },
      "source": [
        "predict_next_days = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYH4j1hpiNGs"
      },
      "source": [
        "def make_model(summary = False):\n",
        "  K.clear_session()\n",
        "  # tf.keras.backend.clear_session()\n",
        "  p_lstm = 64\n",
        "\n",
        "  input = layers.Input(shape = (window_size, factor_num - 1 ,1))\n",
        "\n",
        "\n",
        "  model_1 = layers.Conv2D(64, kernel_size =(1,4))(input)\n",
        "  model_1 = layers.Activation(\"relu\")(model_1)\n",
        "  model_1 = layers.BatchNormalization()(model_1)\n",
        "  model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "  model_1 = layers.Reshape((-1,64))(model_1)\n",
        "\n",
        "  model_1 = layers.LSTM(p_lstm)(model_1)\n",
        "\n",
        "\n",
        "\n",
        "  model_5 = layers.Conv2D(128, kernel_size =(5,4))(input)\n",
        "  model_5 = layers.Activation(\"relu\")(model_5)\n",
        "  model_5 = layers.BatchNormalization()(model_5)\n",
        "  model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "  model_5 = layers.Reshape((-1,128))(model_5)\n",
        "\n",
        "  model_5 = layers.LSTM(p_lstm)(model_5)\n",
        "\n",
        "  model_10 = layers.Conv2D(256, kernel_size =(10,4))(input)\n",
        "  model_10 = layers.Activation(\"relu\")(model_10)\n",
        "  model_10 = layers.BatchNormalization()(model_10)\n",
        "  model_10 = layers.Dropout(0.5)(model_10)\n",
        "\n",
        "  model_10 = layers.Reshape((-1,256))(model_10)\n",
        "\n",
        "  model_10 = layers.LSTM(p_lstm)(model_10)\n",
        "\n",
        "  model_20 = layers.Conv2D(512, kernel_size =(20,4))(input)\n",
        "  model_20 = layers.Activation(\"relu\")(model_20)\n",
        "  model_20 = layers.BatchNormalization()(model_20)\n",
        "  model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "  model_20 = layers.Reshape((-1,512))(model_20)\n",
        "\n",
        "  model_20 = layers.LSTM(p_lstm)(model_20)\n",
        "\n",
        "\n",
        "\n",
        "  model_30 = layers.Conv2D(1024, kernel_size =(30,4))(input)\n",
        "  model_30 = layers.Activation(\"relu\")(model_30)\n",
        "  model_30 = layers.BatchNormalization()(model_30)\n",
        "  model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "  model_30 = layers.Reshape((-1,1024))(model_30)\n",
        "\n",
        "  model_30 = layers.LSTM(p_lstm)(model_30)\n",
        "\n",
        "\n",
        "\n",
        "  model = layers.concatenate([model_1, model_5, model_10, model_20, model_30], axis = -1)\n",
        "\n",
        "\n",
        "  model = layers.Dense(2048)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "\n",
        "  model = layers.Dense(1024)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(512)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(256)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(128)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(64)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(32)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(4)(model)\n",
        "  model = layers.Activation(\"softmax\")(model)\n",
        "\n",
        "  model = Model(inputs = input, outputs = model)\n",
        "\n",
        "  # opt = optimizers.RMSprop(lr = 0.0005, decay = 0.001)\n",
        "  opt = optimizers.Adam(lr = 0.0005, decay = 0.001)\n",
        "\n",
        "  model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics = [\"acc\"])\n",
        "\n",
        "  if summary:\n",
        "    model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "model = make_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9ukse_8iNG1"
      },
      "source": [
        "###dataset 1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY25BWpPiNG3"
      },
      "source": [
        "pct_of_stock  = 0.2\n",
        "stock_qty = len(ts_code)\n",
        "edge = 10.0\n",
        "\n",
        "index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "\n",
        "print(np.sort(index))\n",
        "                      \n",
        "x = []\n",
        "y = []\n",
        "\n",
        "count = 0\n",
        "\n",
        "for i in index:\n",
        "  count = count + 1\n",
        "  if count%200 == 0:\n",
        "    print(count)\n",
        "  stk_data = result[result.code == ts_code.iloc[i][\"code\"]]\n",
        "  length = len(stk_data)\n",
        "  if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "    for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "      ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\"]].values\n",
        "      if np.all(ohlct[:,-2] != 1) and \\\n",
        "        np.all(ohlct[:,-3] != 0) and \\\n",
        "       np.all(ohlct[:,-1] >= -10.0) and \\\n",
        "       np.all(ohlct[:,-1] <= 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-3].max()\n",
        "        min_price = ohlct[:-predict_next_days,:-3].min()\n",
        "        scale = max_price - min_price\n",
        "        ohlct[:-predict_next_days,:-3] = ([ohlct[:-predict_next_days,:-3] - min_price]) /scale\n",
        "\n",
        "        x.append(np.expand_dims(ohlct[:-predict_next_days,:-3], axis=-1))\n",
        "\n",
        "        next_days_pctChg = ohlct[-predict_next_days:,-1].sum()\n",
        "\n",
        "        if next_days_pctChg  <= -edge:\n",
        "          y.append(0)\n",
        "        elif next_days_pctChg  > -edge and \\\n",
        "          next_days_pctChg  <= 0:\n",
        "          y.append(1)\n",
        "        elif next_days_pctChg  > 0 and \\\n",
        "          next_days_pctChg  <= edge:\n",
        "          y.append(2)\n",
        "        else:\n",
        "          y.append(3)\n",
        "\n",
        "\n",
        "X = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "\n",
        "dis_count, dis_edge = np.histogram(y, bins = 4)\n",
        "\n",
        "dis_count_max = dis_count.max()\n",
        "\n",
        "dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "dis_count_max = dis_count_max + 1.0 #because dis_count plus 1 above, \n",
        "                    #here plus 1 to avoid time below to be zero\n",
        "\n",
        "time = dis_count_max/dis_count\n",
        "time = time.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: time[d.astype(int)]), \\\n",
        "                                0 , y)\n",
        "\n",
        "X = np.repeat(X, time_count, axis = 0)\n",
        "y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "# shuffle dataset\n",
        "\n",
        "length = X.shape[0]\n",
        "\n",
        "r_index = np.arange(length)\n",
        "np.random.shuffle(r_index)\n",
        "\n",
        "X = X[r_index]\n",
        "y = y[r_index]\n",
        "\n",
        "# r_index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "# X = X[r_index]\n",
        "# y = y[r_index]\n",
        "\n",
        "y = utils.to_categorical(y)\n",
        "\n",
        "print(X.shape)\n",
        "print(X[1])\n",
        "print(y[0])\n",
        "\n",
        "# X.tofile(\"stock_predict_3_1-7-X.dat\", format=\"%f\")\n",
        "# y.tofile(\"stock_predict_3_1-7-y.dat\", format=\"%f\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEVK7MxFVYRG"
      },
      "source": [
        "yy = y.argmax(axis = 1)\n",
        "\n",
        "if np.any(yy == 2): \n",
        "  print(\"ok\")\n",
        "\n",
        "# y[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqH6I3rXiNHC"
      },
      "source": [
        "###draw data distribution 1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqKw1LJRiNHF"
      },
      "source": [
        "plt.hist(y.argmax(axis = 1), bins = 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtW_rDmOiNHJ"
      },
      "source": [
        "###training 1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iROmT0CUFj0-"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL9fSeR9EDbp"
      },
      "source": [
        "X = np.fromfile(\"stock_predict_3_1-7-X.dat\", dtype=np.float).reshape(26654, 90, 4, 1)\n",
        "y = np.fromfile(\"stock_predict_3_1-7-y.dat\", dtype=np.float32).reshape(26654, 4)\n",
        "print(X[100])\n",
        "print(y[180])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L_YQrb0iNHJ"
      },
      "source": [
        "\n",
        "\n",
        "model.load_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "for t in range(20):\n",
        "  history = model.fit(X, y, batch_size = 1024, epochs = 4,\\\n",
        "              validation_split = 0.1, shuffle = True, \\\n",
        "              # callbacks = [tbCallBack]\\\n",
        "              )\n",
        "\n",
        "  model.save_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "plt.plot(history.history[\"acc\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-zJ_Mpr6Jcr"
      },
      "source": [
        "###training by generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsLfRnUL6R8S"
      },
      "source": [
        "####define a generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJGGjQ6C6aYP"
      },
      "source": [
        "def gntor(ts_data, ts_code, pct_of_stock=0.05, edge=10.0, batch_size = 512, generator = True):  \n",
        "\n",
        "  stock_qty = len(ts_code)\n",
        "  # index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "\n",
        "  while True :\n",
        "\n",
        "    index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))                   \n",
        "\n",
        "    # x = []\n",
        "    # y = []\n",
        "\n",
        "    for i in index:\n",
        "    # for i in range(stock_qty):\n",
        "\n",
        "      x = []\n",
        "      y = []\n",
        "\n",
        "      stk_data = ts_data[ts_data.code == ts_code.iloc[i][\"code\"]]\n",
        "      length = len(stk_data)\n",
        "      if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "        for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "          ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "              [[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\"]].values.astype(\"float\")\n",
        "          if np.all(ohlct[:,-2] == 0) and \\\n",
        "            np.all(ohlct[:,-3] == 1) and \\\n",
        "          np.all(ohlct[:,-1] >= -10.0) and \\\n",
        "          np.all(ohlct[:,-1] <= 10.0) :\n",
        "            max_price = ohlct[:-predict_next_days,:-3].max()\n",
        "            min_price = ohlct[:-predict_next_days,:-3].min()\n",
        "            scale = max_price - min_price\n",
        "            ohlct[:-predict_next_days,:-3] = ([ohlct[:-predict_next_days,:-3] - min_price]) /scale\n",
        "\n",
        "            x.append(np.expand_dims(ohlct[:-predict_next_days,:-3], axis=-1))\n",
        "\n",
        "            next_days_pctChg = (ohlct[-1,-4] - ohlct[-predict_next_days,-4]) * 100 / ohlct[-predict_next_days,-4]\n",
        "\n",
        "            if next_days_pctChg  <= -edge:\n",
        "              y.append(0)\n",
        "            elif next_days_pctChg  > -edge and \\\n",
        "              next_days_pctChg  <= 0:\n",
        "              y.append(1)\n",
        "            elif next_days_pctChg  > 0 and \\\n",
        "              next_days_pctChg  <= edge:\n",
        "              y.append(2)\n",
        "            else:\n",
        "              y.append(3)\n",
        "    # print(\"there\")\n",
        "    if len(x) > 0:\n",
        "      # print(\"here\")\n",
        "      X = np.array(x)\n",
        "      y = np.array(y)\n",
        "\n",
        "      # upsample\n",
        "      # dis = district\n",
        "\n",
        "      dis_count, dis_edge = np.histogram(y, bins = 4, range=(0,3))\n",
        "\n",
        "      dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "      dis_count_max = dis_count.max()\n",
        "\n",
        "      time = dis_count_max/dis_count\n",
        "      time = time.astype(int)\n",
        "\n",
        "      time_count = np.apply_along_axis((lambda d: time[d.astype(int)]), \\\n",
        "                                      0 , y)\n",
        "\n",
        "      X = np.repeat(X, time_count, axis = 0)\n",
        "      y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "      # shuffle dataset\n",
        "\n",
        "      length = X.shape[0]\n",
        "\n",
        "      r_index = np.arange(length)\n",
        "      np.random.shuffle(r_index)\n",
        "\n",
        "      X = X[r_index]\n",
        "      y = y[r_index]\n",
        "\n",
        "      # r_index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "      # X = X[r_index]\n",
        "      # y = y[r_index]\n",
        "\n",
        "      y = utils.to_categorical(y, num_classes=4)\n",
        "      if generator:\n",
        "        print(\"a\")\n",
        "        # for ll in range((length-1)//batch_size + 1):\n",
        "        #   start_i = ll*batch_size\n",
        "        #   end_i = start_i + batch_size\n",
        "        #   if end_i <= length:\n",
        "        #     yield(X[start_i:end_i],y[start_i:end_i])\n",
        "        #   else:\n",
        "        #     yield(X[start_i:],y[start_i:])\n",
        "      else:\n",
        "        return 1,2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1qdym0V2iqc"
      },
      "source": [
        "i = random.randint(0,len(ts_code))\n",
        "print(i)\n",
        "stk_data = result[result.code == ts_code.iloc[i][\"code\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEESjjS12jor"
      },
      "source": [
        "####define a generator 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwMnoq0X2jot"
      },
      "source": [
        "def gntor_1(ts_data, ts_code, edge=10.0):\n",
        "  stock_qty = len(ts_code)\n",
        "\n",
        "  # print(np.sort(index))                        \n",
        "\n",
        "\n",
        "  while True:\n",
        "    i = random.randint(0,stock_qty-1)\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    # if count%1 == 0:\n",
        "    #   print(count)\n",
        "    stk_data = ts_data[ts_data.code == ts_code.iloc[i][\"code\"]]\n",
        "    length = len(stk_data)\n",
        "    if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "      for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "        ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "            [[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\"]].values\n",
        "        if np.all(ohlct[:,-2] != 1) and \\\n",
        "          np.all(ohlct[:,-3] != 0) and \\\n",
        "        np.all(ohlct[:,-1] >= -10.0) and \\\n",
        "        np.all(ohlct[:,-1] <= 10.0) :\n",
        "          max_price = ohlct[:-predict_next_days,:-3].max()\n",
        "          min_price = ohlct[:-predict_next_days,:-3].min()\n",
        "          scale = max_price - min_price\n",
        "          ohlct[:-predict_next_days,:-3] = ([ohlct[:-predict_next_days,:-3] - min_price]) /scale\n",
        "\n",
        "          x.append(np.expand_dims(ohlct[:-predict_next_days,:-3], axis=-1))\n",
        "\n",
        "          next_days_pctChg = (ohlct[-1,-4] - ohlct[-predict_next_days,-4]) * 100 / ohlct[-predict_next_days,-4]\n",
        "\n",
        "          if next_days_pctChg  <= -edge:\n",
        "            y.append(0)\n",
        "          elif next_days_pctChg  > -edge and \\\n",
        "            next_days_pctChg  <= 0:\n",
        "            y.append(1)\n",
        "          elif next_days_pctChg  > 0 and \\\n",
        "            next_days_pctChg  <= edge:\n",
        "            y.append(2)\n",
        "          else:\n",
        "            y.append(3)\n",
        "\n",
        "      if len(x) > 0:\n",
        "        X = np.array(x)\n",
        "        y = np.array(y)\n",
        "\n",
        "        # upsample\n",
        "        # dis = district\n",
        "\n",
        "        dis_count, dis_edge = np.histogram(y, bins = 4)\n",
        "\n",
        "        dis_count_max = dis_count.max()\n",
        "\n",
        "        dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "        dis_count_max = dis_count_max + 1.0 #because dis_count plus 1 above, \n",
        "                            #here plus 1 to avoid time below to be zero\n",
        "\n",
        "        time = dis_count_max/dis_count\n",
        "        time = time.astype(int)\n",
        "\n",
        "        time_count = np.apply_along_axis((lambda d: time[d.astype(int)]), \\\n",
        "                                        0 , y)\n",
        "\n",
        "        X = np.repeat(X, time_count, axis = 0)\n",
        "        y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "        # shuffle dataset\n",
        "\n",
        "        length = X.shape[0]\n",
        "\n",
        "        r_index = np.arange(length)\n",
        "        np.random.shuffle(r_index)\n",
        "\n",
        "        X = X[r_index]\n",
        "        y = y[r_index]\n",
        "\n",
        "        # r_index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "        # X = X[r_index]\n",
        "        # y = y[r_index]\n",
        "\n",
        "        y = utils.to_categorical(y, num_classes=4)\n",
        "\n",
        "        batch_size = 512\n",
        "        for ll in range((length-1)//batch_size + 1):\n",
        "          start_i = ll*batch_size\n",
        "          end_i = start_i + batch_size\n",
        "          if end_i <= length:\n",
        "            yield(X[start_i:end_i],y[start_i:end_i])\n",
        "          else:\n",
        "            yield(X[start_i:],y[start_i:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP_3K0IJ8zH4"
      },
      "source": [
        "for i in range(10000):\n",
        "  rr = gntor_1(result, ts_code, edge)\n",
        "  if i%1000 ==0:\n",
        "    print(i)\n",
        "    for ll in rr:\n",
        "      print(ll[0][0,0], ll[1][0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXwWmBX8-cNS"
      },
      "source": [
        "\n",
        "\n",
        "####training by generator 1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwEnxvBo-j-D"
      },
      "source": [
        "\n",
        "pct_of_stock  = 0.005\n",
        "stock_qty = len(ts_code)\n",
        "edge = 10.0\n",
        "\n",
        "step_num = int(stock_qty*pct_of_stock)\n",
        "\n",
        "model.load_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "for i in range(20):\n",
        "  history = model.fit_generator(gntor(result, ts_code, pct_of_stock, edge, batch_size = 512), steps_per_epoch=step_num, epochs=1)\n",
        "  model.save_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "plt.plot(history.history[\"acc\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTFLPfni28mC"
      },
      "source": [
        "edge = 10.0\n",
        "\n",
        "model.load_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "for i in range(20):\n",
        "  history = model.fit_generator(gntor_1(result, ts_code, edge), steps_per_epoch=50000, epochs=1)\n",
        "  model.save_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "plt.plot(history.history[\"acc\"])\n",
        "# plt.plot(history.history[\"loss\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmOyk5_t-Wk6"
      },
      "source": [
        "####define a generator like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-WdEIOh-cZ5"
      },
      "source": [
        "def gntor_like(ts_data, ts_code, pct_of_stock=0.05, edge=10.0):  \n",
        "\n",
        "  stock_qty = len(ts_code)\n",
        "\n",
        "  while True:\n",
        "    index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "    print(len(index))                   \n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in index:\n",
        "    # for i in range(stock_qty):\n",
        "\n",
        "      # x = []\n",
        "      # y = []\n",
        "\n",
        "      stk_data = ts_data[ts_data.code == ts_code.iloc[i][\"code\"]]\n",
        "      length = len(stk_data)\n",
        "      if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "        for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "          ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "              [[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\"]].values.astype(\"float\")\n",
        "          if np.all(ohlct[:,-2] != 1) and \\\n",
        "            np.all(ohlct[:,-3] != 0) and \\\n",
        "          np.all(ohlct[:,-1] >= -10.0) and \\\n",
        "          np.all(ohlct[:,-1] <= 10.0) :\n",
        "            max_price = ohlct[:-predict_next_days,:-3].max()\n",
        "            min_price = ohlct[:-predict_next_days,:-3].min()\n",
        "            scale = max_price - min_price\n",
        "            ohlct[:-predict_next_days,:-3] = ([ohlct[:-predict_next_days,:-3] - min_price]) /scale\n",
        "\n",
        "            x.append(np.expand_dims(ohlct[:-predict_next_days,:-3], axis=-1))\n",
        "\n",
        "            next_days_pctChg = ohlct[-predict_next_days:,-1].sum()\n",
        "\n",
        "            if next_days_pctChg  <= -edge:\n",
        "              y.append(0)\n",
        "            elif next_days_pctChg  > -edge and \\\n",
        "              next_days_pctChg  <= 0:\n",
        "              y.append(1)\n",
        "            elif next_days_pctChg  > 0 and \\\n",
        "              next_days_pctChg  <= edge:\n",
        "              y.append(2)\n",
        "            else:\n",
        "              y.append(3)\n",
        "    # print(\"there\")\n",
        "    if len(x) > 0:\n",
        "      # print(\"here\")\n",
        "      X = np.array(x)\n",
        "      y = np.array(y)\n",
        "\n",
        "      # upsample\n",
        "      # dis = district\n",
        "\n",
        "      dis_count, dis_edge = np.histogram(y, bins = 4, range=(0,3))\n",
        "\n",
        "      dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "      dis_count_max = dis_count.max()\n",
        "\n",
        "      time = dis_count_max/dis_count\n",
        "      time = time.astype(int)\n",
        "\n",
        "      time_count = np.apply_along_axis((lambda d: time[d.astype(int)]), \\\n",
        "                                      0 , y)\n",
        "\n",
        "      X = np.repeat(X, time_count, axis = 0)\n",
        "      y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "      # shuffle dataset\n",
        "\n",
        "      # length = X.shape[0]\n",
        "\n",
        "      # r_index = np.arange(length)\n",
        "      # np.random.shuffle(r_index)\n",
        "\n",
        "      # X = X[r_index]\n",
        "      # y = y[r_index]\n",
        "\n",
        "      # r_index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "      # X = X[r_index]\n",
        "      # y = y[r_index]\n",
        "\n",
        "      y = utils.to_categorical(y, num_classes=4)\n",
        "      return X, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9yUCTn8-cZ-"
      },
      "source": [
        "X, y = gntor_like(result, ts_code, pct_of_stock, edge)\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1VCwWp0M3Ez"
      },
      "source": [
        "####define a generator like using class_weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRcAlNbuMmcH"
      },
      "source": [
        "def gntor_like_cls_w(ts_data, ts_code, pct_of_stock=0.05, edge=10.0,\\\n",
        "                     index=[], shuffle = 0, y_cate = 1):  \n",
        "\n",
        "  stock_qty = len(ts_code)\n",
        "\n",
        "  while True:\n",
        "    if len(index) == 0:\n",
        "      index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "    \n",
        "    print(len(index))                   \n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in index:\n",
        "    # for i in range(stock_qty):\n",
        "\n",
        "      # x = []\n",
        "      # y = []\n",
        "\n",
        "      stk_data = ts_data[ts_data.code == ts_code.iloc[i][\"code\"]]\n",
        "      length = len(stk_data)\n",
        "      if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "        for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "          ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "              [[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\"]].values.astype(\"float\")\n",
        "          if np.all(ohlct[:,-2] != 1) and \\\n",
        "            np.all(ohlct[:,-3] != 0) and \\\n",
        "          np.all(ohlct[:,-1] >= -10.1) and \\\n",
        "          np.all(ohlct[:,-1] <= 10.1) :\n",
        "            max_price = ohlct[:-predict_next_days,:-3].max()\n",
        "            min_price = ohlct[:-predict_next_days,:-3].min()\n",
        "            scale = max_price - min_price\n",
        "            ohlct[:-predict_next_days,:-3] = ([ohlct[:-predict_next_days,:-3] - min_price]) /scale\n",
        "\n",
        "            x.append(np.expand_dims(ohlct[:-predict_next_days,:-3], axis=-1))\n",
        "\n",
        "            # next_days_pctChg = ohlct[-predict_next_days:,-1].sum()  #wrong\n",
        "            next_days_pctChg = (ohlct[-1,-4] - ohlct[-predict_next_days,-4]) * 100 / ohlct[-predict_next_days,-4]\n",
        "\n",
        "            if next_days_pctChg  <= -edge:\n",
        "              y.append(0)\n",
        "            elif next_days_pctChg  <= 0:\n",
        "              y.append(1)\n",
        "            elif next_days_pctChg <= edge:\n",
        "              y.append(2)\n",
        "            else:\n",
        "              y.append(3)\n",
        "    # print(\"there\")\n",
        "    if len(x) > 0:\n",
        "      # print(\"here\")\n",
        "      X = np.array(x)\n",
        "      y = np.array(y)\n",
        "\n",
        "      # upsample\n",
        "      # dis = district\n",
        "\n",
        "      dis_count, dis_edge = np.histogram(y, bins = 4, range=(0,3))\n",
        "\n",
        "      dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "      dis_count_max = dis_count.max()\n",
        "\n",
        "      time = dis_count_max/dis_count\n",
        "\n",
        "      # time = time.astype(int)\n",
        "\n",
        "      # time_count = np.apply_along_axis((lambda d: time[d.astype(int)]), \\\n",
        "      #                                 0 , y)\n",
        "\n",
        "      # X = np.repeat(X, time_count, axis = 0)\n",
        "      # y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "      # shuffle dataset\n",
        "      if shuffle:\n",
        "        length = X.shape[0]\n",
        "\n",
        "        r_index = np.arange(length)\n",
        "        np.random.shuffle(r_index)\n",
        "\n",
        "        X = X[r_index]\n",
        "        y = y[r_index]\n",
        "\n",
        "      # r_index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "      # X = X[r_index]\n",
        "      # y = y[r_index]\n",
        "\n",
        "      if y_cate:\n",
        "        y = utils.to_categorical(y, num_classes=4)\n",
        "      return X, y, time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgEChfSCBMIA"
      },
      "source": [
        "####trainning by generaor like 1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJLhOgd6BdSL"
      },
      "source": [
        "pct_of_stock  = 0.1\n",
        "stock_qty = len(ts_code)\n",
        "edge = 10.0\n",
        "\n",
        "model.load_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "for i in range(10):\n",
        "  X, y = gntor_like(result, ts_code, pct_of_stock, edge)\n",
        "  \n",
        "  print(datetime.datetime.today())\n",
        "\n",
        "  for l in range(10):\n",
        "    history = model.fit(X, y, batch_size = 1024, epochs = 10,\\\n",
        "                validation_split = 0.1, shuffle = True, \\\n",
        "                # callbacks = [tbCallBack]\\\n",
        "                ) \n",
        "\n",
        "    model.save_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "plt.plot(history.history[\"acc\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "incI8NtA8Lp9"
      },
      "source": [
        "t1, t2 = gntor(result, ts_code, pct_of_stock, edge, batch_size = 512, generator = False)\n",
        "print(t1, t2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3ZxQVErSrQO"
      },
      "source": [
        "####trainning by generator like 1-7 using class_weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xwu_B3SS_04"
      },
      "source": [
        "pct_of_stock  = 0.05\n",
        "stock_qty = len(ts_code)\n",
        "edge = 10.0\n",
        "\n",
        "model.load_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "X, y, cls_weight = gntor_like_cls_w(result, ts_code, pct_of_stock, edge)\n",
        "\n",
        "for i in range(5):\n",
        "  \n",
        "  print(datetime.datetime.today())\n",
        "\n",
        "  for l in range(5):\n",
        "    history = model.fit(X, y, batch_size = 1024, epochs = 40,\\\n",
        "                validation_split = 0.1, shuffle = True, \\\n",
        "                class_weight = cls_weight,\\\n",
        "                # callbacks = [tbCallBack]\\\n",
        "                ) \n",
        "\n",
        "    model.save_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "plt.plot(history.history[\"acc\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f15-DeZjU8ha"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIZeDUd339Cy"
      },
      "source": [
        "####trainning by generator like 1-7 using class_weight using specific set of ts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgvgNx7j4MjX"
      },
      "source": [
        "ts_list = [\"sh.600004\", \"sh.600352\", \"sh.601588\", \"sh.601111\", \"sz.000898\",\\\n",
        "       \"sh.601088\", \"sh.600368\", \"sz.002178\", \"sz.000761\", \"sh.600596\",\\\n",
        "       \"sz.300110\", \"sz.300308\", \"sz.000333\", \"sh.600588\", \"sh.600036\",\\\n",
        "       \"sz.002215\", \"sz.002353\", \"sz.002695\", \"sz.002422\", \"sz.000651\",\\\n",
        "       \"sz.002178\", \"sz.002023\", \"sz.300146\", \"sz.002001\", \"sz.300110\",\\\n",
        "       \"sz.000830\", \"sz.000157\"\n",
        "        ]\n",
        "\n",
        "ts_code = ts_code[~ts_code[\"code\"].isin(ts_list)]        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UWUQuQw4VKg"
      },
      "source": [
        "ts_code = ts_code[~ts_code[\"code\"].isin(ts_list)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSa7K-wB8KPP"
      },
      "source": [
        "ts_code.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uuZd5EhXsRz"
      },
      "source": [
        "####dataset preparing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrqizQgqK-Li"
      },
      "source": [
        "#vaildation set\n",
        "\n",
        "ts_set = pd.DataFrame(ts_list, columns = [\"code\"])\n",
        "\n",
        "pct_of_stock  = 1\n",
        "stock_qty = len(ts_code)\n",
        "edge = 10.0\n",
        "\n",
        "X, y, cls_weight = gntor_like_cls_w(result, ts_set, pct_of_stock, edge, [], 1, 0)\n",
        "\n",
        "np.save(\"X-3-1-7-w.npy\", X)\n",
        "np.save(\"y-3-1-7-w.npy\", y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6TFl7ZJDNki"
      },
      "source": [
        "#training set\n",
        "\n",
        "# pct_of_stock = 0.1\n",
        "pct_of_stock = 1\n",
        "edge = 10.0\n",
        "\n",
        "X1, y1, cls_weight = gntor_like_cls_w(result, ts_code[:1000], pct_of_stock, edge, [], 1, 0)\n",
        "# np.save(\"X-3-1-7-w1.npy\", X1)\n",
        "# np.save(\"y-3-1-7-w1.npy\", y1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWSjr0sPVqus"
      },
      "source": [
        "pct_of_stock = 1\n",
        "edge = 10.0\n",
        "\n",
        "X1, y1, cls_weight = gntor_like_cls_w(result, ts_code[:1000], pct_of_stock, edge, [], 1, 0)\n",
        "\n",
        "np.save(\"X-3-1-7-w1000.npy\", X1)\n",
        "np.save(\"y-3-1-7-w1000.npy\", y1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNPqO_57Vy7x"
      },
      "source": [
        "pct_of_stock = 1\n",
        "edge = 10.0\n",
        "\n",
        "X1, y1, cls_weight = gntor_like_cls_w(result, ts_code[1000:2000], pct_of_stock, edge, [], 1, 0)\n",
        "\n",
        "np.save(\"X-3-1-7-w2000.npy\", X1)\n",
        "np.save(\"y-3-1-7-w2000.npy\", y1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk5E0ngDV6vg"
      },
      "source": [
        "pct_of_stock = 1\n",
        "edge = 10.0\n",
        "\n",
        "X1, y1, cls_weight = gntor_like_cls_w(result, ts_code[2000:3000], pct_of_stock, edge, [], 1, 0)\n",
        "\n",
        "np.save(\"X-3-1-7-w3000.npy\", X1)\n",
        "np.save(\"y-3-1-7-w3000.npy\", y1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhUD9mHcWAIG"
      },
      "source": [
        "pct_of_stock = 1\n",
        "edge = 10.0\n",
        "\n",
        "X1, y1, cls_weight = gntor_like_cls_w(result, ts_code[3000:4000], pct_of_stock, edge, [], 1, 0)\n",
        "\n",
        "np.save(\"X-3-1-7-w4000.npy\", X1)\n",
        "np.save(\"y-3-1-7-w4000.npy\", y1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJUpdOI4X-fy"
      },
      "source": [
        "####data set load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zT8Ovsz1ceJ"
      },
      "source": [
        "print(datetime.datetime.today())\n",
        "\n",
        "pct_of_stock = 0.1\n",
        "edge = 10.0\n",
        "v_split_pct = 0.2\n",
        "\n",
        "X = np.load(\"X-3-1-7-w.npy\")\n",
        "y = np.load(\"y-3-1-7-w.npy\")\n",
        "v_split = int(len(X) * (1-0.2))\n",
        "X_t = X[:v_split]\n",
        "X_v = X[v_split:]\n",
        "y_t = y[:v_split]\n",
        "y_v = y[v_split:]\n",
        "\n",
        "X1 = np.load(\"X-3-1-7-w1.npy\")\n",
        "y1 = np.load(\"y-3-1-7-w1.npy\")\n",
        "\n",
        "X_t = np.concatenate((X_t, X1), axis = 0)\n",
        "y_t = np.concatenate((y_t, y1), axis = 0)\n",
        "\n",
        "dis_count, dis_edge = np.histogram(y_t, bins = 4, range=(0,3))\n",
        "\n",
        "dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "dis_count_max = dis_count.max()\n",
        "\n",
        "cls_weight = dis_count_max/dis_count\n",
        "# cls_weight = cls_weight / cls_weight.sum()\n",
        "\n",
        "cls_weight = dict(zip([0,1,2,3],cls_weight))\n",
        "\n",
        "y_t = utils.to_categorical(y_t, num_classes=4)\n",
        "y_v = utils.to_categorical(y_v, num_classes=4)\n",
        "\n",
        "\n",
        "\n",
        "print(datetime.datetime.today())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqElYGyXFvvm"
      },
      "source": [
        "print(datetime.datetime.today())\n",
        "\n",
        "# pct_of_stock = 0.1\n",
        "# edge = 10.0\n",
        "v_split_pct = 0.2\n",
        "\n",
        "X1000 = np.load(\"X-3-1-7-w1000.npy\")\n",
        "y1000 = np.load(\"y-3-1-7-w1000.npy\")\n",
        "X2000 = np.load(\"X-3-1-7-w2000.npy\")  \n",
        "y2000 = np.load(\"y-3-1-7-w2000.npy\")\n",
        "# X3000 = np.load(\"X-3-1-7-w3000.npy\")\n",
        "# y3000 = np.load(\"y-3-1-7-w3000.npy\")\n",
        "# X4000 = np.load(\"X-3-1-7-w4000.npy\")\n",
        "# y4000 = np.load(\"y-3-1-7-w4000.npy\")\n",
        "\n",
        "\n",
        "X = np.load(\"X-3-1-7-w.npy\")\n",
        "y = np.load(\"y-3-1-7-w.npy\")\n",
        "v_split = int(len(X) * (1-v_split_pct))\n",
        "X_t = X[:v_split]\n",
        "X_v = X[v_split:]\n",
        "y_t = y[:v_split]\n",
        "y_v = y[v_split:]\n",
        "\n",
        "\n",
        "X_t = np.concatenate((X_t, \n",
        "                      X1000, \n",
        "                      X2000, \n",
        "                      # X3000, \n",
        "                      # X4000\n",
        "                      ), axis = 0)\n",
        "y_t = np.concatenate((y_t, \n",
        "                      y1000, \n",
        "                      y2000, \n",
        "                      # y3000, \n",
        "                      # y4000\n",
        "                      ), axis = 0)\n",
        "\n",
        "dis_count, dis_edge = np.histogram(y_t, bins = 4, range=(0,3))\n",
        "\n",
        "dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "dis_count_max = dis_count.max()\n",
        "\n",
        "cls_weight = dis_count_max/dis_count\n",
        "# cls_weight = cls_weight / cls_weight.sum()\n",
        "\n",
        "cls_weight = dict(zip([0,1,2,3],cls_weight))\n",
        "\n",
        "y_t = utils.to_categorical(y_t, num_classes=4)\n",
        "y_v = utils.to_categorical(y_v, num_classes=4)\n",
        "\n",
        "\n",
        "\n",
        "print(datetime.datetime.today())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RHZZTa2sdib"
      },
      "source": [
        "print(y_t.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwGP-23dUoKJ"
      },
      "source": [
        "# cls_weight = cls_weight / cls_weight.sum()\n",
        "print(cls_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dZBRMm3-oPK"
      },
      "source": [
        "cls_weight = dict(zip([0,1,2,3],cls_weight))\n",
        "print(cls_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUNnRWjlHKE9"
      },
      "source": [
        "####training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEB7VSOu39C4"
      },
      "source": [
        "\n",
        "\n",
        "chk_point = ModelCheckpoint(filepath='stock_predict_3_1-7-set.h5')\n",
        "# model.load_weights(\"stock_predict_3_1-7-set.h5\")\n",
        "\n",
        "for i in range(8):\n",
        "\n",
        "  print(datetime.datetime.today())\n",
        "  model = make_model()\n",
        "  model.load_weights(\"stock_predict_3_1-7-set.h5\")\n",
        "  for l in range(2):\n",
        "    history = model.fit(X_t, y_t, batch_size = 1024, epochs = 20,\\\n",
        "                # validation_split = 0.1, shuffle = True, \\\n",
        "                class_weight = cls_weight,\\\n",
        "                validation_data = (X_v ,y_v),\\\n",
        "                # callbacks = [tbCallBack]\\\n",
        "                callbacks = [chk_point],\\\n",
        "                # initial_epoch = 5 \n",
        "                ) \n",
        "\n",
        "    model.save_weights(\"stock_predict_3_1-7-set.h5\")\n",
        "\n",
        "  model.save_weights(\"stock_predict_3_1-7-set_a.h5\")\n",
        "  del model\n",
        "  gc.collect()\n",
        "plt.plot(history.history[\"acc\"]) \n",
        "print(datetime.datetime.today())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPT5JRyR39DH"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwNOB7RxxA5e"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnKqpXfOoUOU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9b2gvH491q5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQi1AwMo_LoC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN8mbQlliNHU"
      },
      "source": [
        "###drawing validation 1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIwL0cmpiNHW"
      },
      "source": [
        "\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = np.random.randint(0, stock_qty)\n",
        "\n",
        "                      \n",
        "x_plot = []\n",
        "y_plot = []\n",
        "yy_plot = []\n",
        "\n",
        "stk_data = result[result.code == ts_code.iloc[index][\"code\"]]\n",
        "length = len(stk_data)\n",
        "if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "  for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "    ohlct = stk_data.iloc[l : l + window_size + predict_next_days] \\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"isST\",\"pctChg\"]].values\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "      min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "      max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "      scale = (max_price - min_price)\n",
        "      ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:-predict_next_days,:-2], axis = -1)])\n",
        "\n",
        "\n",
        "      next_days_pctChg = ohlct[-predict_next_days:,-1].sum()\n",
        "\n",
        "      if next_days_pctChg  <= -edge:\n",
        "        y_plot.append(0)\n",
        "      elif next_days_pctChg  > -edge and \\\n",
        "        next_days_pctChg  <= 0:\n",
        "        y_plot.append(1)\n",
        "      elif next_days_pctChg  > 0 and \\\n",
        "        next_days_pctChg  <= edge:\n",
        "        y_plot.append(2)\n",
        "      else:\n",
        "        y_plot.append(3)\n",
        "\n",
        "\n",
        "\n",
        "  X_plot = np.array(x_plot)\n",
        "  y_plot = np.array(y_plot)\n",
        "\n",
        "  # yy_plot = np.array(yy_plot)\n",
        "  print(ts_code.iloc[index])    \n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvs-gaKkiNHf"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-7.h5\")\n",
        "p = model.predict(X_plot)\n",
        "p = np.apply_along_axis(lambda d: d.argmax() + 1, 1, p)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot[-100:], color=\"blue\")\n",
        "plt.plot(p[-100:], color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNFADremiNHi"
      },
      "source": [
        "np.apply_along_axis(lambda d: d.argmax(), 1, p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytjj8iN5iNHk"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-6.h5\")\n",
        "model.evaluate(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-2eEeYniNHo"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-6.h5\")\n",
        "\n",
        "model.evaluate(X_plot, utils.to_categorical(y_plot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szBLpsH5iNHt"
      },
      "source": [
        "###get data of specific code 1-7\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZErhAkvHiNHt"
      },
      "source": [
        "s_code = \"sh.600004\"\n",
        "\n",
        "s_data_file = s_code + \"_1-7.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB9ICGkaiNHw"
      },
      "source": [
        "\n",
        "s_code = \"sh.600004\"\n",
        "\n",
        "s_data_file = s_code + \"_1-7.csv\"\n",
        "\n",
        "bs.login()\n",
        "\n",
        "rs = bs.query_history_k_data_plus(s_code, \",\".join(data_fields),\\\n",
        "                          start_date = start_date, end_date = end_date,\\\n",
        "                            frequency = \"d\", adjustflag = adjustflag)\n",
        "\n",
        "data_list = []\n",
        "while (rs.error_code == \"0\") and rs.next():\n",
        "    row_data = rs.get_row_data()\n",
        "    if(row_data[6] == \"1\"):         \n",
        "        data_list.append(row_data)\n",
        "s_result = pd.DataFrame(data_list, columns = rs.fields)\n",
        "s_result.to_csv(s_data_file)\n",
        "print(s_result.head())\n",
        "\n",
        "bs.logout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASbuU208iNHz"
      },
      "source": [
        "###dataset of specific code 1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9MrDiHYiNH1"
      },
      "source": [
        "edge = 10.0\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "stk_data = pd.read_csv(s_data_file)\n",
        "length = len(stk_data)\n",
        "if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        " for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "  ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "  if np.all(ohlct[:, -2] != 1) and \\\n",
        "     np.all(ohlct[:,-1] > -10.0) and \\\n",
        "      np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "  min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "  scale = max_price - min_price\n",
        "  ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "  x.append(np.expand_dims(ohlct[:-predict_next_days,:-2], axis=-1))\n",
        "\n",
        "  # y.extend([utils.to_categorical(ohlct[-1,-1], num_classes=201)])\n",
        "  # ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "  next_days_pctChg = ohlct[-predict_next_days:,-1].sum()\n",
        "\n",
        "  if next_days_pctChg  <= -edge:\n",
        "    y.append(0)\n",
        "  elif next_days_pctChg  > -edge and \\\n",
        "    next_days_pctChg  <= 0:\n",
        "    y.append(1)\n",
        "  elif next_days_pctChg  > 0 and \\\n",
        "    next_days_pctChg  <= edge:\n",
        "    y.append(2)\n",
        "  else:\n",
        "    y.append(3)\n",
        "\n",
        "\n",
        "\n",
        "s_X = np.array(x)\n",
        "s_y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "\n",
        "dis_count, dis_edge = np.histogram(y, bins = 4)\n",
        "\n",
        "dis_count_max = dis_count.max()\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[d.astype(int)]), \\\n",
        "                                0 , s_y)\n",
        "\n",
        "s_X = np.repeat(s_X, time_count, axis = 0)\n",
        "s_y = np.repeat(s_y, time_count, axis = 0)\n",
        "\n",
        "\n",
        "#shuffle\n",
        "length = s_X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "s_X = s_X[index]\n",
        "s_y = s_y[index]\n",
        "\n",
        "# index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "s_X = s_X[index]\n",
        "s_y = s_y[index]\n",
        "\n",
        "s_y = utils.to_categorical(s_y)\n",
        "\n",
        "print(s_X.shape)\n",
        "print(s_X[1])\n",
        "print(s_y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsEa0VvSiNH5"
      },
      "source": [
        "###training basing on model1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENAkiw8GiNH6"
      },
      "source": [
        "# model.load_weights(\"stock_predict_3_1-7.h5\")\n",
        "model.load_weights(s_data_file + \".h5\")\n",
        "\n",
        "history = model.fit(s_X, s_y, batch_size = 128, epochs = 64,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "plt.plot(history.history[\"acc\"])\n",
        "# plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(s_data_file + \".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYmf-CfEiNIB"
      },
      "source": [
        "###get more data for specific code 1-7\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1ioEmf6iNIC"
      },
      "source": [
        "s_code = \"sz.002064\"\n",
        "\n",
        "bs.login()\n",
        "\n",
        "s_date = \"2019-07-01\"\n",
        "e_date = \"2020-09-11\"\n",
        "\n",
        "rs = bs.query_history_k_data_plus(s_code, \",\".join(data_fields),\\\n",
        "                          start_date = s_date, end_date = e_date,\\\n",
        "                            frequency = \"d\", adjustflag = adjustflag)\n",
        "\n",
        "data_list = []\n",
        "while (rs.error_code == \"0\") and rs.next():\n",
        "    row_data = rs.get_row_data()\n",
        "    if(row_data[6] == \"1\"):         \n",
        "        data_list.append(row_data)\n",
        "p_result = pd.DataFrame(data_list, columns = rs.fields)\n",
        "\n",
        "print(p_result.head())\n",
        "print(p_result.tail())\n",
        "\n",
        "bs.logout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIeRKqoMiNIE"
      },
      "source": [
        "print(p_result.loc[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHnZibUYiNIJ"
      },
      "source": [
        "print(p_result.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp1z1px9Qi6w"
      },
      "source": [
        "print(p_result[190:210]['close'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkaEWpKUOPkF"
      },
      "source": [
        "plt.figure(figsize=(12,4.8))\n",
        "plt.plot(p_result['close'].values.astype(\"float\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEzSEPJOfO-m"
      },
      "source": [
        "plt.figure(figsize=(12,4.8))\n",
        "plt.plot(p_result['pctChg'].values.astype(\"float\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhGFJpl_iNIN"
      },
      "source": [
        "###predict for specific code 1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhLSAQ7CiNIO"
      },
      "source": [
        "edge = 10.0\n",
        "                      \n",
        "x_plot = []\n",
        "y_plot = []\n",
        "\n",
        "stk_data = p_result\n",
        "length = len(stk_data)\n",
        "if ((length - predict_next_days) > window_size):\n",
        "  for l in range(length - window_size - predict_next_days):\n",
        "\n",
        "    ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "    [[\"open\", \"high\", \"low\", \"close\", \"tradestatus\",\\\n",
        "      \"isST\", \"pctChg\"]].values.astype(\"float\")\n",
        "    if np.all(ohlct[:,-2] != 1) and \\\n",
        "      np.all(ohlct[:,-3] != 0) and \\\n",
        "    np.all(ohlct[:,-1] >= -10.1) and \\\n",
        "    np.all(ohlct[:,-1] <= 10.1) :\n",
        "      max_price = ohlct[:-predict_next_days,:-3].max()\n",
        "      min_price = ohlct[:-predict_next_days,:-3].min()\n",
        "      scale = max_price - min_price\n",
        "      ohlct[:-predict_next_days,:-3] = ([ohlct[:-predict_next_days,:-3] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:-predict_next_days,:-3], axis = -1)])\n",
        "\n",
        "      # next_days_pctChg = ohlct[-predict_next_days:,-1].sum()\n",
        "      next_days_pctChg = (ohlct[-1,-4] - ohlct[-predict_next_days,-4]) * 100 / ohlct[-predict_next_days,-4]\n",
        "\n",
        "      if next_days_pctChg  <= -edge:\n",
        "        y_plot.append(0)\n",
        "      elif next_days_pctChg <= 0:\n",
        "        y_plot.append(1)\n",
        "      elif next_days_pctChg <= edge:\n",
        "        y_plot.append(2)\n",
        "      else:\n",
        "        y_plot.append(3)\n",
        "\n",
        "\n",
        "  X_plot = np.array(x_plot)\n",
        "  y_plot = np.array(y_plot)\n",
        "  # yy_plot = np.array(yy_plot)  \n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owqZQ4p7ICY1"
      },
      "source": [
        "y_plot.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyYHaVJdiNIR"
      },
      "source": [
        "# model.load_weights(s_data_file + \".h5\")\n",
        "# model.load_weights(\"stock_predict_3_1-6.h5\")\n",
        "model.load_weights(\"stock_predict_3_1-7-set.h5\")\n",
        "\n",
        "p = model.predict(X_plot)\n",
        "p = np.apply_along_axis(lambda d: d.argmax() + 1, 1, p)\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot, color=\"blue\")\n",
        "plt.plot(p, color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruLtDQWal0p9"
      },
      "source": [
        "model.evaluate(X_plot[-180:], utils.to_categorical(y_plot[-180:], num_classes=4) )\n",
        "# model.evaluate(X_plot, utils.to_categorical(y_plot, num_classes=4) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfVIVRZYl8m-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsGAjV9-iNIW"
      },
      "source": [
        "model.load_weights(s_data_file + \".h5\")\n",
        "model.evaluate(X_plot, utils.to_categorical(y_plot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKmHj-mc0vGR"
      },
      "source": [
        "###predict1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBjNsl7X0zAR"
      },
      "source": [
        "bs.login()\n",
        "\n",
        "ss_date = \"2018-06-01\"\n",
        "ee_date = \"2019-10-30\"\n",
        "\n",
        "# ss_code = \"sh.600004\"\n",
        "ss_code = \"sz.000717\"\n",
        "\n",
        "rs = bs.query_history_k_data_plus(ss_code, \",\".join(data_fields),\\\n",
        "                          start_date = ss_date, end_date = ee_date,\\\n",
        "                            frequency = \"d\", adjustflag = adjustflag)\n",
        "\n",
        "data_list = []\n",
        "while (rs.error_code == \"0\") and rs.next():\n",
        "    row_data = rs.get_row_data()\n",
        "    if(row_data[6] == \"1\"):         \n",
        "        data_list.append(row_data)\n",
        "pp_result = pd.DataFrame(data_list, columns = rs.fields)\n",
        "\n",
        "print(pp_result.head())\n",
        "\n",
        "bs.logout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUuNOI491P9T"
      },
      "source": [
        "x_plot = []\n",
        "y_plot = []\n",
        "\n",
        "stk_data = pp_result\n",
        "length = len(stk_data)\n",
        "if (length > window_size):\n",
        "  for l in range(length - window_size):\n",
        "    ohlct = stk_data.iloc[l : l + window_size] \\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"isST\",\"pctChg\"]].values\n",
        "    ohlct = ohlct.astype(\"float\")\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "      min_price = ohlct[:,:-2].min()\n",
        "      max_price = ohlct[:,:-2].max()\n",
        "      scale = (max_price - min_price)\n",
        "      ohlct[:,:-2] = ([ohlct[:,:-2] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:,:-2], axis = -1)])\n",
        "\n",
        "  XX_plot = np.array(x_plot)\n",
        "\n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfX2iJ_p1lq8"
      },
      "source": [
        "# model.load_weights(s_data_file + \".h5\")\n",
        "model.load_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "p = model.predict(XX_plot[-40:])\n",
        "p = np.apply_along_axis(lambda d: d.argmax() + 1, 1, p)\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot, color=\"blue\")\n",
        "plt.plot(p, color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAsGTqSIlZwS"
      },
      "source": [
        "#model 2: LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxoK7Xn8ll45"
      },
      "source": [
        "##model 2-1: LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSjx6XIglpZJ"
      },
      "source": [
        "\n",
        "K.clear_session()\n",
        "input = layers.Input(shape = (window_size, factor_num -1 )) #turn is not involved\n",
        "model = layers.LSTM(64, input_shape = (window_size, factor_num - 1), \\\n",
        "                    return_sequences = True)(input)\n",
        "# # model = layers.LSTM(64, return_sequences = True)(model)\n",
        "model = layers.Acvtivation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.LSTM(32)(model)\n",
        "model = layers.Acvtivation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(32)(model)\n",
        "model = layers.Acvtivation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(16)(model)\n",
        "model = layers.Acvtivation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(8)(model)\n",
        "model = layers.Acvtivation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(1)(model)\n",
        "\n",
        "model = Model(inputs = input, outputs = model)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "optimizer = optimizers.RMSprop(lr = 0.005, decay = 1e-5)\n",
        "\n",
        "model.compile(loss = \"mse\", optimizer = optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QXDMeMBn0h6"
      },
      "source": [
        "###dataset 2-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofs8O4q7oF1t"
      },
      "source": [
        "\n",
        "pct_of_stock  = 0.004\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "\n",
        "print(np.sort(index))\n",
        "                      \n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in index:\n",
        "  stk_data = result[result.code == ts_code.iloc[i][\"code\"]]\n",
        "  length = len(stk_data)\n",
        "  if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "    for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "      ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "      if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "        min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "        scale = max_price - min_price\n",
        "        ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "        x.append(ohlct[:-predict_next_days,:-2])\n",
        "\n",
        "        ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "        y.append(ohlct[-predict_next_days,-1])\n",
        "\n",
        "\n",
        "\n",
        "X = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "dis_count, dis_edge = np.histogram(y, bins = 20)\n",
        "\n",
        "dis_count_max = dis_count.max() * 4\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[(d*100//5).astype(int)]), \\\n",
        "                                0 , y)\n",
        "\n",
        "X = np.repeat(X, time_count, axis = 0)\n",
        "y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "length = X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "print(X.shape)\n",
        "print(X[1])\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJVW4-P93844"
      },
      "source": [
        "#Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6ZDA4D_Jw0l"
      },
      "source": [
        "##contants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXQ3phe1gj0i"
      },
      "source": [
        "s_data_file = \"sh.600004\" + \"_1-7.csv\"\n",
        "pstart_date = \"2019-03-01\"\n",
        "pend_date = \"2019-10-25\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkuQIxtaJ7jr"
      },
      "source": [
        "##load ts_code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3pIPHpwKAz_"
      },
      "source": [
        "ts_code = pd.read_csv(ts_code_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaQQSrknNZOb"
      },
      "source": [
        "##*get* data in specific period"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upHl1WCa4NDU",
        "outputId": "17c61873-f184-44b9-8f78-1a6ac0a5d44f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "source": [
        "data_list = []\n",
        "\n",
        "\n",
        "bs.login()\n",
        "for i in range(len(ts_code)):\n",
        "  if i%500 == 0 :\n",
        "    print(i)\n",
        "  rs = bs.query_history_k_data_plus(ts_code.iloc[i][\"code\"], \",\".join(data_fields),\\\n",
        "                              start_date = pstart_date, end_date = pend_date,\\\n",
        "                               frequency = \"d\", adjustflag = adjustflag)\n",
        "\n",
        "\n",
        "  while (rs.error_code == \"0\") and rs.next():\n",
        "    row_data = rs.get_row_data()\n",
        "    if(row_data[6] == \"1\"):         \n",
        "          data_list.append(row_data)\n",
        "\n",
        "bs.logout()\n",
        "\n",
        "presult = pd.DataFrame(data_list, columns = rs.fields)\n",
        "print(presult.head())\n",
        "print(presult.tail())\n",
        "print(presult.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "login success!\n",
            "0\n",
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n",
            "2500\n",
            "3000\n",
            "3500\n",
            "logout success!\n",
            "             open            high             low  ... tradestatus isST       code\n",
            "0  112.0159840200  114.3871945200  110.0241672000  ...           1    0  sh.600000\n",
            "1  114.9562850400  117.4223439600  113.3438619000  ...           1    0  sh.600000\n",
            "2  113.9129524200  114.1974976800  112.5850745400  ...           1    0  sh.600000\n",
            "3  113.7232555800  115.2408303000  112.4902261200  ...           1    0  sh.600000\n",
            "4  114.1026492600  114.1974976800  112.5850745400  ...           1    0  sh.600000\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "                  open            high  ... isST       code\n",
            "575136  368.9718880000  370.4658230900  ...    0  sz.300782\n",
            "575137  363.0262268700  380.0009390000  ...    0  sz.300782\n",
            "575138  378.9982980000  378.9982980000  ...    0  sz.300782\n",
            "575139  358.9454780000  366.7460249800  ...    0  sz.300782\n",
            "575140  354.9349140000  365.9639650000  ...    0  sz.300782\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "575141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVbI0AWZz64w",
        "outputId": "db9da453-00ea-4c8d-9600-0ee487062220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "presult[presult.code == \"sh.600526\"].iloc[-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "open           15.6064688000\n",
              "high           15.6364812400\n",
              "low            15.4263941600\n",
              "close          15.6364812400\n",
              "turn                0.244600\n",
              "pctChg              0.192300\n",
              "tradestatus                1\n",
              "isST                       1\n",
              "code               sh.600526\n",
              "Name: 67543, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p09S-yNaQdIG"
      },
      "source": [
        "##predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKOVw9tMJrzd",
        "outputId": "3be2713c-7f19-4871-de27-4c30e295394a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "ts = []\n",
        "pp = []\n",
        "\n",
        "# model.load_weights(s_data_file + \".h5\")\n",
        "model.load_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "for i in range(len(ts_code)):\n",
        "  if i%500 == 0:\n",
        "    print(i)\n",
        "  x = []\n",
        "  stk_data = presult[presult.code == ts_code.iloc[i][\"code\"]]\n",
        "  length = len(stk_data)\n",
        "  if length >= window_size :\n",
        "    # for l in range(length - window_size):\n",
        "    ohlct = stk_data.iloc[length - window_size : length]\\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\"]].values.astype(\"float\")\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "      np.all(ohlct[:,-3] != 0) and \\\n",
        "      np.all(ohlct[:,-1] >= -10.0) and \\\n",
        "      np.all(ohlct[:,-1] <= 10.0) :\n",
        "      max_price = ohlct[:,:-3].max()\n",
        "      min_price = ohlct[:,:-3].min()\n",
        "      scale = max_price - min_price\n",
        "      ohlct[:,:-3] = ([ohlct[:,:-3] - min_price]) /scale\n",
        "\n",
        "      x.append(np.expand_dims(ohlct[:,:-3], axis=-1))\n",
        "\n",
        "    if len(x) > 0:\n",
        "      pX = np.array([x[-1]])\n",
        "      # print(pX.shape)\n",
        "      p = model.predict(pX)      \n",
        "      if p[-1].argmax() == 3 and p[-1,3] > 0.995:\n",
        "        ts.append(ts_code.iloc[i][\"code\"])\n",
        "        pp.append(p[-1])\n",
        "\n",
        "print(ts)\n",
        "print(len(ts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n",
            "2500\n",
            "3000\n",
            "3500\n",
            "['sh.600009', 'sh.600208', 'sh.600262', 'sh.600271', 'sh.600312', 'sh.600339', 'sh.600436', 'sh.600516', 'sh.600616', 'sh.600633', 'sh.600642', 'sh.600653', 'sh.601900', 'sh.601985', 'sh.603165', 'sh.603444', 'sh.603650', 'sh.603658', 'sh.603757', 'sh.603828', 'sz.000100', 'sz.000403', 'sz.000597', 'sz.000761', 'sz.000932', 'sz.000999', 'sz.001965', 'sz.002148', 'sz.002322', 'sz.002335', 'sz.002545', 'sz.002558', 'sz.002583', 'sz.002588', 'sz.002594', 'sz.002634', 'sz.002706', 'sz.002721', 'sz.002805', 'sz.002912', 'sz.002932', 'sz.002940', 'sz.300192', 'sz.300226', 'sz.300246', 'sz.300393', 'sz.300421', 'sz.300558', 'sz.300572', 'sz.300759']\n",
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E40kbJsKsKNE",
        "outputId": "44c43377-4fe6-4bec-ef4a-835b9318ae0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "for i in range(len(ts)):\n",
        "  if(pp[i][3]) > 0.998:\n",
        "    print(ts[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sh.600009\n",
            "sh.600208\n",
            "sh.600262\n",
            "sh.600271\n",
            "sh.600339\n",
            "sh.600436\n",
            "sh.600516\n",
            "sh.600616\n",
            "sh.600633\n",
            "sh.601900\n",
            "sh.601985\n",
            "sh.603444\n",
            "sh.603650\n",
            "sh.603757\n",
            "sh.603828\n",
            "sz.000100\n",
            "sz.000403\n",
            "sz.000932\n",
            "sz.000999\n",
            "sz.001965\n",
            "sz.002335\n",
            "sz.002545\n",
            "sz.002558\n",
            "sz.002583\n",
            "sz.002588\n",
            "sz.002594\n",
            "sz.002634\n",
            "sz.002706\n",
            "sz.002721\n",
            "sz.002805\n",
            "sz.002912\n",
            "sz.002940\n",
            "sz.300192\n",
            "sz.300226\n",
            "sz.300246\n",
            "sz.300393\n",
            "sz.300421\n",
            "sz.300558\n",
            "sz.300572\n",
            "sz.300759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbBeYEWfnzJf",
        "outputId": "50f29491-46dc-46e6-ca92-92a22685449e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "pp[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.8331841e-07, 8.0780563e-07, 4.6011610e-03, 9.9539739e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8jQDq6fm-Sh",
        "outputId": "62a684e4-d5f5-46e7-f651-c3d2b89c7a0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "for i in range(len(ts)):\n",
        "  if ts[i] == \"sh.600009\":\n",
        "     print(pp[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.6273554e-08 1.2722618e-08 3.0374675e-04 9.9969625e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCP5NnXqsKAK",
        "outputId": "121fea90-3759-4a3a-ffdd-00c52f538e08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "# a = np.array(x[185:188])\n",
        "a =  pd.DataFrame(x[190].reshape(90,-1))\n",
        "print(a.shape)\n",
        "# print(x[187])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-174-714c304c87f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m190\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(x[187])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 344 into shape (90,newaxis)"
          ]
        }
      ]
    }
  ]
}