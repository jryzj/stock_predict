{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "stock_preditct_4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OM3cHJETg0om",
        "uhyvAKdihCrC",
        "JZusNrhchOaS",
        "WkqwirDJnxSs",
        "RIfx1kh8nkAq",
        "v7l4RbsFikd4",
        "x17JDv8Wi12w",
        "lUmQNnUyjYPO",
        "llpn-tNzjwO3",
        "uzFRlwZzkAfw",
        "bgdFoC0ZqnOx",
        "ycLBn2g9eJoz",
        "yMaIpUCXdKAd",
        "TtE3stnXDcz1",
        "cH6D_yy4Dc0K",
        "4ji2nW0xNK9-",
        "0n5kGdKdNK-L",
        "XitBIfkFNK-d",
        "NeK-E3zCsBub",
        "QCpq_WYJsBux",
        "IOpu9aoFsBu-",
        "mFHZhz3NsBvU",
        "8WUuXH9nsBvj",
        "V1_k6_jXNQx6",
        "6qYIH7kzfsNh",
        "Vj_SJEQ_f2-h",
        "DyRaLilwhXDe",
        "51tkVCF26Agg",
        "LBQKvnNyvv76",
        "L7ibvQRGvv8G",
        "AQeHx4HBvv8O",
        "lJUabvmyvv8e",
        "NCk527zzvv85",
        "kKXBcctzvv95",
        "P8iDRzhCvv-S",
        "cKV8H5ywvv-m",
        "3TdN1QEEvv-5",
        "jo_Nwseevv_d",
        "V9ukse_8iNG1",
        "XqH6I3rXiNHC",
        "BN8mbQlliNHU",
        "szBLpsH5iNHt",
        "ASbuU208iNHz",
        "zkuQIxtaJ7jr"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jryzj/stock_predict/blob/master/stock_preditct_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhCsGpjYwSIn"
      },
      "source": [
        "##Begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwEQMN-Eeza0"
      },
      "source": [
        "##mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNldvRWte7Q6",
        "outputId": "25a4cbf3-4407-48ce-f302-f2761d47e103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdriver')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdriver\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pxq9oTrwWEX",
        "outputId": "eb0967d3-c840-4ea6-945c-f6dca8c8847d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Nov  1 04:24:59 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCWJ6YrL3SXL",
        "outputId": "631b8d68-ca8f-40db-99b7-0ab89bb8f18a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2200.000\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 4\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 2\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 4400.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2200.000\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 4\n",
            "core id\t\t: 1\n",
            "cpu cores\t: 2\n",
            "apicid\t\t: 2\n",
            "initial apicid\t: 2\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 4400.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 2\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2200.000\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 4\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 2\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 4400.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 3\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2200.000\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 4\n",
            "core id\t\t: 1\n",
            "cpu cores\t: 2\n",
            "apicid\t\t: 3\n",
            "initial apicid\t: 3\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 4400.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8B6uYPSrUhK",
        "outputId": "9d7adaa7-7557-45ac-ebd1-f4fae40609c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "TCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6197923708dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'TCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMkJNgh2VhLp",
        "outputId": "759ba468-9fc3-4a57-d963-9372004074e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "#for tensorflow2\n",
        "\n",
        "import sys\n",
        "sys.path\n",
        "# sys.path[0]='/tensorflow-2.1.0/python3.6'\n",
        "sys.path[0]='/usr/local/lib/python3.6'\n",
        "sys.path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/usr/local/lib/python3.6',\n",
              " '/env/python',\n",
              " '/usr/lib/python36.zip',\n",
              " '/usr/lib/python3.6',\n",
              " '/usr/lib/python3.6/lib-dynload',\n",
              " '/usr/local/lib/python3.6/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n",
              " '/root/.ipython']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkMcs1dkWG__",
        "outputId": "4cf2c8a0-1f36-469f-d7c2-e53a553cd176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n",
        "\n",
        "# tf.compat.v1.disable_eager_execution()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNUuY7ea-XCf"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVxo7raE4Psq",
        "outputId": "ab5e91ea-1f1e-4d08-913a-58e71c7f4efc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "!ls -l /"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 104\n",
            "drwxr-xr-x   1 root root 4096 Sep 28 16:28 bin\n",
            "drwxr-xr-x   2 root root 4096 Apr 24  2018 boot\n",
            "drwxr-xr-x   1 root root 4096 Oct  1 06:34 content\n",
            "drwxr-xr-x   1 root root 4096 Sep 29 16:40 datalab\n",
            "drwxr-xr-x   5 root root  440 Oct  1 06:27 dev\n",
            "drwxr-xr-x   1 root root 4096 Oct  1 06:27 etc\n",
            "drwxr-xr-x   2 root root 4096 Apr 24  2018 home\n",
            "drwxr-xr-x   1 root root 4096 Sep 28 16:30 lib\n",
            "drwxr-xr-x   2 root root 4096 Sep 28 16:23 lib32\n",
            "drwxr-xr-x   2 root root 4096 Aug  7 22:41 lib64\n",
            "drwxr-xr-x   2 root root 4096 Aug  7 22:39 media\n",
            "drwxr-xr-x   2 root root 4096 Aug  7 22:39 mnt\n",
            "drwxr-xr-x   1 root root 4096 Oct  1 06:27 opt\n",
            "dr-xr-xr-x 141 root root    0 Oct  1 06:27 proc\n",
            "drwx------   1 root root 4096 Oct  1 07:10 root\n",
            "drwxr-xr-x   1 root root 4096 Sep 28 16:25 run\n",
            "drwxr-xr-x   1 root root 4096 Sep 28 16:28 sbin\n",
            "drwxr-xr-x   2 root root 4096 Aug  7 22:39 srv\n",
            "drwxr-xr-x   4 root root 4096 Sep 29 16:40 swift\n",
            "dr-xr-xr-x  12 root root    0 Oct  1 06:40 sys\n",
            "drwxr-xr-x   4 root root 4096 Sep 29 16:36 tensorflow-1.15.2\n",
            "drwxrwxrwt   1 root root 4096 Oct  1 09:46 tmp\n",
            "drwxr-xr-x   1 root root 4096 Sep 29 16:40 tools\n",
            "drwxr-xr-x   1 root root 4096 Oct  1 06:27 usr\n",
            "drwxr-xr-x   1 root root 4096 Oct  1 06:27 var\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-hnrLCb6zC6",
        "outputId": "074de70d-2f86-47db-8cd1-b6b4328d338e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        }
      },
      "source": [
        "# !pip install tensorflow==2.3.1\n",
        "# !pip install tf-nightly\n",
        "!pip install tf-nightly-gpu\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/00/ed4255da5bf45bbe7f833c5cc1b1caa4069349f84259ec1d6c8917cf5273/tf_nightly_gpu-2.4.0.dev20201001-cp36-cp36m-manylinux2010_x86_64.whl (391.8MB)\n",
            "\u001b[K     |████████████████████████████████| 391.8MB 36kB/s \n",
            "\u001b[?25hCollecting flatbuffers>=1.12\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.6.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.12.4)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.35.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.7.4.3)\n",
            "Collecting tb-nightly<3.0.0a0,>=2.4.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/16/60ac560dcc8822ba7f2ca07efa7828a4214b0c73689792ed0d17c5d5cf5d/tb_nightly-2.4.0a20201001-py3-none-any.whl (10.6MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6MB 62.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.3.0)\n",
            "Collecting tf-estimator-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/71/4ad680a5b14763cbf219f5c78448eb2a1329744b5cd05107ff7d5d8f4747/tf_estimator_nightly-2.4.0.dev2020100101-py2.py3-none-any.whl (461kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 56.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tf-nightly-gpu) (50.3.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (1.7.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (2.0.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly-gpu) (3.2.0)\n",
            "Installing collected packages: flatbuffers, tb-nightly, tf-estimator-nightly, tf-nightly-gpu\n",
            "Successfully installed flatbuffers-1.12 tb-nightly-2.4.0a20201001 tf-estimator-nightly-2.4.0.dev2020100101 tf-nightly-gpu-2.4.0.dev20201001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GnFSbgvWbzH",
        "outputId": "26d7e071-b237-4c3d-aace-ff4d1c8284b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcSRLDccj538"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6Yc7lCufBu2"
      },
      "source": [
        "#change path to notebook in cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw16QijjfKWc"
      },
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "# os.chdir(\"/../content/gdriver/My Drive/Colab Notebooks (1)\")\n",
        "os.chdir(\"/../content/gdriver/My Drive/Colab Notebooks\")\n",
        "\n",
        "# os.chdir(\"/../content/gdriver/My Drive\")\n",
        "# os.chdir(\"gdriver/My Drive/Colab Notebooks\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyT1uBAgFb2U"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZpj2K30fRlh"
      },
      "source": [
        "#set constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye2mvVArfUru"
      },
      "source": [
        "#常数设置\n",
        "\n",
        "threshold = 3.0 #涨幅\n",
        "start_date = \"2006-01-01\"\n",
        "end_date = \"2019-06-23\"\n",
        "stock_prefix = \"sh.6|sz.0|sz.300\"\n",
        "\n",
        "window_size = 90\n",
        "predict_window_size = 5\n",
        "# adjustflag = \"1\" #后复权\n",
        "adjustflag = \"3\" #不复权\n",
        "days_from_ipo = 40 #新股上市一般会连涨很多天，排除这些比较异常的天数\n",
        "\n",
        "data_fields = [\"open\", \"high\", \"low\", \"close\", \"turn\",\"pctChg\",\"tradestatus\",\"isST\",\"code\"]\n",
        "all_data_fields = \"date,code,open,high,low,close,preclose,volume,amount,adjustflag,turn,tradestatus,pctChg,isST\"\n",
        "data_fields_market_index = [\"open\", \"high\", \"low\", \"close\", \"turn\",\"pctChg\",\"date\"]\n",
        "\n",
        "\n",
        "# factor_num = len(data_fields) - 4 #pctChg, tradestatus, code isST不参与模型计算\n",
        "factor_num = 12\n",
        "\n",
        "all_stock_data = \"all_stock_data.csv\"\n",
        "all_stock_data_1 = \"all_stock_data_1.csv\"\n",
        "all_stock_data_2 = \"all_stock_data_2.csv\" #不复权\n",
        "all_stock_data_3 = \"all_stock_data_3.csv\" #with all data fields, no divid adjusting\n",
        "\n",
        "ts_code_file = \"ts_code.csv\"\n",
        "market_index = [\"sh.000001\", \"sz.399106\"]\n",
        "market_index_data_file = \"market_index_data.csv\"\n",
        "\n",
        "# predict_next_days = 1\n",
        "predict_next_days = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4pkQsmZepWu"
      },
      "source": [
        "#intall pack necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1OEz0tQfgFo",
        "outputId": "95fdf4c0-d060-4ef9-85cc-2dc96d40f4f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "!pip install baostock\n",
        "import baostock as bs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting baostock\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/e2/b367c78db42bafcf752442b7d582ba2a724286313d9f126c5fee06064fb2/baostock-0.8.8-py3-none-any.whl (55kB)\n",
            "\r\u001b[K     |██████                          | 10kB 20.4MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 40kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from baostock) (1.0.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->baostock) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->baostock) (1.18.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->baostock) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.18.0->baostock) (1.15.0)\n",
            "Installing collected packages: baostock\n",
            "Successfully installed baostock-0.8.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nveGeL_Mfl7K"
      },
      "source": [
        "#import basic pack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9pLaOARfpbN",
        "outputId": "a81efebb-e263-4f87-e895-bf8566247779",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#引入基本包\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# import baostock as bs\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "import gc\n",
        "\n",
        "import tracemalloc\n",
        "\n",
        "\n",
        "np.random.seed(2019)\n",
        "\n",
        "!pip install gputil\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "\n",
        "def show_mem(gpu = False):\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " if gpu:\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "show_mem(gpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7411 sha256=7ff2d5c28050f0e2b6def4c161a12d195f356de9c00c05a10e823704d214bc8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Gen RAM Free: 26.4 GB  | Proc size: 111.4 MB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVjGRTMSfvxZ"
      },
      "source": [
        "#import keras pack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm0RuSbhgDQ4"
      },
      "source": [
        "##for tensorflow2\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "\n",
        "class Running(Callback):\n",
        "  def on_train_begin(self, sign_gen):\n",
        "    next(sign_gen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTrvFj1EQ3AQ",
        "outputId": "a86eab9e-a509-4a7e-c0c7-f0cec7d08cf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "!pip install --upgrade keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\r\u001b[K     |▉                               | 10kB 26.9MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 31.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 34.9MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 38.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 33.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 35.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 29.2MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 31.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102kB 27.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 27.2MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122kB 27.2MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133kB 27.2MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143kB 27.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 27.2MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163kB 27.2MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174kB 27.2MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184kB 27.2MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194kB 27.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204kB 27.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 215kB 27.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 27.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 27.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245kB 27.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256kB 27.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 266kB 27.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 276kB 27.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 286kB 27.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296kB 27.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307kB 27.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 317kB 27.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 327kB 27.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 337kB 27.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348kB 27.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358kB 27.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 368kB 27.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 27.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "Successfully installed keras-2.3.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKa8wdFDCWee",
        "outputId": "af7e8ebd-a391-45da-cb6b-cdf543b51b13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(keras.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD7sGnxvgEkL"
      },
      "source": [
        "#setup tensorboard if need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3XacfEsgJjo"
      },
      "source": [
        "LOG_DIR = \"/content/gdriver/My\\ Drive/Colab\\ Notebooks/tensor_board/stock_predict_2\"\n",
        "LOG_DIR1 = \"/content/gdriver/My Drive/Colab Notebooks/tensor_board/stock_predict_2\"\n",
        "\n",
        "from keras.callbacks import TensorBoard\n",
        "tbCallBack = TensorBoard(log_dir= LOG_DIR1 , histogram_freq=1,\n",
        "#                          write_graph=True,\n",
        "                         write_grads=True,\n",
        "                         batch_size=batch_size,\n",
        "#                          write_images=True\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXxNrLWKgX1m"
      },
      "source": [
        "#setup ngrok if need"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwbHlvXUgjp-"
      },
      "source": [
        "##install ngrok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5GO9QfOgR_h"
      },
      "source": [
        "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM3cHJETg0om"
      },
      "source": [
        "##setup ngrok account"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z7OQRlDgn0i"
      },
      "source": [
        "!./ngrok authtoken 6wz1ai2QDnxrk1HYUor4U_4zjD6PD8wUkGyMnWnmXiQ\n",
        "# get_ipython().system_raw('./ngrok authtoken 6wz1ai2QDnxrk1HYUor4U_4zjD6PD8wUkGyMnWnmXiQ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhyvAKdihCrC"
      },
      "source": [
        "##start ngrok service"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfjG1F1UhInS"
      },
      "source": [
        "#开启ngrok service，绑定port 6006(tensorboard)\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "# !./ngrok http 6006"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZusNrhchOaS"
      },
      "source": [
        "##get ngrok website"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6V1nPulhmJs"
      },
      "source": [
        "! curl http://localhost:4040/api/tunnels | python3 -c \\\n",
        "\"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkqwirDJnxSs"
      },
      "source": [
        "#get ts code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2dEYovPn1EZ"
      },
      "source": [
        "#获取股票代码清单\n",
        "#### 登陆系统 ####\n",
        "lg = bs.login()\n",
        "# 显示登陆返回信息\n",
        "print('login respond error_code:'+lg.error_code)\n",
        "print('login respond  error_msg:'+lg.error_msg)\n",
        "\n",
        "#### 获取证券信息 ####\n",
        "rs = bs.query_all_stock(day=\"2019-06-28\")\n",
        "print('query_all_stock respond error_code:'+rs.error_code)\n",
        "print('query_all_stock respond  error_msg:'+rs.error_msg)\n",
        "\n",
        "#### 打印结果集 ####\n",
        "data_list = []\n",
        "while (rs.error_code == '0') & rs.next():\n",
        "    # 获取一条记录，将记录合并在一起\n",
        "    data_list.append(rs.get_row_data())\n",
        "ts_code = pd.DataFrame(data_list, columns=rs.fields)\n",
        "\n",
        "#### 结果集输出到csv文件 ####   \n",
        "# ts_code.to_csv(\"D:\\\\all_stock.csv\", encoding=\"gbk\", index=False)\n",
        "print(ts_code)\n",
        "\n",
        "#### 登出系统 ####\n",
        "bs.logout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIfx1kh8nkAq"
      },
      "source": [
        "#get ts histroy data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SETWlt3InopO",
        "outputId": "ebafd0ed-126c-4fe0-abf6-f48ebbf75a34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#获取股票历史数据\n",
        "#用DataFrame.append的方法是需要重新分配内存，完成追加数据\n",
        "#先用list.append的方法是用指针指向追加数据，内存不重新分配，数据大的时候，效率高\n",
        "#每次获取一支股票的数据，追加到csv文件中。\n",
        "#不复权\n",
        "\n",
        "adjustflag = \"3\"  #不复权\n",
        "\n",
        "result = pd.DataFrame([],columns = all_data_fields.split(\",\"))\n",
        "result.to_csv(all_stock_data_3)\n",
        "\n",
        "bs.login()\n",
        "for i in range(len(ts_code)):\n",
        "    if i % 50 == 0:\n",
        "        print(i)\n",
        "    rs = bs.query_history_k_data_plus(ts_code.iloc[i][\"code\"], all_data_fields,\\\n",
        "                start_date = start_date, end_date = end_date,\\\n",
        "                  frequency = \"d\", adjustflag = adjustflag) #不复权\n",
        "\n",
        "    data_list = []\n",
        "    while (rs.error_code == \"0\") and rs.next():\n",
        "        row_data = rs.get_row_data()\n",
        "        # if(row_data[6] == \"1\"):    #  停牌的日子还是应该获取的。   \n",
        "        data_list.append(row_data)\n",
        "    result = pd.DataFrame(data_list, columns = rs.fields)\n",
        "    result.to_csv(all_stock_data_3, mode = \"a\", header=False)\n",
        "\n",
        "bs.logout()\n",
        "\n",
        "\n",
        "print(result.head())\n",
        "print(result.tail())\n",
        "print(result.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "login success!\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n",
            "550\n",
            "600\n",
            "650\n",
            "700\n",
            "750\n",
            "800\n",
            "850\n",
            "900\n",
            "950\n",
            "1000\n",
            "1050\n",
            "1100\n",
            "1150\n",
            "1200\n",
            "1250\n",
            "1300\n",
            "1350\n",
            "1400\n",
            "1450\n",
            "1500\n",
            "1550\n",
            "1600\n",
            "1650\n",
            "1700\n",
            "1750\n",
            "1800\n",
            "1850\n",
            "1900\n",
            "1950\n",
            "2000\n",
            "2050\n",
            "2100\n",
            "2150\n",
            "2200\n",
            "2250\n",
            "2300\n",
            "2350\n",
            "2400\n",
            "2450\n",
            "2500\n",
            "2550\n",
            "2600\n",
            "2650\n",
            "2700\n",
            "2750\n",
            "2800\n",
            "2850\n",
            "2900\n",
            "2950\n",
            "3000\n",
            "3050\n",
            "3100\n",
            "3150\n",
            "3200\n",
            "3250\n",
            "3300\n",
            "3350\n",
            "3400\n",
            "3450\n",
            "3500\n",
            "3550\n",
            "3600\n",
            "logout success!\n",
            "         date       code     open  ... tradestatus     pctChg isST\n",
            "0  2019-06-18  sz.300782  42.3500  ...           1  44.006800    0\n",
            "1  2019-06-19  sz.300782  55.9000  ...           1   9.996069    0\n",
            "2  2019-06-20  sz.300782  61.4900  ...           1  10.000000    0\n",
            "3  2019-06-21  sz.300782  67.6400  ...           1  10.001620    0\n",
            "\n",
            "[4 rows x 14 columns]\n",
            "         date       code     open  ... tradestatus     pctChg isST\n",
            "0  2019-06-18  sz.300782  42.3500  ...           1  44.006800    0\n",
            "1  2019-06-19  sz.300782  55.9000  ...           1   9.996069    0\n",
            "2  2019-06-20  sz.300782  61.4900  ...           1  10.000000    0\n",
            "3  2019-06-21  sz.300782  67.6400  ...           1  10.001620    0\n",
            "\n",
            "[4 rows x 14 columns]\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLnYyu6HqGbE",
        "outputId": "db94b553-3099-4fe9-d09c-fd40efdc89cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "#\n",
        "\n",
        "adjustflag = \"3\"  #不复权\n",
        "\n",
        "result = pd.DataFrame([],columns = all_data_fields.split(\",\"))\n",
        "result.to_csv(market_index_data)\n",
        "\n",
        "bs.login()\n",
        "for i in range(len(market_index)):\n",
        "  print(market_index[i])\n",
        "  rs = bs.query_history_k_data_plus(market_index[i], all_data_fields,\\\n",
        "                start_date = start_date, end_date = end_date,\\\n",
        "                  frequency = \"d\", adjustflag = adjustflag) #不复权\n",
        "\n",
        "  data_list = []\n",
        "  while (rs.error_code == \"0\") and rs.next():\n",
        "      row_data = rs.get_row_data() \n",
        "      data_list.append(row_data)\n",
        "  result = pd.DataFrame(data_list, columns = rs.fields)\n",
        "  result.to_csv(market_index_data, mode = \"a\", header=False)\n",
        "\n",
        "bs.logout()\n",
        "\n",
        "\n",
        "print(result.head())\n",
        "print(result.tail())\n",
        "print(result.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "login success!\n",
            "sh.000001\n",
            "sz.399106\n",
            "logout success!\n",
            "         date       code      open  ... tradestatus    pctChg isST\n",
            "0  2006-01-04  sz.399106  278.9900  ...           1  1.696865    0\n",
            "1  2006-01-05  sz.399106  283.8000  ...           1  1.993084    0\n",
            "2  2006-01-06  sz.399106  289.5700  ...           1  0.747070    0\n",
            "3  2006-01-09  sz.399106  291.4800  ...           1  1.023029    0\n",
            "4  2006-01-10  sz.399106  294.1500  ...           1  0.584498    0\n",
            "\n",
            "[5 rows x 14 columns]\n",
            "            date       code       open  ... tradestatus     pctChg isST\n",
            "3268  2019-06-17  sz.399106  1504.2410  ...           1  -0.195000    0\n",
            "3269  2019-06-18  sz.399106  1502.9070  ...           1   0.162700    0\n",
            "3270  2019-06-19  sz.399106  1542.4600  ...           1   1.475500    0\n",
            "3271  2019-06-20  sz.399106  1525.5030  ...           1   1.953800    0\n",
            "3272  2019-06-21  sz.399106  1567.0710  ...           1   1.339000    0\n",
            "\n",
            "[5 rows x 14 columns]\n",
            "3273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJZpV7D7ht8X"
      },
      "source": [
        "#load stock data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJrO2bDihxV1",
        "outputId": "13ca2346-ae34-4fea-891a-92ddc0a835d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ts_data = pd.read_csv(all_stock_data_3)\n",
        "print(ts_data.head())\n",
        "# print(ts_data.tail())\n",
        "\n",
        "ts_code = pd.read_csv(ts_code_file)\n",
        "# print(ts_code.head())\n",
        "\n",
        "market_index_data = pd.read_csv(market_index_data_file)\n",
        "print(market_index_data.head())\n",
        "# print(market_index_data.tail())\n",
        "\n",
        "market_index_data_sh = market_index_data[market_index_data[\"code\"] == market_index[0]]\n",
        "market_index_data_sh = market_index_data_sh[data_fields_market_index]\n",
        "print(market_index_data_sh.head())\n",
        "print(market_index_data_sh.tail())\n",
        "market_index_data_sz = market_index_data[market_index_data[\"code\"] == market_index[1]].reset_index()\n",
        "market_index_data_sz = market_index_data_sz[data_fields_market_index]\n",
        "print(market_index_data_sz.head())\n",
        "print(market_index_data_sz.tail())\n",
        "market_index_data = (market_index_data_sh.values,\\\n",
        "                     market_index_data_sz.values)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0        date       code  ...  tradestatus    pctChg  isST\n",
            "0           0  2006-01-04  sh.600000  ...            1  5.128205     0\n",
            "1           1  2006-01-05  sh.600000  ...            1  1.658537     0\n",
            "2           2  2006-01-06  sh.600000  ...            1  2.879081     0\n",
            "3           3  2006-01-09  sh.600000  ...            1 -1.212688     0\n",
            "4           4  2006-01-10  sh.600000  ...            1  0.283284     0\n",
            "\n",
            "[5 rows x 15 columns]\n",
            "   Unnamed: 0        date       code  ...  tradestatus    pctChg  isST\n",
            "0           0  2006-01-04  sh.000001  ...            1  1.714473   0.0\n",
            "1           1  2006-01-05  sh.000001  ...            1  1.380740   0.0\n",
            "2           2  2006-01-06  sh.000001  ...            1  1.015056   0.0\n",
            "3           3  2006-01-09  sh.000001  ...            1  0.516443   0.0\n",
            "4           4  2006-01-10  sh.000001  ...            1  0.407190   0.0\n",
            "\n",
            "[5 rows x 15 columns]\n",
            "       open      high       low     close      turn    pctChg        date\n",
            "0  1163.878  1181.004  1161.906  1180.963  0.015049  1.714473  2006-01-04\n",
            "1  1183.305  1197.837  1180.451  1197.269  0.019075  1.380740  2006-01-05\n",
            "2  1198.811  1215.536  1191.614  1209.422  0.022207  1.015056  2006-01-06\n",
            "3  1210.320  1217.314  1205.248  1215.668  0.018558  0.516443  2006-01-09\n",
            "4  1215.848  1220.756  1203.651  1220.618  0.017307  0.407190  2006-01-10\n",
            "          open      high       low     close      turn  pctChg        date\n",
            "3268  2880.422  2902.480  2877.394  2887.622  0.451135  0.1960  2019-06-17\n",
            "3269  2891.091  2898.330  2874.312  2890.158  0.432766  0.0878  2019-06-18\n",
            "3270  2944.115  2953.335  2916.214  2917.802  0.675970  0.9565  2019-06-19\n",
            "3271  2917.331  2997.388  2915.089  2987.118  0.852394  2.3756  2019-06-20\n",
            "3272  2990.368  3010.349  2989.245  3001.980  0.840151  0.4975  2019-06-21\n",
            "     open    high     low   close      turn    pctChg        date\n",
            "0  278.99  283.48  278.99  283.48  0.012810  1.696865  2006-01-04\n",
            "1  283.80  289.22  283.61  289.13  0.019449  1.993084  2006-01-05\n",
            "2  289.57  292.88  287.43  291.29  0.021566  0.747070  2006-01-06\n",
            "3  291.48  294.34  290.45  294.27  0.019832  1.023029  2006-01-09\n",
            "4  294.15  296.07  291.57  295.99  0.017998  0.584498  2006-01-10\n",
            "          open      high       low     close      turn  pctChg        date\n",
            "3268  1504.241  1513.623  1496.824  1502.122  1.243835 -0.1950  2019-06-17\n",
            "3269  1502.907  1510.965  1492.585  1504.566  1.173195  0.1627  2019-06-18\n",
            "3270  1542.460  1548.719  1526.071  1526.766  1.666313  1.4755  2019-06-19\n",
            "3271  1525.503  1563.322  1520.045  1556.596  1.918687  1.9538  2019-06-20\n",
            "3272  1567.071  1582.275  1566.933  1577.438  2.125991  1.3390  2019-06-21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnry9CHxaC6a",
        "outputId": "bf01b0c6-7685-4775-b7c7-5c5d4d82ee07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "type(ts_data.date[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_9rM78cheUj",
        "outputId": "58174e64-be72-4af3-a080-c1f9bcaab20a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "market_index_data[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, '2006-01-04', 'sh.000001', ..., 1, 1.714473, 0.0],\n",
              "       [1, '2006-01-05', 'sh.000001', ..., 1, 1.3807399999999999, 0.0],\n",
              "       [2, '2006-01-06', 'sh.000001', ..., 1, 1.015056, 0.0],\n",
              "       ...,\n",
              "       [3270, '2019-06-19', 'sh.000001', ..., 1, 0.9565, 0.0],\n",
              "       [3271, '2019-06-20', 'sh.000001', ..., 1, 2.3756, 0.0],\n",
              "       [3272, '2019-06-21', 'sh.000001', ..., 1, 0.4975, 0.0]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlNy3GK9cJFe",
        "outputId": "e5a39fc0-6dd4-4d35-944f-39292d0ddce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "ts_code = pd.read_csv(ts_code_file)\n",
        "print(ts_code.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0       code code_name\n",
            "0           0  sh.600000      浦发银行\n",
            "1           1  sh.600004      白云机场\n",
            "2           2  sh.600006      东风汽车\n",
            "3           3  sh.600007      中国国贸\n",
            "4           4  sh.600008      首创股份\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST2PaXXb0Fp_"
      },
      "source": [
        "#prepare dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eeF-RTo0OT_"
      },
      "source": [
        "##define dataset generating function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NjqnYAcG_Ja"
      },
      "source": [
        "def normals(np_array):\n",
        "  max_price = np_array.max()\n",
        "  min_price = np_array.min()\n",
        "  scale = max_price - min_price\n",
        "  return (np_array - min_price)/scale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kEmGkQd0D6N"
      },
      "source": [
        "\n",
        "\n",
        "def gntor_like(ts_data, ts_code, mark_index_data, pct_of_stock=0.05, edge=10.0,\\\n",
        "                    shuffle = 0, y_cate = 1):  \n",
        "\n",
        "  stock_qty = len(ts_code)\n",
        "\n",
        "  \n",
        "  if pct_of_stock != 1:\n",
        "    index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "    ts_code = ts_code[ts_code.index.isin(index)].code.values\n",
        "  else:\n",
        "    index = ts_code.code.values\n",
        "  \n",
        "  print(len(index))                   \n",
        "\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  for i in index:\n",
        "\n",
        "    stk_data = ts_data[ts_data[\"code\"] == i]\n",
        "    length = len(stk_data)\n",
        "    if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "      for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "        ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "            [[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\"]].values.astype(\"float\")\n",
        "        if np.all(ohlct[:,5] != 1) and \\\n",
        "          np.all(ohlct[:,4] != 0) and \\\n",
        "        np.all(ohlct[:,6] >= -10.1) and \\\n",
        "        np.all(ohlct[:,6] <= 10.1) :\n",
        "          ohlct[:-predict_next_days,:4] = normals(ohlct[:-predict_next_days,:4])\n",
        "\n",
        "          date_range = stk_data.iloc[l : l + window_size + predict_next_days][\"date\"]\n",
        "\n",
        "          sh_index_data = market_index_data[(market_index_data[\"code\"] == market_index[0])\\\n",
        "           & (market_index_data[\"date\"].isin(date_range))]\n",
        "          sh_index_data = sh_index_data[:-predict_next_days][[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "          sh_index_data = normals(sh_index_data)\n",
        "\n",
        "\n",
        "          sz_index_data = market_index_data[(market_index_data[\"code\"] == market_index[1])\\\n",
        "           & (market_index_data[\"date\"].isin(date_range))]\n",
        "          sz_index_data = sz_index_data[:-predict_next_days][[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "          sz_index_data = normals(sz_index_data) \n",
        "\n",
        "          x.append(np.expand_dims(np.hstack((ohlct[:-predict_next_days,:4], sh_index_data, sz_index_data)), axis=-1))\n",
        "\n",
        "          # next_days_pctChg = ohlct[-predict_next_days:,-1].sum()  #wrong\n",
        "          next_days_pctChg = (ohlct[-1,3] - ohlct[-predict_next_days,3]) * 100 / ohlct[-predict_next_days,3]\n",
        "\n",
        "          if next_days_pctChg  <= -edge:\n",
        "            y.append(0)\n",
        "          elif next_days_pctChg  <= 0:\n",
        "            y.append(1)\n",
        "          elif next_days_pctChg <= edge:\n",
        "            y.append(2)\n",
        "          else:\n",
        "            y.append(3)\n",
        "\n",
        "  if len(x) > 0:\n",
        "    X = np.array(x)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # upsample\n",
        "    # dis = district\n",
        "\n",
        "    dis_count, dis_edge = np.histogram(y, bins = 4, range=(0,3))\n",
        "\n",
        "    dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "    dis_count_max = dis_count.max()\n",
        "\n",
        "    time = dis_count_max/dis_count\n",
        "\n",
        "    # time = time.astype(int)\n",
        "\n",
        "    # time_count = np.apply_along_axis((lambda d: time[d.astype(int)]), \\\n",
        "    #                                 0 , y)\n",
        "\n",
        "    # X = np.repeat(X, time_count, axis = 0)\n",
        "    # y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "    # shuffle dataset\n",
        "    if shuffle:\n",
        "      length = X.shape[0]\n",
        "\n",
        "      r_index = np.arange(length)\n",
        "      np.random.shuffle(r_index)\n",
        "\n",
        "      X = X[r_index]\n",
        "      y = y[r_index]\n",
        "\n",
        "    # r_index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "    # X = X[r_index]\n",
        "    # y = y[r_index]\n",
        "\n",
        "    if y_cate:\n",
        "      y = utils.to_categorical(y, num_classes=4)\n",
        "    return X, y, time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqdiohvyhSe1"
      },
      "source": [
        "##make dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czNCx7cYhYWx",
        "outputId": "912805d5-ca62-4c41-f0eb-16d9d8ce619b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pct_of_stock = 1\n",
        "edge = 10.0\n",
        "\n",
        "X1, y1, cls_weight = gntor(result, ts_code[:10], market_index_data, pct_of_stock, edge, [], 1, 0)\n",
        "\n",
        "np.save(\"X-3-1-7-w1000.npy\", X1)\n",
        "np.save(\"y-3-1-7-w1000.npy\", y1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iYKSNjCGwLG"
      },
      "source": [
        "##generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_lTbB-JGzzQ"
      },
      "source": [
        "###G1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZv5MMkQG2MW"
      },
      "source": [
        "def G1(ts_data, ts_code, mark_index_data, pct_of_stock=0.05, edge=10.0,\\\n",
        "        shuffle = 0, y_cate = 1, batch_size=256, generator = 1, repeat_time = 10):  \n",
        "\n",
        "  stock_qty = len(ts_code)\n",
        "\n",
        "  while True:\n",
        "    if pct_of_stock != 1:\n",
        "      index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "      index = ts_code[ts_code.index.isin(index)].code.values\n",
        "    else:\n",
        "      index = ts_code.code.values\n",
        "    \n",
        "    print(len(index))                   \n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in index:\n",
        "      stk_data = ts_data[ts_data[\"code\"] == i]\n",
        "      length = len(stk_data)\n",
        "      if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "        # print(\"0:\", datetime.datetime.today())\n",
        "        for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "          # print(\"1:\", datetime.datetime.today())\n",
        "          ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "              [[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\"]].values.astype(\"float\")\n",
        "          if np.all(ohlct[:,5] != 1) and \\\n",
        "            np.all(ohlct[:,4] != 0) and \\\n",
        "          np.all(ohlct[:,6] >= -10.1) and \\\n",
        "          np.all(ohlct[:,6] <= 10.1) :\n",
        "            # print(\"2:\", datetime.datetime.today())\n",
        "            ohlct[:-predict_next_days,:4] = normals(ohlct[:-predict_next_days,:4])\n",
        "\n",
        "            date_range = stk_data.iloc[l : l + window_size + predict_next_days][\"date\"]\n",
        "            # print(date_range.iloc[0])\n",
        "            # print(type(date_range))\n",
        "\n",
        "            # print(\"2.1:\", datetime.datetime.today())\n",
        "            index_begin = market_index_data[0][market_index_data[0][\"date\"] == date_range.iloc[0]].index.tolist()[0]\n",
        "            # print(\"2.2:\", datetime.datetime.today())\n",
        "            sh_index_data = market_index_data[0][index_begin: index_begin + window_size]\n",
        "            # print(\"2.3:\", datetime.datetime.today())\n",
        "            sh_index_data = sh_index_data[[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "            # print(\"2.4:\", datetime.datetime.today())\n",
        "            sh_index_data = normals(sh_index_data)\n",
        "            # print(\"2.5:\", datetime.datetime.today())\n",
        "\n",
        "            index_begin = market_index_data[1][market_index_data[1][\"date\"] == date_range.iloc[0]].index.tolist()[0]\n",
        "            sz_index_data = market_index_data[1][index_begin: index_begin + window_size]\n",
        "            sz_index_data = sz_index_data[[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "            sz_index_data = normals(sz_index_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # date_range = stk_data.iloc[l : l + window_size + predict_next_days][\"date\"]\n",
        "\n",
        "            # sh_index_data = market_index_data[0][market_index_data[0][\"date\"].isin(date_range)]\n",
        "            # sh_index_data = sh_index_data[:-predict_next_days][[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "            # sh_index_data = normals(sh_index_data)\n",
        "\n",
        "\n",
        "            # sz_index_data = market_index_data[1][market_index_data[1][\"date\"].isin(date_range)]\n",
        "            # sz_index_data = sz_index_data[:-predict_next_days][[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "            # sz_index_data = normals(sz_index_data) \n",
        "\n",
        "            # print(\"3:\", datetime.datetime.today())\n",
        "            x.append(np.expand_dims(np.hstack((ohlct[:-predict_next_days,:4], sh_index_data, sz_index_data)), axis=-1))\n",
        "\n",
        "            # print(\"4:\", datetime.datetime.today())\n",
        "            next_days_pctChg = (ohlct[-1,3] - ohlct[-predict_next_days,3]) * 100 / ohlct[-predict_next_days,3]\n",
        "\n",
        "            if next_days_pctChg  <= -edge:\n",
        "              y.append(0)\n",
        "            elif next_days_pctChg  <= 0:\n",
        "              y.append(1)\n",
        "            elif next_days_pctChg <= edge:\n",
        "              y.append(2)\n",
        "            else:\n",
        "              y.append(3)\n",
        "\n",
        "    if len(x) > 0:\n",
        "      print(len(x), datetime.datetime.today())\n",
        "      X = np.array(x)\n",
        "      y = np.array(y)\n",
        "\n",
        "      # upsample\n",
        "      # dis = district\n",
        "\n",
        "      dis_count, dis_edge = np.histogram(y, bins = 4, range=(0,3))\n",
        "\n",
        "      dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "      dis_count_min = dis_count.min()\n",
        "      print(dis_count_min)\n",
        "\n",
        "      time = dis_count_min/dis_count\n",
        "      print(time)\n",
        "\n",
        "      sample_weight = np.array(y).astype(\"float\")\n",
        "      sample_weight[sample_weight == 0] = time[0]\n",
        "      sample_weight[sample_weight == 1] = time[1]\n",
        "      sample_weight[sample_weight == 2] = time[2]\n",
        "      sample_weight[sample_weight == 3] = time[3]\n",
        "\n",
        "\n",
        "      # time = time.astype(int)\n",
        "\n",
        "      # time_count = np.apply_along_axis((lambda d: time[d.astype(int)]), \\\n",
        "      #                                 0 , y)\n",
        "\n",
        "      # X = np.repeat(X, time_count, axis = 0)\n",
        "      # y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "      # shuffle dataset\n",
        "      if shuffle:\n",
        "        length = X.shape[0]\n",
        "\n",
        "        r_index = np.arange(length)\n",
        "        np.random.shuffle(r_index)\n",
        "\n",
        "        X = X[r_index]\n",
        "        y = y[r_index]\n",
        "\n",
        "      # r_index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "      # X = X[r_index]\n",
        "      # y = y[r_index]\n",
        "\n",
        "      if y_cate:\n",
        "        y = utils.to_categorical(y, num_classes=4)\n",
        "      \n",
        "      if generator == 1:\n",
        "        for ii in range(repeat_time):\n",
        "          for i in range(0, y.shape[0], batch_size):\n",
        "            yield (X[i : i + batch_size], y[i : i + batch_size]\\\n",
        "                  , sample_weight[i : i + batch_size]\\\n",
        "                  )\n",
        "      else:\n",
        "        print(\"0\")\n",
        "        return X, y, sample_weight\n",
        "\n",
        "        \n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzt9JFfbWSMn"
      },
      "source": [
        "###G2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgbmxNZIWWb6"
      },
      "source": [
        "def for_isin(row, index, collection):\n",
        "  return row[index] in collection\n",
        "\n",
        "def G2(ts_data, ts_code, mark_index_data, pct_of_stock=0.05, edge=10.0,\\\n",
        "        shuffle = 0, y_cate = 1, batch_size=256, generator = 1, repeat_time = 10):  \n",
        "\n",
        "  stock_qty = len(ts_code)\n",
        "\n",
        "  # x = None\n",
        "  # y = None\n",
        "  # X = None\n",
        "  # sample_weight = None\n",
        "  # stk_data = None\n",
        "\n",
        "  while True:    \n",
        "\n",
        "\n",
        "    x = None\n",
        "    y = None\n",
        "    X = None\n",
        "    sample_weight = None\n",
        "    stk_data = None\n",
        "    print(\"collect\",gc.collect())\n",
        "\n",
        "    show_mem(gpu)\n",
        "\n",
        "    if pct_of_stock != 1:\n",
        "      index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "      index = ts_code[ts_code.index.isin(index)].code.values\n",
        "    else:\n",
        "      index = ts_code.code.values\n",
        "    \n",
        "    print(len(index))                   \n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in index:\n",
        "      stk_data = ts_data[ts_data[\"code\"] == i]\n",
        "      stk_data = stk_data[[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\", \"date\"]].values\n",
        "      length = len(stk_data)\n",
        "      if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "        # print(\"0:\", datetime.datetime.today())\n",
        "        for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "          # print(\"1:\", datetime.datetime.today())\n",
        "          ohlct = stk_data[l : l + window_size + predict_next_days,:-1].astype(\"float\")\n",
        "          # print(id(ohlct))\n",
        "          # print(id(stk_data[l : l + window_size + predict_next_days,:-1]))\n",
        "          if np.all(ohlct[:,5] != 1) and \\\n",
        "            np.all(ohlct[:,4] != 0) and \\\n",
        "            np.all(ohlct[:,6] >= -10.1) and \\\n",
        "            np.all(ohlct[:,6] <= 10.1) :\n",
        "            # print(\"2:\", datetime.datetime.today())\n",
        "            ohlct[:-predict_next_days,:4] = normals(ohlct[:-predict_next_days,:4])\n",
        "            # print(id(ohlct))\n",
        "            date_range = stk_data[l, 7]\n",
        "            # print(date_range.iloc[0])\n",
        "            # print(type(date_range))\n",
        "\n",
        "            # print(\"2.1:\", datetime.datetime.today())\n",
        "            index_begin = np.where(market_index_data[0][:,-1] == date_range)[0][0]\n",
        "            # print(index_begin)\n",
        "            # print(\"2.2:\", datetime.datetime.today())\n",
        "            sh_index_data = market_index_data[0][index_begin: index_begin + window_size, :]\n",
        "            # sh_index_data = market_index_data[0][np.in1d(market_index_data[0][:,-1],date_range)]\n",
        "            # sh_index_data = market_index_data[0][np.apply_along_axis(for_isin, 1, market_index_data[0], -1, date_range)]\n",
        "            # print(\"2.3:\", datetime.datetime.today())\n",
        "            sh_index_data = sh_index_data[:,:4].astype(\"float\")\n",
        "            # print(\"2.4:\", datetime.datetime.today())\n",
        "            sh_index_data = normals(sh_index_data)\n",
        "            # print(\"2.5:\", datetime.datetime.today())\n",
        "\n",
        "            sz_index_data = market_index_data[1][index_begin: index_begin + window_size, :]\n",
        "            # sz_index_data = market_index_data[1][np.in1d(market_index_data[1][:,-1],date_range)]\n",
        "            sz_index_data = sz_index_data[:,:4].astype(\"float\")\n",
        "            sz_index_data = normals(sz_index_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # date_range = stk_data.iloc[l : l + window_size + predict_next_days][\"date\"]\n",
        "\n",
        "            # sh_index_data = market_index_data[0][market_index_data[0][\"date\"].isin(date_range)]\n",
        "            # sh_index_data = sh_index_data[:-predict_next_days][[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "            # sh_index_data = normals(sh_index_data)\n",
        "\n",
        "\n",
        "            # sz_index_data = market_index_data[1][market_index_data[1][\"date\"].isin(date_range)]\n",
        "            # sz_index_data = sz_index_data[:-predict_next_days][[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "            # sz_index_data = normals(sz_index_data) \n",
        "\n",
        "            # print(\"3:\", datetime.datetime.today())\n",
        "            x.append(np.expand_dims(np.hstack((ohlct[:-predict_next_days,:4], sh_index_data, sz_index_data)), axis=-1))\n",
        "\n",
        "            # print(\"4:\", datetime.datetime.today())\n",
        "            next_days_pctChg = (ohlct[-1,3] - ohlct[-predict_next_days,3]) * 100 / ohlct[-predict_next_days,3]\n",
        "\n",
        "            if next_days_pctChg  <= -edge:\n",
        "              y.append(0)\n",
        "            elif next_days_pctChg  <= 0:\n",
        "              y.append(1)\n",
        "            elif next_days_pctChg <= edge:\n",
        "              y.append(2)\n",
        "            else:\n",
        "              y.append(3)\n",
        "\n",
        "\n",
        "    if len(x) > 0:\n",
        "      print(len(x), datetime.datetime.today())\n",
        "      X = np.array(x)\n",
        "      y = np.array(y)\n",
        "\n",
        "      # upsample\n",
        "      # dis = district\n",
        "\n",
        "      dis_count, dis_edge = np.histogram(y, bins = 4, range=(0,3))\n",
        "\n",
        "      dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "      dis_count_min = dis_count.min()\n",
        "      print(dis_count_min)\n",
        "\n",
        "      time = dis_count_min/dis_count\n",
        "      print(time)\n",
        "\n",
        "      sample_weight = np.array(y).astype(\"float\")\n",
        "      sample_weight[sample_weight == 0] = time[0]\n",
        "      sample_weight[sample_weight == 1] = time[1]\n",
        "      sample_weight[sample_weight == 2] = time[2]\n",
        "      sample_weight[sample_weight == 3] = time[3]\n",
        "\n",
        "\n",
        "      # time = time.astype(int)\n",
        "\n",
        "      # time_count = np.apply_along_axis((lambda d: time[d.astype(int)]), \\\n",
        "      #                                 0 , y)\n",
        "\n",
        "      # X = np.repeat(X, time_count, axis = 0)\n",
        "      # y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "      # shuffle dataset\n",
        "      if shuffle:\n",
        "        length = X.shape[0]\n",
        "\n",
        "        r_index = np.arange(length)\n",
        "        np.random.shuffle(r_index)\n",
        "\n",
        "        X = X[r_index]\n",
        "        y = y[r_index]\n",
        "        sample_weight = sample_weight[r_index]\n",
        "\n",
        "      # r_index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "      # X = X[r_index]\n",
        "      # y = y[r_index]\n",
        "\n",
        "      if y_cate:\n",
        "        y = utils.to_categorical(y, num_classes=4)\n",
        "      \n",
        "      if generator == 1:\n",
        "        set_length = int(y.shape[0]/batch_size) * batch_size\n",
        "        X = X[:set_length]\n",
        "        y = y[:set_length]\n",
        "        sample_weight = sample_weight[:set_length]        \n",
        "        for ii in range(repeat_time):\n",
        "          for i in range(0, set_length, batch_size):\n",
        "            # pppp= 0\n",
        "            yield (X[i : i + batch_size], y[i : i + batch_size]\\\n",
        "                  , sample_weight[i : i + batch_size]\\\n",
        "                  )\n",
        "        \n",
        "      else:\n",
        "        print(\"0\")\n",
        "        # return X, y, sample_weight\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbV86J7K5n1v"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTCPbOqg5sIM"
      },
      "source": [
        "###G2_1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOA3sFLE5sIO"
      },
      "source": [
        "def for_isin(row, index, collection):\n",
        "  return row[index] in collection\n",
        "\n",
        "def running_sign(msg=\"\"):\n",
        "    sign = [\"\\r\\\\\", \"\\r|\", \"\\r/\", \"\\r-\", \"\\r|\", \"\\r/\", \"\\r-\" ]\n",
        "    while True:\n",
        "        for i in range(len(sign)):\n",
        "            print(sign[i], end=msg)\n",
        "            yield i\n",
        "            \n",
        "r = running_sign()\n",
        "\n",
        "def G2_1(ts_data, ts_code, mark_index_data, pct_of_stock=0.05, edge=10.0,\\\n",
        "        shuffle = 0, y_cate = 1, batch_size=256, generator = 1, repeat_time = 10):  \n",
        "\n",
        "  stock_qty = len(ts_code)\n",
        "\n",
        "  # x = None\n",
        "  # y = None\n",
        "  # X = None\n",
        "  # sample_weight = None\n",
        "  # stk_data = None\n",
        "\n",
        "  while True:    \n",
        "\n",
        "\n",
        "    x = None\n",
        "    y = None\n",
        "    X = None\n",
        "    sample_weight = None\n",
        "    stk_data = None\n",
        "    print(\"collect\",gc.collect())\n",
        "\n",
        "    show_mem(gpu)\n",
        "\n",
        "    if pct_of_stock != 1:\n",
        "      index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "      index = ts_code[ts_code.index.isin(index)].code.values\n",
        "    else:\n",
        "      index = ts_code.code.values\n",
        "    \n",
        "    print(len(index))                   \n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in index:\n",
        "      stk_data = ts_data[ts_data[\"code\"] == i]\n",
        "      stk_data = stk_data[[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\", \"date\"]].values\n",
        "      length = len(stk_data)\n",
        "      if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "        # print(\"0:\", datetime.datetime.today())\n",
        "        for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "          # print(\"1:\", datetime.datetime.today())\n",
        "          ohlct = stk_data[l : l + window_size + predict_next_days,:-1].astype(\"float\")\n",
        "          # print(id(ohlct))\n",
        "          # print(id(stk_data[l : l + window_size + predict_next_days,:-1]))\n",
        "          if np.all(ohlct[:,5] != 1) and \\\n",
        "            np.all(ohlct[:,4] != 0) and \\\n",
        "            np.all(ohlct[:,6] >= -10.1) and \\\n",
        "            np.all(ohlct[:,6] <= 10.1) :\n",
        "            # print(\"2:\", datetime.datetime.today())\n",
        "            ohlct[:-predict_next_days,:4] = normals(ohlct[:-predict_next_days,:4])\n",
        "            # print(id(ohlct))\n",
        "            date_range = stk_data[l, 7]\n",
        "            # print(date_range.iloc[0])\n",
        "            # print(type(date_range))\n",
        "\n",
        "            # print(\"2.1:\", datetime.datetime.today())\n",
        "            index_begin = np.where(market_index_data[0][:,-1] == date_range)[0][0]\n",
        "            # print(index_begin)\n",
        "            # print(\"2.2:\", datetime.datetime.today())\n",
        "            sh_index_data = market_index_data[0][index_begin: index_begin + window_size, :]\n",
        "            # sh_index_data = market_index_data[0][np.in1d(market_index_data[0][:,-1],date_range)]\n",
        "            # sh_index_data = market_index_data[0][np.apply_along_axis(for_isin, 1, market_index_data[0], -1, date_range)]\n",
        "            # print(\"2.3:\", datetime.datetime.today())\n",
        "            sh_index_data = sh_index_data[:,:4].astype(\"float\")\n",
        "            # print(\"2.4:\", datetime.datetime.today())\n",
        "            sh_index_data = normals(sh_index_data)\n",
        "            # print(\"2.5:\", datetime.datetime.today())\n",
        "\n",
        "            sz_index_data = market_index_data[1][index_begin: index_begin + window_size, :]\n",
        "            # sz_index_data = market_index_data[1][np.in1d(market_index_data[1][:,-1],date_range)]\n",
        "            sz_index_data = sz_index_data[:,:4].astype(\"float\")\n",
        "            sz_index_data = normals(sz_index_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # date_range = stk_data.iloc[l : l + window_size + predict_next_days][\"date\"]\n",
        "\n",
        "            # sh_index_data = market_index_data[0][market_index_data[0][\"date\"].isin(date_range)]\n",
        "            # sh_index_data = sh_index_data[:-predict_next_days][[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "            # sh_index_data = normals(sh_index_data)\n",
        "\n",
        "\n",
        "            # sz_index_data = market_index_data[1][market_index_data[1][\"date\"].isin(date_range)]\n",
        "            # sz_index_data = sz_index_data[:-predict_next_days][[\"open\", \"high\", \"low\", \"close\"]].values.astype(\"float\")\n",
        "            # sz_index_data = normals(sz_index_data) \n",
        "\n",
        "            # print(\"3:\", datetime.datetime.today())\n",
        "            x.append(np.expand_dims(np.hstack((ohlct[:-predict_next_days,:4], sh_index_data, sz_index_data)), axis=-1))\n",
        "\n",
        "            # print(\"4:\", datetime.datetime.today())\n",
        "            next_days_pctChg = (ohlct[-1,3] - ohlct[-predict_next_days,3]) * 100 / ohlct[-predict_next_days,3]\n",
        "\n",
        "            if next_days_pctChg  <= -edge:\n",
        "              y.append(0)\n",
        "            elif next_days_pctChg  <= 0:\n",
        "              y.append(1)\n",
        "            elif next_days_pctChg <= edge:\n",
        "              y.append(2)\n",
        "            else:\n",
        "              y.append(3)\n",
        "\n",
        "            next(r)\n",
        "\n",
        "    if len(x) > 0:\n",
        "      print(len(x), datetime.datetime.today())\n",
        "      X = np.array(x)\n",
        "      y = np.array(y)\n",
        "\n",
        "      # upsample\n",
        "      # dis = district\n",
        "\n",
        "      dis_count, dis_edge = np.histogram(y, bins = 4, range=(0,3))\n",
        "\n",
        "      dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "      dis_count_min = dis_count.min()\n",
        "      print(dis_count_min)\n",
        "\n",
        "      time = dis_count_min/dis_count\n",
        "      print(time)\n",
        "\n",
        "      sample_weight = np.array(y).astype(\"float\")\n",
        "      sample_weight[sample_weight == 0] = time[0]\n",
        "      sample_weight[sample_weight == 1] = time[1]\n",
        "      sample_weight[sample_weight == 2] = time[2]\n",
        "      sample_weight[sample_weight == 3] = time[3]\n",
        "\n",
        "\n",
        "      # time = time.astype(int)\n",
        "\n",
        "      # time_count = np.apply_along_axis((lambda d: time[d.astype(int)]), \\\n",
        "      #                                 0 , y)\n",
        "\n",
        "      # X = np.repeat(X, time_count, axis = 0)\n",
        "      # y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "      # shuffle dataset\n",
        "      if shuffle:\n",
        "        length = X.shape[0]\n",
        "\n",
        "        r_index = np.arange(length)\n",
        "        np.random.shuffle(r_index)\n",
        "\n",
        "        X = X[r_index]\n",
        "        y = y[r_index]\n",
        "        sample_weight = sample_weight[r_index]\n",
        "\n",
        "      # r_index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "      # X = X[r_index]\n",
        "      # y = y[r_index]\n",
        "\n",
        "      if y_cate:\n",
        "        y = utils.to_categorical(y, num_classes=4)\n",
        "      \n",
        "      if generator == 1:\n",
        "        set_length = int(y.shape[0]/batch_size) * batch_size\n",
        "        X = X[:set_length]\n",
        "        y = y[:set_length]\n",
        "        sample_weight = sample_weight[:set_length]        \n",
        "        for ii in range(repeat_time):\n",
        "          for i in range(0, set_length, batch_size):\n",
        "            pass\n",
        "            # yield (X[i : i + batch_size], y[i : i + batch_size]\\\n",
        "            #       # , sample_weight[i : i + batch_size]\\\n",
        "            #       # )        \n",
        "      else:\n",
        "        print(\"0\")\n",
        "        return X, y, sample_weight\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeVRSkylh3Ve"
      },
      "source": [
        "#test model are at below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQNsLIzGjMhB"
      },
      "source": [
        "##Model 4-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWVVNt95jbHl"
      },
      "source": [
        "def make_model(summary = False):\n",
        "  K.clear_session()\n",
        "  # tf.keras.backend.clear_session()\n",
        "  p_lstm = 256\n",
        "  p_conv = 512\n",
        "\n",
        "  input = layers.Input(shape = (window_size, factor_num, 1))\n",
        "\n",
        "\n",
        "  model_1 = layers.Conv2D(p_conv, kernel_size =(1,factor_num))(input)\n",
        "  model_1 = layers.Activation(\"relu\")(model_1)\n",
        "  model_1 = layers.BatchNormalization()(model_1)\n",
        "  model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "  model_1 = layers.Reshape((-1,p_conv))(model_1)\n",
        "\n",
        "  model_1 = layers.LSTM(p_lstm)(model_1)\n",
        "\n",
        "\n",
        "  model_5 = layers.Conv2D(p_conv, kernel_size =(5,factor_num))(input)\n",
        "  model_5 = layers.Activation(\"relu\")(model_5)\n",
        "  model_5 = layers.BatchNormalization()(model_5)\n",
        "  model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "  model_5 = layers.Reshape((-1,p_conv))(model_5)\n",
        "\n",
        "  model_5 = layers.LSTM(p_lstm)(model_5)\n",
        "\n",
        "  model_10 = layers.Conv2D(p_conv, kernel_size =(10,factor_num))(input)\n",
        "  model_10 = layers.Activation(\"relu\")(model_10)\n",
        "  model_10 = layers.BatchNormalization()(model_10)\n",
        "  model_10 = layers.Dropout(0.5)(model_10)\n",
        "\n",
        "  model_10 = layers.Reshape((-1,p_conv))(model_10)\n",
        "\n",
        "  model_10 = layers.LSTM(p_lstm)(model_10)\n",
        "\n",
        "  model_15 = layers.Conv2D(p_conv, kernel_size =(15,factor_num))(input)\n",
        "  model_15 = layers.Activation(\"relu\")(model_15)\n",
        "  model_15 = layers.BatchNormalization()(model_15)\n",
        "  model_15 = layers.Dropout(0.5)(model_15)\n",
        "\n",
        "  model_15 = layers.Reshape((-1,p_conv))(model_15)\n",
        "\n",
        "  model_15 = layers.LSTM(p_lstm)(model_15)\n",
        "\n",
        "\n",
        "  model_20 = layers.Conv2D(p_conv, kernel_size =(20,factor_num))(input)\n",
        "  model_20 = layers.Activation(\"relu\")(model_20)\n",
        "  model_20 = layers.BatchNormalization()(model_20)\n",
        "  model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "  model_20 = layers.Reshape((-1,p_conv))(model_20)\n",
        "\n",
        "  model_20 = layers.LSTM(p_lstm)(model_20)\n",
        "\n",
        "\n",
        "  model_30 = layers.Conv2D(p_conv, kernel_size =(30,factor_num))(input)\n",
        "  model_30 = layers.Activation(\"relu\")(model_30)\n",
        "  model_30 = layers.BatchNormalization()(model_30)\n",
        "  model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "  model_30 = layers.Reshape((-1,p_conv))(model_30)\n",
        "\n",
        "  model_30 = layers.LSTM(p_lstm)(model_30)\n",
        "\n",
        "  model_45 = layers.Conv2D(p_conv, kernel_size =(45,factor_num))(input)\n",
        "  model_45 = layers.Activation(\"relu\")(model_45)\n",
        "  model_45 = layers.BatchNormalization()(model_45)\n",
        "  model_45 = layers.Dropout(0.5)(model_45)\n",
        "\n",
        "  model_45 = layers.Reshape((-1,p_conv))(model_45)\n",
        "\n",
        "  model_45 = layers.LSTM(p_lstm)(model_45)\n",
        "\n",
        "\n",
        "  model = layers.concatenate([model_1, model_5, model_10,\\\n",
        "              model_15, model_20, model_30, model_45], axis = -1)\n",
        "\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(2048)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "\n",
        "  model = layers.Dense(1024)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(512)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(256)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(128)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(64)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(32)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(4)(model)\n",
        "  model = layers.Activation(\"softmax\")(model)\n",
        "\n",
        "  model = Model(inputs = input, outputs = model)\n",
        "\n",
        "  # opt = optimizers.RMSprop(lr = 0.0005, decay = 0.001)\n",
        "  opt = optimizers.Adam(lr = 0.05, decay = 0.01)\n",
        "\n",
        "  model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics = [\"acc\"])\n",
        "\n",
        "  if summary:\n",
        "    model.summary()\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCxY748jaeSr",
        "outputId": "a99ceab3-5190-47db-89bd-9274e642773c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "make_model(summary = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 90, 12, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 90, 1, 256)   3328        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 86, 1, 256)   15616       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 81, 1, 256)   30976       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 76, 1, 256)   46336       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 71, 1, 256)   61696       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 61, 1, 256)   92416       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 46, 1, 256)   138496      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 90, 1, 256)   0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 86, 1, 256)   0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 81, 1, 256)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 76, 1, 256)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 71, 1, 256)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 61, 1, 256)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 46, 1, 256)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 90, 1, 256)   1024        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 86, 1, 256)   1024        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 81, 1, 256)   1024        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 76, 1, 256)   1024        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 71, 1, 256)   1024        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 61, 1, 256)   1024        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 46, 1, 256)   1024        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 90, 1, 256)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 86, 1, 256)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 81, 1, 256)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 76, 1, 256)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 71, 1, 256)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 61, 1, 256)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 46, 1, 256)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 90, 256)      0           dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 86, 256)      0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 81, 256)      0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 76, 256)      0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_4 (Reshape)             (None, 71, 256)      0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_5 (Reshape)             (None, 61, 256)      0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_6 (Reshape)             (None, 46, 256)      0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 64)           82176       reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 64)           82176       reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 64)           82176       reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 64)           82176       reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   (None, 64)           82176       reshape_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   (None, 64)           82176       reshape_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   (None, 64)           82176       reshape_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 448)          0           lstm[0][0]                       \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_2[0][0]                     \n",
            "                                                                 lstm_3[0][0]                     \n",
            "                                                                 lstm_4[0][0]                     \n",
            "                                                                 lstm_5[0][0]                     \n",
            "                                                                 lstm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 448)          0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 2048)         919552      dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 2048)         0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 2048)         8192        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 2048)         0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1024)         2098176     dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 1024)         0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 1024)         4096        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 1024)         0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 512)          524800      dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 512)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 512)          2048        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 512)          0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 256)          131328      dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 256)          0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 256)          1024        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 256)          0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 128)          32896       dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 128)          0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 128)          512         activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 128)          0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 64)           8256        dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 64)           0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 64)           256         activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 64)           0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 32)           2080        dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32)           0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32)           128         activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 32)           0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 4)            132         dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 4)            0           dense_7[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 4,704,740\n",
            "Trainable params: 4,693,028\n",
            "Non-trainable params: 11,712\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.functional.Functional at 0x7fa8a9195160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4ngNCbgjo-K"
      },
      "source": [
        "###training 4-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTiHoClHj1QF"
      },
      "source": [
        "batch_size = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaNA166bhTuz"
      },
      "source": [
        "xxx = G2(ts_data, ts_code, market_index_data,\\\n",
        "                    pct_of_stock=0.1, edge=10.0,\\\n",
        "                    shuffle = 0, y_cate = 1,\\\n",
        "                    batch_size=batch_size, generator = 0, repeat_time = 1)\n",
        "print(xxx[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O1xCNTKqati",
        "outputId": "66119cba-de9d-4d9f-ddc3-9176395c0aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "xxx[2].min()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4427645788336933"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FegrRqvTkDTm",
        "outputId": "65f28af5-45a7-4f75-de89-088b2b130513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "chk_point = ModelCheckpoint(filepath='stock_predict_4-1-set.h5')\n",
        "\n",
        "for i in range(8):\n",
        "\n",
        "  print(datetime.datetime.today())\n",
        "  model = make_model()\n",
        "  model.load_weights(\"stock_predict_4-1-set.h5\") \n",
        "\n",
        "  #\n",
        "  tracemalloc.start()\n",
        "\n",
        "  for l in range(2):\n",
        "    history = model.fit(G2(ts_data, ts_code, market_index_data,\\\n",
        "                    pct_of_stock=0.1, edge=10.0,\\\n",
        "                    shuffle = 0, y_cate = 1, batch_size=batch_size, repeat_time = 2),\\\n",
        "                    steps_per_epoch = batch_size,\\\n",
        "                    epochs = 10,\\\n",
        "                    callbacks = [chk_point],\\\n",
        "                    # validation_data = G1(ts_data, ts_code, mark_index_data, pct_of_stock=0.1, edge=10.0,\\\n",
        "                    # shuffle = 0, y_cate = 1, batchsize=batch_size)\n",
        "                    )\n",
        "    # history = model.fit(X_t, y_t, batch_size = 1024, epochs = '20,\\\n",
        "    #             # validation_split = 0.1, shuffle = True, \\\n",
        "    #             class_weight = cls_weight,\\\n",
        "    #             validation_data = (X_v ,y_v),\\\n",
        "    #             # callbacks = [tbCallBack]\\\n",
        "    #             callbacks = [chk_point],\\\n",
        "    #             # initial_epoch = 5 \n",
        "    #             ) \n",
        "\n",
        "    model.save_weights(\"stock_predict_4-1-set.h5\")\n",
        "  model.save_weights(\"stock_predict_4-1-set_a.h5\")\n",
        "\n",
        "  del model\n",
        "  gc.collect()\n",
        "  \n",
        "  #\n",
        "  snapshot = tracemalloc.take_snapshot()\n",
        "  top_stats = snapshot.statistics('lineno')\n",
        "  print(\"[ Top 10 ]\")\n",
        "  for stat in top_stats[:10]:\n",
        "    print(stat)\n",
        "    \n",
        "\n",
        "plt.plot(history.history[\"acc\"]) \n",
        "print(datetime.datetime.today())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-85d4d482bd2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchk_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stock_predict_4-1-set.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ModelCheckpoint' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnRHfPlDi-wv",
        "outputId": "3776b44d-880d-41f0-fed2-dd54932ec4c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "show_mem()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 7.3 GB  | Proc size: 24.0 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyavqb98yMQP",
        "outputId": "8c0ccf48-bfeb-437e-c5b2-dadbc3420ec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(sys.getsizeof(G2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kjyFdwcJ1Br"
      },
      "source": [
        "K.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9IE74qsKF_n",
        "outputId": "42b528e6-7706-44da-defe-6beec34cec6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# del x\n",
        "# del y\n",
        "# del X\n",
        "# del sample_weight\n",
        "# del stk_data\n",
        "# del model\n",
        "del history\n",
        "print(\"collect\",gc.collect())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "collect 296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGans-FU6GvB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wopchxFu6IZo"
      },
      "source": [
        "###training 4-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjX5KhEH6IZr"
      },
      "source": [
        "batch_size = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V4qlWxb6IZy"
      },
      "source": [
        "xxx = G2(ts_data, ts_code, market_index_data,\\\n",
        "                    pct_of_stock=0.1, edge=10.0,\\\n",
        "                    shuffle = 0, y_cate = 1,\\\n",
        "                    batch_size=batch_size, generator = 0, repeat_time = 1)\n",
        "print(xxx[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YohJHw66IZ4",
        "outputId": "66119cba-de9d-4d9f-ddc3-9176395c0aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "xxx[2].min()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4427645788336933"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkDHmsi76IZ-",
        "outputId": "7c260fc8-5042-42a3-9f82-512ebc8d9364",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "chk_point = ModelCheckpoint(filepath='stock_predict_4-1-set.h5')\n",
        "csv_logger = CSVLogger(filename='stock_predict_4-1-set.log', separator=',', append=False)\n",
        "running = LambdaCallback(on_train_begin = lambda logs: next(r))\n",
        "\n",
        "print(datetime.datetime.today())\n",
        "model = make_model()\n",
        "# model.load_weights(\"stock_predict_4-1-set.h5\") \n",
        "\n",
        "for i in range(3):\n",
        "\n",
        "  # print(datetime.datetime.today())\n",
        "  # model = make_model()\n",
        "  model.load_weights(\"stock_predict_4-1-set.h5\") \n",
        "\n",
        "  #\n",
        "  tracemalloc.start()\n",
        "\n",
        "  for l in range(2):\n",
        "    X, y, sample_weight = G2_1(ts_data, ts_code, market_index_data,\\\n",
        "                    pct_of_stock=0.1, edge=10.0,\\\n",
        "                    shuffle = 0, y_cate = 1, generator = 0\\\n",
        "                    #  batch_size=batch_size, repeat_time = 2\\\n",
        "                     )\n",
        "    history = model.fit(X, y, sample_weight = sample_weight,\\\n",
        "                    # steps_per_epoch = batch_size,\\\n",
        "                    epochs = 20,\\\n",
        "                    verbose = 2,\\\n",
        "                    callbacks = [chk_point, csv_logger, running],\\\n",
        "                    batch_size = batch_size,\n",
        "                    # validation_data = G1(ts_data, ts_code, mark_index_data, pct_of_stock=0.1, edge=10.0,\\\n",
        "                    # shuffle = 0, y_cate = 1, batchsize=batch_size)\n",
        "                    validation_split = 0.1\n",
        "                    )\n",
        "    # history = model.fit(X_t, y_t, batch_size = 1024, epochs = '20,\\\n",
        "    #             # validation_split = 0.1, shuffle = True, \\\n",
        "    #             class_weight = cls_weight,\\\n",
        "    #             validation_data = (X_v ,y_v),\\\n",
        "    #             # callbacks = [tbCallBack]\\\n",
        "    #             callbacks = [chk_point],\\\n",
        "    #             # initial_epoch = 5 \n",
        "    #             ) \n",
        "\n",
        "    model.save_weights(\"stock_predict_4-1-set.h5\")\n",
        "  model.save_weights(\"stock_predict_4-1-set_a.h5\")\n",
        "\n",
        "  # del model\n",
        "  # gc.collect()\n",
        "  \n",
        "  #\n",
        "  snapshot = tracemalloc.take_snapshot()\n",
        "  top_stats = snapshot.statistics('lineno')\n",
        "  print(\"[ Top 10 ]\")\n",
        "  for stat in top_stats[:10]:\n",
        "    print(stat)\n",
        "    \n",
        "\n",
        "plt.plot(history.history[\"acc\"]) \n",
        "print(datetime.datetime.today())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-01 05:28:50.400017\n",
            "collect 0\n",
            "Gen RAM Free: 22.6 GB  | Proc size: 13.8 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "-349198 2020-11-01 05:36:21.001816\n",
            "57051.0\n",
            "[1.         0.49088797 0.52090904 0.85908536]\n",
            "0\n",
            "\\Epoch 1/20\n",
            "1228/1228 - 243s - loss: 0.6650 - acc: 0.4564 - val_loss: 0.6568 - val_acc: 0.4780\n",
            "Epoch 2/20\n",
            "1228/1228 - 234s - loss: 0.6444 - acc: 0.4790 - val_loss: 0.6519 - val_acc: 0.4795\n",
            "Epoch 3/20\n",
            "1228/1228 - 234s - loss: 0.6382 - acc: 0.4863 - val_loss: 0.6492 - val_acc: 0.4830\n",
            "Epoch 4/20\n",
            "1228/1228 - 233s - loss: 0.6352 - acc: 0.4909 - val_loss: 0.6479 - val_acc: 0.4820\n",
            "Epoch 5/20\n",
            "1228/1228 - 233s - loss: 0.6331 - acc: 0.4932 - val_loss: 0.6476 - val_acc: 0.4859\n",
            "Epoch 6/20\n",
            "1228/1228 - 234s - loss: 0.6314 - acc: 0.4938 - val_loss: 0.6454 - val_acc: 0.4885\n",
            "Epoch 7/20\n",
            "1228/1228 - 233s - loss: 0.6305 - acc: 0.4957 - val_loss: 0.6471 - val_acc: 0.4882\n",
            "Epoch 8/20\n",
            "1228/1228 - 234s - loss: 0.6288 - acc: 0.4965 - val_loss: 0.6467 - val_acc: 0.4864\n",
            "Epoch 9/20\n",
            "1228/1228 - 234s - loss: 0.6286 - acc: 0.4976 - val_loss: 0.6476 - val_acc: 0.4891\n",
            "Epoch 10/20\n",
            "1228/1228 - 233s - loss: 0.6277 - acc: 0.4978 - val_loss: 0.6473 - val_acc: 0.4902\n",
            "Epoch 11/20\n",
            "1228/1228 - 233s - loss: 0.6275 - acc: 0.4985 - val_loss: 0.6468 - val_acc: 0.4893\n",
            "Epoch 12/20\n",
            "1228/1228 - 234s - loss: 0.6265 - acc: 0.4995 - val_loss: 0.6469 - val_acc: 0.4900\n",
            "Epoch 13/20\n",
            "1228/1228 - 234s - loss: 0.6260 - acc: 0.4997 - val_loss: 0.6455 - val_acc: 0.4892\n",
            "Epoch 14/20\n",
            "1228/1228 - 234s - loss: 0.6259 - acc: 0.4999 - val_loss: 0.6454 - val_acc: 0.4898\n",
            "Epoch 15/20\n",
            "1228/1228 - 234s - loss: 0.6257 - acc: 0.5002 - val_loss: 0.6455 - val_acc: 0.4902\n",
            "Epoch 16/20\n",
            "1228/1228 - 234s - loss: 0.6252 - acc: 0.5011 - val_loss: 0.6456 - val_acc: 0.4903\n",
            "Epoch 17/20\n",
            "1228/1228 - 234s - loss: 0.6251 - acc: 0.5002 - val_loss: 0.6453 - val_acc: 0.4903\n",
            "Epoch 18/20\n",
            "1228/1228 - 234s - loss: 0.6242 - acc: 0.5013 - val_loss: 0.6451 - val_acc: 0.4906\n",
            "Epoch 19/20\n",
            "1228/1228 - 234s - loss: 0.6243 - acc: 0.5015 - val_loss: 0.6446 - val_acc: 0.4898\n",
            "Epoch 20/20\n",
            "1228/1228 - 234s - loss: 0.6242 - acc: 0.5014 - val_loss: 0.6443 - val_acc: 0.4893\n",
            "collect 1612\n",
            "Gen RAM Free: 19.2 GB  | Proc size: 16.8 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "/339358 2020-11-01 07:02:35.407939\n",
            "55218.0\n",
            "[1.         0.47571787 0.52299678 0.88361524]\n",
            "0\n",
            "-Epoch 1/20\n",
            "1194/1194 - 234s - loss: 0.6160 - acc: 0.5082 - val_loss: 0.6383 - val_acc: 0.4823\n",
            "Epoch 2/20\n",
            "1194/1194 - 227s - loss: 0.6155 - acc: 0.5081 - val_loss: 0.6378 - val_acc: 0.4827\n",
            "Epoch 3/20\n",
            "1194/1194 - 227s - loss: 0.6156 - acc: 0.5092 - val_loss: 0.6372 - val_acc: 0.4826\n",
            "Epoch 4/20\n",
            "1194/1194 - 227s - loss: 0.6150 - acc: 0.5100 - val_loss: 0.6370 - val_acc: 0.4842\n",
            "Epoch 5/20\n",
            "1194/1194 - 227s - loss: 0.6138 - acc: 0.5082 - val_loss: 0.6371 - val_acc: 0.4850\n",
            "Epoch 6/20\n",
            "1194/1194 - 227s - loss: 0.6141 - acc: 0.5095 - val_loss: 0.6368 - val_acc: 0.4846\n",
            "Epoch 7/20\n",
            "1194/1194 - 228s - loss: 0.6132 - acc: 0.5110 - val_loss: 0.6361 - val_acc: 0.4847\n",
            "Epoch 8/20\n",
            "1194/1194 - 227s - loss: 0.6137 - acc: 0.5102 - val_loss: 0.6366 - val_acc: 0.4851\n",
            "Epoch 9/20\n",
            "1194/1194 - 227s - loss: 0.6129 - acc: 0.5104 - val_loss: 0.6359 - val_acc: 0.4846\n",
            "Epoch 10/20\n",
            "1194/1194 - 227s - loss: 0.6131 - acc: 0.5104 - val_loss: 0.6358 - val_acc: 0.4846\n",
            "Epoch 11/20\n",
            "1194/1194 - 227s - loss: 0.6127 - acc: 0.5095 - val_loss: 0.6354 - val_acc: 0.4855\n",
            "Epoch 12/20\n",
            "1194/1194 - 227s - loss: 0.6124 - acc: 0.5115 - val_loss: 0.6357 - val_acc: 0.4852\n",
            "Epoch 13/20\n",
            "1194/1194 - 227s - loss: 0.6126 - acc: 0.5112 - val_loss: 0.6348 - val_acc: 0.4857\n",
            "Epoch 14/20\n",
            "1194/1194 - 228s - loss: 0.6119 - acc: 0.5117 - val_loss: 0.6346 - val_acc: 0.4853\n",
            "Epoch 15/20\n",
            "1194/1194 - 227s - loss: 0.6117 - acc: 0.5118 - val_loss: 0.6350 - val_acc: 0.4854\n",
            "Epoch 16/20\n",
            "1194/1194 - 228s - loss: 0.6117 - acc: 0.5121 - val_loss: 0.6344 - val_acc: 0.4856\n",
            "Epoch 17/20\n",
            "1194/1194 - 227s - loss: 0.6117 - acc: 0.5110 - val_loss: 0.6340 - val_acc: 0.4857\n",
            "Epoch 18/20\n",
            "1194/1194 - 227s - loss: 0.6117 - acc: 0.5119 - val_loss: 0.6341 - val_acc: 0.4861\n",
            "Epoch 19/20\n",
            "1194/1194 - 227s - loss: 0.6114 - acc: 0.5118 - val_loss: 0.6341 - val_acc: 0.4865\n",
            "Epoch 20/20\n",
            "1194/1194 - 227s - loss: 0.6116 - acc: 0.5113 - val_loss: 0.6339 - val_acc: 0.4863\n",
            "[ Top 10 ]\n",
            "<ipython-input-9-cf33a72c82f4>:117: size=5548 MiB, count=5, average=1110 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:98: size=2777 MiB, count=56, average=49.6 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/range.py:164: size=58.0 MiB, count=4, average=14.5 MiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1983: size=20.8 MiB, count=37843, average=576 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:382: size=12.6 MiB, count=47437, average=279 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/np_utils.py:77: size=10.3 MiB, count=4, average=2630 KiB\n",
            "<ipython-input-9-cf33a72c82f4>:66: size=10.3 MiB, count=244638, average=44 B\n",
            "<ipython-input-9-cf33a72c82f4>:132: size=5261 KiB, count=4, average=1315 KiB\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_stack.py:153: size=4844 KiB, count=81917, average=61 B\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:5061: size=3817 KiB, count=23265, average=168 B\n",
            "collect 1641\n",
            "Gen RAM Free: 19.2 GB  | Proc size: 19.7 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "\\339074 2020-11-01 08:26:28.399919\n",
            "55815.0\n",
            "[1.         0.48459355 0.53103534 0.88626187]\n",
            "0\n",
            "|Epoch 1/20\n",
            "1193/1193 - 233s - loss: 0.6211 - acc: 0.5097 - val_loss: 0.6562 - val_acc: 0.4760\n",
            "Epoch 2/20\n",
            "1193/1193 - 227s - loss: 0.6209 - acc: 0.5092 - val_loss: 0.6560 - val_acc: 0.4762\n",
            "Epoch 3/20\n",
            "1193/1193 - 227s - loss: 0.6204 - acc: 0.5096 - val_loss: 0.6562 - val_acc: 0.4768\n",
            "Epoch 4/20\n",
            "1193/1193 - 227s - loss: 0.6207 - acc: 0.5087 - val_loss: 0.6563 - val_acc: 0.4776\n",
            "Epoch 5/20\n",
            "1193/1193 - 227s - loss: 0.6202 - acc: 0.5106 - val_loss: 0.6568 - val_acc: 0.4778\n",
            "Epoch 6/20\n",
            "1193/1193 - 228s - loss: 0.6193 - acc: 0.5105 - val_loss: 0.6572 - val_acc: 0.4771\n",
            "Epoch 7/20\n",
            "1193/1193 - 227s - loss: 0.6195 - acc: 0.5103 - val_loss: 0.6569 - val_acc: 0.4780\n",
            "Epoch 8/20\n",
            "1193/1193 - 227s - loss: 0.6195 - acc: 0.5106 - val_loss: 0.6577 - val_acc: 0.4768\n",
            "Epoch 9/20\n",
            "1193/1193 - 227s - loss: 0.6199 - acc: 0.5091 - val_loss: 0.6576 - val_acc: 0.4781\n",
            "Epoch 10/20\n",
            "1193/1193 - 227s - loss: 0.6196 - acc: 0.5105 - val_loss: 0.6575 - val_acc: 0.4776\n",
            "Epoch 11/20\n",
            "1193/1193 - 226s - loss: 0.6191 - acc: 0.5109 - val_loss: 0.6575 - val_acc: 0.4778\n",
            "Epoch 12/20\n",
            "1193/1193 - 227s - loss: 0.6189 - acc: 0.5109 - val_loss: 0.6581 - val_acc: 0.4778\n",
            "Epoch 13/20\n",
            "1193/1193 - 227s - loss: 0.6189 - acc: 0.5106 - val_loss: 0.6581 - val_acc: 0.4769\n",
            "Epoch 14/20\n",
            "1193/1193 - 227s - loss: 0.6182 - acc: 0.5103 - val_loss: 0.6579 - val_acc: 0.4775\n",
            "Epoch 15/20\n",
            "1193/1193 - 227s - loss: 0.6189 - acc: 0.5108 - val_loss: 0.6580 - val_acc: 0.4774\n",
            "Epoch 16/20\n",
            "1193/1193 - 227s - loss: 0.6182 - acc: 0.5109 - val_loss: 0.6585 - val_acc: 0.4776\n",
            "Epoch 17/20\n",
            "1193/1193 - 227s - loss: 0.6183 - acc: 0.5107 - val_loss: 0.6586 - val_acc: 0.4769\n",
            "Epoch 18/20\n",
            "1193/1193 - 227s - loss: 0.6180 - acc: 0.5103 - val_loss: 0.6587 - val_acc: 0.4771\n",
            "Epoch 19/20\n",
            "1193/1193 - 227s - loss: 0.6187 - acc: 0.5097 - val_loss: 0.6589 - val_acc: 0.4773\n",
            "Epoch 20/20\n",
            "1193/1193 - 227s - loss: 0.6178 - acc: 0.5123 - val_loss: 0.6584 - val_acc: 0.4770\n",
            "collect 1641\n",
            "Gen RAM Free: 22.0 GB  | Proc size: 17.9 GB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n",
            "363\n",
            "|347319 2020-11-01 09:50:12.285689\n",
            "55847.0\n",
            "[1.         0.47376145 0.51002758 0.87127523]\n",
            "0\n",
            "/Epoch 1/20\n",
            "1222/1222 - 238s - loss: 0.6058 - acc: 0.5060 - val_loss: 0.6340 - val_acc: 0.4950\n",
            "Epoch 2/20\n",
            "1222/1222 - 232s - loss: 0.6052 - acc: 0.5075 - val_loss: 0.6338 - val_acc: 0.4955\n",
            "Epoch 3/20\n",
            "1222/1222 - 233s - loss: 0.6055 - acc: 0.5068 - val_loss: 0.6340 - val_acc: 0.4943\n",
            "Epoch 4/20\n",
            "1222/1222 - 233s - loss: 0.6052 - acc: 0.5077 - val_loss: 0.6335 - val_acc: 0.4939\n",
            "Epoch 5/20\n",
            "1222/1222 - 233s - loss: 0.6046 - acc: 0.5077 - val_loss: 0.6340 - val_acc: 0.4942\n",
            "Epoch 6/20\n",
            "1222/1222 - 232s - loss: 0.6045 - acc: 0.5088 - val_loss: 0.6341 - val_acc: 0.4946\n",
            "Epoch 7/20\n",
            "1222/1222 - 233s - loss: 0.6046 - acc: 0.5084 - val_loss: 0.6338 - val_acc: 0.4945\n",
            "Epoch 8/20\n",
            "1222/1222 - 232s - loss: 0.6042 - acc: 0.5078 - val_loss: 0.6337 - val_acc: 0.4939\n",
            "Epoch 9/20\n",
            "1222/1222 - 233s - loss: 0.6040 - acc: 0.5084 - val_loss: 0.6342 - val_acc: 0.4937\n",
            "Epoch 10/20\n",
            "1222/1222 - 233s - loss: 0.6037 - acc: 0.5084 - val_loss: 0.6341 - val_acc: 0.4944\n",
            "Epoch 11/20\n",
            "1222/1222 - 232s - loss: 0.6041 - acc: 0.5092 - val_loss: 0.6340 - val_acc: 0.4940\n",
            "Epoch 12/20\n",
            "1222/1222 - 233s - loss: 0.6040 - acc: 0.5084 - val_loss: 0.6339 - val_acc: 0.4942\n",
            "Epoch 13/20\n",
            "1222/1222 - 233s - loss: 0.6038 - acc: 0.5081 - val_loss: 0.6339 - val_acc: 0.4931\n",
            "Epoch 14/20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y74_fyv6IaH",
        "outputId": "3776b44d-880d-41f0-fed2-dd54932ec4c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "show_mem()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 7.3 GB  | Proc size: 24.0 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3iBNhnu6IaN",
        "outputId": "8c0ccf48-bfeb-437e-c5b2-dadbc3420ec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(sys.getsizeof(G2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0ySLFLn6IaR"
      },
      "source": [
        "K.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMr60z-I6IaV",
        "outputId": "42b528e6-7706-44da-defe-6beec34cec6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# del x\n",
        "# del y\n",
        "# del X\n",
        "# del sample_weight\n",
        "# del stk_data\n",
        "# del model\n",
        "del history\n",
        "print(\"collect\",gc.collect())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "collect 296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxNoc_9giERB"
      },
      "source": [
        "#Model 1: CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p6l2wfbHXZB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPFa4FEWkCFH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7l4RbsFikd4"
      },
      "source": [
        "##model 1-1: CNN + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1lW5hjDiTBy",
        "outputId": "bbc51a7d-70e2-4137-a1af-c16576ee1336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "input = layers.Input(shape = (window_size, factor_num - 2 ,1))\n",
        "\n",
        "model = layers.Conv2D(32, kernel_size = 4)(input)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Reshape((-1, 32))(model)\n",
        "\n",
        "model = layers.Conv1D(32, kernel_size = 5)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Conv1D(64, kernel_size = 5)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Conv1D(256, kernel_size = 5)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Conv1D(512, kernel_size = 5)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.LSTM(64)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(32)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(1)(model)\n",
        "\n",
        "model = Model(inputs = input, outputs = model)\n",
        "\n",
        "opt = optimizers.RMSprop(lr = 0.001, decay = 0.001)\n",
        "\n",
        "model.compile(loss = \"mse\", optimizer = opt)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-965e3ecc17cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor_num\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'K' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x17JDv8Wi12w"
      },
      "source": [
        "###dataset 1-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvM6qL0EiQln"
      },
      "source": [
        "\n",
        "pct_of_stock  = 0.004\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "\n",
        "print(np.sort(index))\n",
        "                      \n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in index:\n",
        "  stk_data = result[result.code == ts_code.iloc[i][\"code\"]]\n",
        "  length = len(stk_data)\n",
        "  if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "    for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "      ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "      if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-1,:-2].max()\n",
        "        min_price = ohlct[:-1,:-2].min()\n",
        "        scale = max_price - min_price\n",
        "        ohlct[:-1,:-2] = ([ohlct[:-1,:-2] - min_price]) /scale\n",
        "\n",
        "        x.extend([np.expand_dims(ohlct[:-predict_next_days,:-2], axis=-1)])\n",
        "\n",
        "        # y.extend([utils.to_categorical(ohlct[-1,-1], num_classes=201)])\n",
        "        ohlct[-1,-1] = (ohlct[-1,-1] + 10.0) / 20.0\n",
        "        y.extend([ohlct[-1,-1]])\n",
        "\n",
        "\n",
        "\n",
        "X = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "dis_count, dis_edge = np.histogram(y, bins = 20)\n",
        "\n",
        "dis_count_max = dis_count.max() * 4\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[(d*100//5).astype(int)]), \\\n",
        "                                0 , y)\n",
        "\n",
        "X = np.repeat(X, time_count, axis = 0)\n",
        "y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "length = X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "print(X.shape)\n",
        "print(X[1])\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUmQNnUyjYPO"
      },
      "source": [
        "###draw data distribution 1-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDQEhrWGjW0p"
      },
      "source": [
        "plt.hist(y, bins = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llpn-tNzjwO3"
      },
      "source": [
        "###training 1-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QIWpgbdj797"
      },
      "source": [
        "# model.load_weights(\"stock_predict_3_1-1.h5\")\n",
        "\n",
        "history = model.fit(X, y, batch_size = 1024, epochs = 240,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "# plt.plot(history.history[\"acc\"])\n",
        "plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(\"stock_predict_3_1-1.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzFRlwZzkAfw"
      },
      "source": [
        "###drawing validation 1-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v9SqPo8kIxb"
      },
      "source": [
        "# model.load_weights(\"stock_predict_3_1-1.h5\")\n",
        "\n",
        "history = model.fit(X, y, batch_size = 1024, epochs = 240,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "# plt.plot(history.history[\"acc\"])\n",
        "plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(\"stock_predict_3_1-1.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLL6GnFjyQW-"
      },
      "source": [
        "##model 1-2: CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCtdPQjVyEzG"
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "input = layers.Input(shape = (window_size, factor_num - 1 ,1))\n",
        "\n",
        "\n",
        "model_1 = layers.Conv2D(32, kernel_size =(1,4))(input)\n",
        "model_1 = layers.Activation(\"relu\")(model_1)\n",
        "model_1 = layers.BatchNormalization()(model_1)\n",
        "model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "model_1 = layers.Reshape((-1, 32))(model_1)\n",
        "\n",
        "# model_1 = layers.Conv1D(32, kernel_size = 5)(model_1)\n",
        "# model_1 = layers.Activation(\"relu\")(model_1)\n",
        "# model_1 = layers.BatchNormalization()(model_1)\n",
        "# model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "# model_1 = layers.Conv1D(64, kernel_size = 5)(model_1)\n",
        "# model_1 = layers.Activation(\"relu\")(model_1)\n",
        "# model_1 = layers.BatchNormalization()(model_1)\n",
        "# model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "# model_1 = layers.Conv1D(128, kernel_size = 5)(model_1)\n",
        "# model_1 = layers.Activation(\"relu\")(model_1)\n",
        "# model_1 = layers.BatchNormalization()(model_1)\n",
        "# model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "\n",
        "model_5 = layers.Conv2D(32, kernel_size =(5,4))(input)\n",
        "model_5 = layers.Activation(\"relu\")(model_5)\n",
        "model_5 = layers.BatchNormalization()(model_5)\n",
        "model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "model_5 = layers.Reshape((-1, 32))(model_5)\n",
        "\n",
        "# model_5 = layers.Conv1D(64, kernel_size = 5)(model_5)\n",
        "# model_5 = layers.Activation(\"relu\")(model_5)\n",
        "# model_5 = layers.BatchNormalization()(model_5)\n",
        "# model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "# model_5 = layers.Conv1D(128, kernel_size = 5)(model_5)\n",
        "# model_5 = layers.Activation(\"relu\")(model_5)\n",
        "# model_5 = layers.BatchNormalization()(model_5)\n",
        "# model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "# model_5 = layers.Conv1D(256, kernel_size = 5)(model_5)\n",
        "# model_5 = layers.Activation(\"relu\")(model_5)\n",
        "# model_5 = layers.BatchNormalization()(model_5)\n",
        "# model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "\n",
        "model_20 = layers.Conv2D(32, kernel_size =(20,4))(input)\n",
        "model_20 = layers.Activation(\"relu\")(model_20)\n",
        "model_20 = layers.BatchNormalization()(model_20)\n",
        "model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "model_20 = layers.Reshape((-1, 32))(model_20)\n",
        "\n",
        "# model_20 = layers.Conv1D(64, kernel_size = 20)(model_20)\n",
        "# model_20 = layers.Activation(\"relu\")(model_20)\n",
        "# model_20 = layers.BatchNormalization()(model_20)\n",
        "# model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "# model_20 = layers.Conv1D(128, kernel_size = 20)(model_20)\n",
        "# model_20 = layers.Activation(\"relu\")(model_20)\n",
        "# model_20 = layers.BatchNormalization()(model_20)\n",
        "# model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "# model_20 = layers.Conv1D(256, kernel_size = 20)(model_20)\n",
        "# model_20 = layers.Activation(\"relu\")(model_20)\n",
        "# model_20 = layers.BatchNormalization()(model_20)\n",
        "# model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "\n",
        "model_30 = layers.Conv2D(32, kernel_size =(30,4))(input)\n",
        "model_30 = layers.Activation(\"relu\")(model_30)\n",
        "model_30 = layers.BatchNormalization()(model_30)\n",
        "model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "model_30 = layers.Reshape((-1, 32))(model_30)\n",
        "\n",
        "# model_30 = layers.Conv1D(64, kernel_size = 30)(model_30)\n",
        "# model_30 = layers.Activation(\"relu\")(model_30)\n",
        "# model_30 = layers.BatchNormalization()(model_30)\n",
        "# model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "# # model_30 = layers.Conv1D(64, kernel_size = 30)(model_30)\n",
        "# # model_30 = layers.Activation(\"relu\")(model_30)\n",
        "# # model_30 = layers.BatchNormalization()(model_30)\n",
        "# # model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "# model_30 = layers.Conv1D(256, kernel_size = 30)(model_30)\n",
        "# model_30 = layers.Activation(\"relu\")(model_30)\n",
        "# model_30 = layers.BatchNormalization()(model_30)\n",
        "# model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "\n",
        "\n",
        "model = layers.concatenate([model_1, model_5, model_20, model_30], axis = -2)\n",
        "\n",
        "model = layers.Flatten()(model)\n",
        "\n",
        "model = layers.Dense(64)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(32)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(1)(model)\n",
        "\n",
        "model = Model(inputs = input, outputs = model)\n",
        "\n",
        "opt = optimizers.RMSprop(lr = 0.001, decay = 0.001)\n",
        "\n",
        "model.compile(loss = \"mse\", optimizer = opt)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHbZzoodbpC"
      },
      "source": [
        "###dataset 1-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAF9GL6Ydnas"
      },
      "source": [
        "pct_of_stock  = 0.004\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "\n",
        "print(np.sort(index))\n",
        "                      \n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in index:\n",
        "  stk_data = result[result.code == ts_code.iloc[i][\"code\"]]\n",
        "  length = len(stk_data)\n",
        "  if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "    for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "      ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "      if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "        min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "        scale = max_price - min_price\n",
        "        ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "        x.append(np.expand_dims(ohlct[:-predict_next_days,:-2], axis=-1))\n",
        "\n",
        "        # y.extend([utils.to_categorical(ohlct[-1,-1], num_classes=201)])\n",
        "        ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "        y.append(ohlct[-predict_next_days,-1])\n",
        "\n",
        "\n",
        "\n",
        "X = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "dis_count, dis_edge = np.histogram(y, bins = 20)\n",
        "\n",
        "dis_count_max = dis_count.max() * 4\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[(d*100//5).astype(int)]), \\\n",
        "                                0 , y)\n",
        "\n",
        "X = np.repeat(X, time_count, axis = 0)\n",
        "y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "length = X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "print(X.shape)\n",
        "print(X[1])\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgdFoC0ZqnOx"
      },
      "source": [
        "###draw data distribution 1-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMVjP76Mqonw"
      },
      "source": [
        "plt.hist(y, bins = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycLBn2g9eJoz"
      },
      "source": [
        "###training 1-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqCxwMbQeMqw"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-2.h5\")\n",
        "\n",
        "history = model.fit(X, y, batch_size = 1024, epochs = 16,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "# plt.plot(history.history[\"acc\"])\n",
        "plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(\"stock_predict_3_1-2.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMaIpUCXdKAd"
      },
      "source": [
        "###drawing validation 1-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N17j1OcleJko"
      },
      "source": [
        "\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = np.random.randint(0, stock_qty)\n",
        "\n",
        "                      \n",
        "x_plot = []\n",
        "y_plot = []\n",
        "yy_plot = []\n",
        "\n",
        "stk_data = result[result.code == ts_code.iloc[index][\"code\"]]\n",
        "length = len(stk_data)\n",
        "if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "  for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "    ohlct = stk_data.iloc[l : l + window_size + predict_next_days] \\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"isST\",\"pctChg\"]].values\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "      min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "      max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "      scale = (max_price - min_price)\n",
        "      ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:-predict_next_days,:-2], axis = -1)])\n",
        "\n",
        "      # ohlct[-1,-1] = (ohlct[-1,-1] + 10.0) / 20.0\n",
        "      y_plot.extend([ohlct[-predict_next_days,-1]])\n",
        "\n",
        "  X_plot = np.array(x_plot)\n",
        "  y_plot = np.array(y_plot)\n",
        "  # yy_plot = np.array(yy_plot)\n",
        "  print(ts_code.iloc[index])    \n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Kbry9q0dOgQ"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-2.h5\")\n",
        "p = model.predict(X_plot).reshape(-1)\n",
        "\n",
        "p = p * 20.0 - 10.0\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot[-100:], color=\"blue\")\n",
        "plt.plot(p[-100:], color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpCX5_PRvWDq"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-2.h5\")\n",
        "model.evaluate(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDVfkKLHxD1a"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-2.h5\")\n",
        "model.evaluate(X_plot, y_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6hsUOdJDczM"
      },
      "source": [
        "##model 1-3: CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5BAsBF6DczY"
      },
      "source": [
        "def make_model(summary = False):\n",
        "  K.clear_session()\n",
        "\n",
        "  input = layers.Input(shape = (window_size, factor_num - 1 ,1))\n",
        "\n",
        "\n",
        "  model_1 = layers.Conv2D(256, kernel_size =(1,4))(input)\n",
        "  model_1 = layers.Activation(\"relu\")(model_1)\n",
        "  model_1 = layers.BatchNormalization()(model_1)\n",
        "  model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "  model_1 = layers.Reshape((-1, 256))(model_1)\n",
        "\n",
        "\n",
        "\n",
        "  model_5 = layers.Conv2D(256, kernel_size =(5,4))(input)\n",
        "  model_5 = layers.Activation(\"relu\")(model_5)\n",
        "  model_5 = layers.BatchNormalization()(model_5)\n",
        "  model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "  model_5 = layers.Reshape((-1, 256))(model_5)\n",
        "\n",
        "\n",
        "\n",
        "  model_20 = layers.Conv2D(256, kernel_size =(20,4))(input)\n",
        "  model_20 = layers.Activation(\"relu\")(model_20)\n",
        "  model_20 = layers.BatchNormalization()(model_20)\n",
        "  model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "  model_20 = layers.Reshape((-1, 256))(model_20)\n",
        "\n",
        "\n",
        "\n",
        "  model_30 = layers.Conv2D(256, kernel_size =(30,4))(input)\n",
        "  model_30 = layers.Activation(\"relu\")(model_30)\n",
        "  model_30 = layers.BatchNormalization()(model_30)\n",
        "  model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "  model_30 = layers.Reshape((-1, 256))(model_30)\n",
        "\n",
        "\n",
        "  model_60 = layers.Conv2D(256, kernel_size =(60,4))(input)\n",
        "  model_60 = layers.Activation(\"relu\")(model_60)\n",
        "  model_60 = layers.BatchNormalization()(model_60)\n",
        "  model_60 = layers.Dropout(0.5)(model_60)\n",
        "\n",
        "  model_60 = layers.Reshape((-1, 256))(model_60)\n",
        "\n",
        "\n",
        "  model_90 = layers.Conv2D(256, kernel_size =(90,4))(input)\n",
        "  model_90 = layers.Activation(\"relu\")(model_90)\n",
        "  model_90 = layers.BatchNormalization()(model_90)\n",
        "  model_90 = layers.Dropout(0.5)(model_90)\n",
        "\n",
        "  model_90 = layers.Reshape((-1, 256))(model_90)\n",
        "\n",
        "\n",
        "\n",
        "  model = layers.concatenate([model_1, model_5, model_20, model_30, model_60, model_90], axis = -2)\n",
        "\n",
        "  model = layers.Flatten()(model)\n",
        "\n",
        "  # model = layers.Dense(64)(model)\n",
        "  # model = layers.Activation(\"relu\")(model)\n",
        "  # model = layers.BatchNormalization()(model)\n",
        "  # model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(32)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "\n",
        "  model = layers.Dense(16)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  # model = layers.Dense(1)(model)\n",
        "\n",
        "  model = layers.Dense(4)(model)\n",
        "  model = layers.Activation(\"softmax\")(model)\n",
        "\n",
        "  model = Model(inputs = input, outputs = model)\n",
        "\n",
        "  # opt = optimizers.RMSprop(lr = 0.001, decay = 0.001)\n",
        "  opt = optimizers.Adam(lr = 0.005, decay = 0.001)\n",
        "\n",
        "  # model.compile(loss = \"mse\", optimizer = opt)\n",
        "  model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics = [\"acc\"])\n",
        "\n",
        "  if(summary == True):\n",
        "    model.summary()\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3AUWCwiDczo"
      },
      "source": [
        "###dataset 1-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0xCITARDczr"
      },
      "source": [
        "pct_of_stock  = 0.004\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "\n",
        "print(np.sort(index))\n",
        "                      \n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in index:\n",
        "  stk_data = result[result.code == ts_code.iloc[i][\"code\"]]\n",
        "  length = len(stk_data)\n",
        "  if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "    for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "      ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "      if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "        min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "        scale = max_price - min_price\n",
        "        ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "        x.append(np.expand_dims(ohlct[:-predict_next_days,:-2], axis=-1))\n",
        "\n",
        "        # y.extend([utils.to_categorical(ohlct[-1,-1], num_classes=201)])\n",
        "        ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "        y.append(ohlct[-predict_next_days,-1])\n",
        "\n",
        "\n",
        "\n",
        "X = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "dis_count, dis_edge = np.histogram(y, bins = 20)\n",
        "\n",
        "dis_count_max = dis_count.max() * 4\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[(d*100//5).astype(int)]), \\\n",
        "                                0 , y)\n",
        "\n",
        "X = np.repeat(X, time_count, axis = 0)\n",
        "y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "length = X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "print(X.shape)\n",
        "print(X[1])\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc1VTJ4WfPtd"
      },
      "source": [
        "#20200824\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtE3stnXDcz1"
      },
      "source": [
        "###draw data distribution 1-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tq0EZ8uDcz3"
      },
      "source": [
        "plt.hist(y, bins = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYp2sAQEDcz7"
      },
      "source": [
        "###training 1-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx9Ps7PODcz9"
      },
      "source": [
        "# model.load_weights(\"stock_predict_3_1-3.h5\")\n",
        "\n",
        "history = model.fit(X, y, batch_size = 256, epochs = 64,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "# plt.plot(history.history[\"acc\"])\n",
        "plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(\"stock_predict_3_1-3.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga5h6zWQUi-J"
      },
      "source": [
        "#reset model params file\n",
        "model = make_model()\n",
        "model.save_weights(\"stock_predict_3_1-3-set.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azBWGp6TwroY"
      },
      "source": [
        "#20200824\n",
        "\n",
        "chk_point = ModelCheckpoint(filepath='stock_predict_3_1-3-set.h5')\n",
        "# model.load_weights(\"stock_predict_3_1-3-set.h5\")\n",
        "\n",
        "\n",
        "for i in range(4):\n",
        "  print(datetime.datetime.today())\n",
        "  model = make_model()\n",
        "  model.load_weights(\"stock_predict_3_1-3-set.h5\")  \n",
        "\n",
        "  for l in range(2):\n",
        "    history = model.fit(X_t, y_t, batch_size = 1024, epochs = 10,\\\n",
        "                # validation_split = 0.1, shuffle = True, \\\n",
        "                class_weight = cls_weight,\\\n",
        "                validation_data = (X_v ,y_v),\\\n",
        "                # callbacks = [tbCallBack]\\\n",
        "                callbacks = [chk_point],\\\n",
        "                # initial_epoch = 5 \n",
        "                ) \n",
        "\n",
        "    model.save_weights(\"stock_predict_3_1-3-set.h5\")\n",
        "\n",
        "  model.save_weights(\"stock_predict_3_1-3-set_a.h5\")\n",
        "  del model\n",
        "  gc.collect()\n",
        "plt.plot(history.history[\"acc\"])\n",
        "print(datetime.datetime.today())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH6D_yy4Dc0K"
      },
      "source": [
        "###drawing validation 1-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGJySR5VDc0L"
      },
      "source": [
        "\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = np.random.randint(0, stock_qty)\n",
        "\n",
        "                      \n",
        "x_plot = []\n",
        "y_plot = []\n",
        "yy_plot = []\n",
        "\n",
        "stk_data = result[result.code == ts_code.iloc[index][\"code\"]]\n",
        "length = len(stk_data)\n",
        "if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "  for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "    ohlct = stk_data.iloc[l : l + window_size + predict_next_days] \\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"isST\",\"pctChg\"]].values\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "      min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "      max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "      scale = (max_price - min_price)\n",
        "      ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:-predict_next_days,:-2], axis = -1)])\n",
        "\n",
        "      # ohlct[-1,-1] = (ohlct[-1,-1] + 10.0) / 20.0\n",
        "      y_plot.extend([ohlct[-predict_next_days,-1]])\n",
        "\n",
        "  X_plot = np.array(x_plot)\n",
        "  y_plot = np.array(y_plot)\n",
        "  # yy_plot = np.array(yy_plot)\n",
        "  print(ts_code.iloc[index])    \n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjSK3PNgDc0S"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-3.h5\")\n",
        "p = model.predict(X_plot).reshape(-1)\n",
        "\n",
        "p = p * 20.0 - 10.0\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot[-100:], color=\"blue\")\n",
        "plt.plot(p[-100:], color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRnEIxZrDc0c"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-3.h5\")\n",
        "model.evaluate(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdLHMEpQDc0g"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-3.h5\")\n",
        "model.evaluate(X_plot, y_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz7H77AjNK9p"
      },
      "source": [
        "##model 1-4: CNN + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0zrUfHFNK9t"
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "input = layers.Input(shape = (window_size, factor_num - 1 ,1))\n",
        "\n",
        "\n",
        "model_1 = layers.Conv2D(256, kernel_size =(1,4))(input)\n",
        "model_1 = layers.Activation(\"relu\")(model_1)\n",
        "model_1 = layers.BatchNormalization()(model_1)\n",
        "model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "model_1 = layers.Reshape((-1,256))(model_1)\n",
        "\n",
        "model_1 = layers.LSTM(256)(model_1)\n",
        "\n",
        "\n",
        "\n",
        "model_5 = layers.Conv2D(256, kernel_size =(5,4))(input)\n",
        "model_5 = layers.Activation(\"relu\")(model_5)\n",
        "model_5 = layers.BatchNormalization()(model_5)\n",
        "model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "model_5 = layers.Reshape((-1,256))(model_5)\n",
        "\n",
        "model_5 = layers.LSTM(256)(model_5)\n",
        "\n",
        "model_10 = layers.Conv2D(256, kernel_size =(5,4))(input)\n",
        "model_10 = layers.Activation(\"relu\")(model_10)\n",
        "model_10 = layers.BatchNormalization()(model_10)\n",
        "model_10 = layers.Dropout(0.5)(model_10)\n",
        "\n",
        "model_10 = layers.Reshape((-1,256))(model_10)\n",
        "\n",
        "model_10 = layers.LSTM(256)(model_10)\n",
        "\n",
        "model_20 = layers.Conv2D(256, kernel_size =(20,4))(input)\n",
        "model_20 = layers.Activation(\"relu\")(model_20)\n",
        "model_20 = layers.BatchNormalization()(model_20)\n",
        "model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "model_20 = layers.Reshape((-1,256))(model_20)\n",
        "\n",
        "model_20 = layers.LSTM(256)(model_20)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_30 = layers.Conv2D(256, kernel_size =(30,4))(input)\n",
        "model_30 = layers.Activation(\"relu\")(model_30)\n",
        "model_30 = layers.BatchNormalization()(model_30)\n",
        "model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "model_30 = layers.Reshape((-1,256))(model_30)\n",
        "\n",
        "model_30 = layers.LSTM(256)(model_30)\n",
        "\n",
        "\n",
        "\n",
        "model = layers.concatenate([model_1, model_5, model_10, model_20, model_30], axis = -1)\n",
        "\n",
        "model = layers.Dense(256)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(128)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(64)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(32)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(1)(model)\n",
        "\n",
        "model = Model(inputs = input, outputs = model)\n",
        "\n",
        "opt = optimizers.RMSprop(lr = 0.001, decay = 0.001)\n",
        "\n",
        "model.compile(loss = \"mse\", optimizer = opt)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ji2nW0xNK9-"
      },
      "source": [
        "###dataset 1-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8tzPppANK-B"
      },
      "source": [
        "pct_of_stock  = 0.01\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "\n",
        "print(np.sort(index))\n",
        "                      \n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in index:\n",
        "  stk_data = result[result.code == ts_code.iloc[i][\"code\"]]\n",
        "  length = len(stk_data)\n",
        "  if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "    for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "      ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "      if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "        min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "        scale = max_price - min_price\n",
        "        ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "        x.append(np.expand_dims(ohlct[:-predict_next_days,:-2], axis=-1))\n",
        "\n",
        "        # y.extend([utils.to_categorical(ohlct[-1,-1], num_classes=201)])\n",
        "        ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "        y.append(ohlct[-predict_next_days,-1])\n",
        "\n",
        "\n",
        "\n",
        "X = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "dis_count, dis_edge = np.histogram(y, bins = 20)\n",
        "\n",
        "dis_count_max = dis_count.max() * 4\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[(d*100//5).astype(int)]), \\\n",
        "                                0 , y)\n",
        "\n",
        "X = np.repeat(X, time_count, axis = 0)\n",
        "y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "length = X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "print(X.shape)\n",
        "print(X[1])\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n5kGdKdNK-L"
      },
      "source": [
        "###draw data distribution 1-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jkxO4iBNK-O"
      },
      "source": [
        "plt.hist(y, bins = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh9nRcdaNK-U"
      },
      "source": [
        "###training 1-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6DkXeRWNK-V"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-4.h5\")\n",
        "\n",
        "history = model.fit(X, y, batch_size = 1024, epochs = 24,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "# plt.plot(history.history[\"acc\"])\n",
        "plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(\"stock_predict_3_1-4.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XitBIfkFNK-d"
      },
      "source": [
        "###drawing validation 1-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMF-aFYHNK-g"
      },
      "source": [
        "\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = np.random.randint(0, stock_qty)\n",
        "\n",
        "                      \n",
        "x_plot = []\n",
        "y_plot = []\n",
        "yy_plot = []\n",
        "\n",
        "stk_data = result[result.code == ts_code.iloc[index][\"code\"]]\n",
        "length = len(stk_data)\n",
        "if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "  for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "    ohlct = stk_data.iloc[l : l + window_size + predict_next_days] \\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"isST\",\"pctChg\"]].values\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "      min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "      max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "      scale = (max_price - min_price)\n",
        "      ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:-predict_next_days,:-2], axis = -1)])\n",
        "\n",
        "      # ohlct[-1,-1] = (ohlct[-1,-1] + 10.0) / 20.0\n",
        "      y_plot.extend([ohlct[-predict_next_days,-1]])\n",
        "\n",
        "  X_plot = np.array(x_plot)\n",
        "  y_plot = np.array(y_plot)\n",
        "  # yy_plot = np.array(yy_plot)\n",
        "  print(ts_code.iloc[index])    \n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMwImG5KNK-m"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-4.h5\")\n",
        "p = model.predict(X_plot).reshape(-1)\n",
        "\n",
        "p = p * 20.0 - 10.0\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot[-100:], color=\"blue\")\n",
        "plt.plot(p[-100:], color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG3zMp0SNK-r"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-4.h5\")\n",
        "model.evaluate(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJUgwzrjNK_J"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-4.h5\")\n",
        "model.evaluate(X_plot, y_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeK-E3zCsBub"
      },
      "source": [
        "##model 1-5: CNN + LSTM with dif params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDadEhEQsBun"
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "input = layers.Input(shape = (window_size, factor_num - 1 ,1))\n",
        "\n",
        "\n",
        "model_1 = layers.Conv2D(64, kernel_size =(1,4))(input)\n",
        "model_1 = layers.Activation(\"relu\")(model_1)\n",
        "model_1 = layers.BatchNormalization()(model_1)\n",
        "model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "model_1 = layers.Reshape((-1,64))(model_1)\n",
        "\n",
        "model_1 = layers.LSTM(256)(model_1)\n",
        "\n",
        "\n",
        "\n",
        "model_5 = layers.Conv2D(128, kernel_size =(5,4))(input)\n",
        "model_5 = layers.Activation(\"relu\")(model_5)\n",
        "model_5 = layers.BatchNormalization()(model_5)\n",
        "model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "model_5 = layers.Reshape((-1,128))(model_5)\n",
        "\n",
        "model_5 = layers.LSTM(256)(model_5)\n",
        "\n",
        "model_10 = layers.Conv2D(256, kernel_size =(5,4))(input)\n",
        "model_10 = layers.Activation(\"relu\")(model_10)\n",
        "model_10 = layers.BatchNormalization()(model_10)\n",
        "model_10 = layers.Dropout(0.5)(model_10)\n",
        "\n",
        "model_10 = layers.Reshape((-1,256))(model_10)\n",
        "\n",
        "model_10 = layers.LSTM(256)(model_10)\n",
        "\n",
        "model_20 = layers.Conv2D(512, kernel_size =(20,4))(input)\n",
        "model_20 = layers.Activation(\"relu\")(model_20)\n",
        "model_20 = layers.BatchNormalization()(model_20)\n",
        "model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "model_20 = layers.Reshape((-1,512))(model_20)\n",
        "\n",
        "model_20 = layers.LSTM(256)(model_20)\n",
        "\n",
        "\n",
        "\n",
        "model_30 = layers.Conv2D(1024, kernel_size =(30,4))(input)\n",
        "model_30 = layers.Activation(\"relu\")(model_30)\n",
        "model_30 = layers.BatchNormalization()(model_30)\n",
        "model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "model_30 = layers.Reshape((-1,1024))(model_30)\n",
        "\n",
        "model_30 = layers.LSTM(256)(model_30)\n",
        "\n",
        "\n",
        "\n",
        "model = layers.concatenate([model_1, model_5, model_10, model_20, model_30], axis = -1)\n",
        "\n",
        "model = layers.Dense(256)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(128)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(64)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(32)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(1)(model)\n",
        "\n",
        "model = Model(inputs = input, outputs = model)\n",
        "\n",
        "opt = optimizers.RMSprop(lr = 0.001, decay = 0.001)\n",
        "\n",
        "model.compile(loss = \"mse\", optimizer = opt)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCpq_WYJsBux"
      },
      "source": [
        "###dataset 1-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vhbfyRhsBu0"
      },
      "source": [
        "pct_of_stock  = 0.01\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "\n",
        "print(np.sort(index))\n",
        "                      \n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in index:\n",
        "  stk_data = result[result.code == ts_code.iloc[i][\"code\"]]\n",
        "  length = len(stk_data)\n",
        "  if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "    for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "      ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "      if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "        min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "        scale = max_price - min_price\n",
        "        ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "        x.append(np.expand_dims(ohlct[:-predict_next_days,:-2], axis=-1))\n",
        "\n",
        "        # y.extend([utils.to_categorical(ohlct[-1,-1], num_classes=201)])\n",
        "        ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "        y.append(ohlct[-predict_next_days,-1])\n",
        "\n",
        "\n",
        "\n",
        "X = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "dis_count, dis_edge = np.histogram(y, bins = 20)\n",
        "\n",
        "dis_count_max = dis_count.max() * 4\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[(d*100//5).astype(int)]), \\\n",
        "                                0 , y)\n",
        "\n",
        "X = np.repeat(X, time_count, axis = 0)\n",
        "y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "length = X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "print(X.shape)\n",
        "print(X[1])\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOpu9aoFsBu-"
      },
      "source": [
        "###draw data distribution 1-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G94R0K9vsBvB"
      },
      "source": [
        "plt.hist(y, bins = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFHZhz3NsBvU"
      },
      "source": [
        "###training 1-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktctvB8lsBvY"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-5.h5\")\n",
        "\n",
        "history = model.fit(X, y, batch_size = 1024, epochs = 24,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "# plt.plot(history.history[\"acc\"])\n",
        "plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(\"stock_predict_3_1-5.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FOqy4bPDjgz"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-5.h5\")\n",
        "\n",
        "history = model.fit(X, y, batch_size = 1024, epochs = 64,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "# plt.plot(history.history[\"acc\"])\n",
        "plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(\"stock_predict_3_1-5.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WUuXH9nsBvj"
      },
      "source": [
        "###drawing validation 1-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82V0HmWFsBvl"
      },
      "source": [
        "\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = np.random.randint(0, stock_qty)\n",
        "\n",
        "                      \n",
        "x_plot = []\n",
        "y_plot = []\n",
        "yy_plot = []\n",
        "\n",
        "stk_data = result[result.code == ts_code.iloc[index][\"code\"]]\n",
        "length = len(stk_data)\n",
        "if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "  for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "    ohlct = stk_data.iloc[l : l + window_size + predict_next_days] \\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"isST\",\"pctChg\"]].values\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "      min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "      max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "      scale = (max_price - min_price)\n",
        "      ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:-predict_next_days,:-2], axis = -1)])\n",
        "\n",
        "      # ohlct[-1,-1] = (ohlct[-1,-1] + 10.0) / 20.0\n",
        "      y_plot.extend([ohlct[-predict_next_days,-1]])\n",
        "\n",
        "  X_plot = np.array(x_plot)\n",
        "  y_plot = np.array(y_plot)\n",
        "  # yy_plot = np.array(yy_plot)\n",
        "  print(ts_code.iloc[index])    \n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U-DxIc_sBvt"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-5.h5\")\n",
        "p = model.predict(X_plot).reshape(-1)\n",
        "\n",
        "p = p * 20.0 - 10.0\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot[-100:], color=\"blue\")\n",
        "plt.plot(p[-100:], color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHAjTtQVsBvx"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-5.h5\")\n",
        "model.evaluate(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZghInDdrsBv0"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-5.h5\")\n",
        "model.evaluate(X_plot, y_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1_k6_jXNQx6"
      },
      "source": [
        "###get data of specific code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t3nvTRA5asI"
      },
      "source": [
        "s_code = \"sh.000016\"\n",
        "\n",
        "s_data_file = s_code + \".csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9aqqD2EfEm0"
      },
      "source": [
        "\n",
        "s_code = \"sh.000016\"\n",
        "\n",
        "s_data_file = s_code + \".csv\"\n",
        "\n",
        "bs.login()\n",
        "\n",
        "rs = bs.query_history_k_data_plus(s_code, \",\".join(data_fields),\\\n",
        "                          start_date = start_date, end_date = end_date,\\\n",
        "                            frequency = \"d\", adjustflag = adjustflag)\n",
        "\n",
        "data_list = []\n",
        "while (rs.error_code == \"0\") and rs.next():\n",
        "    row_data = rs.get_row_data()\n",
        "    if(row_data[6] == \"1\"):         \n",
        "        data_list.append(row_data)\n",
        "s_result = pd.DataFrame(data_list, columns = rs.fields)\n",
        "s_result.to_csv(s_data_file)\n",
        "print(s_result.head())\n",
        "\n",
        "bs.logout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qYIH7kzfsNh"
      },
      "source": [
        "###dataset of specific code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh0IgWV-fHcE"
      },
      "source": [
        "x = []\n",
        "y = []\n",
        "\n",
        "stk_data = pd.read_csv(s_data_file)\n",
        "length = len(stk_data)\n",
        "if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        " for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "  ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "  if np.all(ohlct[:, -2] != 1) and \\\n",
        "     np.all(ohlct[:,-1] > -10.0) and \\\n",
        "      np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "  min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "  scale = max_price - min_price\n",
        "  ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "  x.append(np.expand_dims(ohlct[:-predict_next_days,:-2], axis=-1))\n",
        "\n",
        "  # y.extend([utils.to_categorical(ohlct[-1,-1], num_classes=201)])\n",
        "  ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "  y.append(ohlct[-predict_next_days,-1])\n",
        "\n",
        "\n",
        "s_X = np.array(x)\n",
        "s_y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "dis_count, dis_edge = np.histogram(s_y, bins = 20)\n",
        "\n",
        "dis_count_max = dis_count.max() * 4\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[(d*100//5).astype(int)]), \\\n",
        "                                0 , s_y)\n",
        "\n",
        "s_X = np.repeat(s_X, time_count, axis = 0)\n",
        "s_y = np.repeat(s_y, time_count, axis = 0)\n",
        "\n",
        "length = s_X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "s_X = s_X[index]\n",
        "s_y = s_y[index]\n",
        "\n",
        "index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "s_X = s_X[index]\n",
        "s_y = s_y[index]\n",
        "\n",
        "print(s_X.shape)\n",
        "print(s_X[1])\n",
        "print(s_y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj_SJEQ_f2-h"
      },
      "source": [
        "###training basing on model1-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3fEb2kdf-Hc"
      },
      "source": [
        "# model.load_weights(\"stock_predict_3_1-5.h5\")\n",
        "model.load_weights(s_data_file + \".h5\")\n",
        "\n",
        "history = model.fit(s_X, s_y, batch_size = 128, epochs = 24,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "# plt.plot(history.history[\"acc\"])\n",
        "plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(s_data_file + \".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyRaLilwhXDe"
      },
      "source": [
        "###get more data for specific code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brNM09tUhc3W"
      },
      "source": [
        "bs.login()\n",
        "\n",
        "s_date = \"2018-06-01\"\n",
        "e_date = \"2019-09-30\"\n",
        "\n",
        "rs = bs.query_history_k_data_plus(s_code, \",\".join(data_fields),\\\n",
        "                          start_date = s_date, end_date = e_date,\\\n",
        "                            frequency = \"d\", adjustflag = adjustflag)\n",
        "\n",
        "data_list = []\n",
        "while (rs.error_code == \"0\") and rs.next():\n",
        "    row_data = rs.get_row_data()\n",
        "    if(row_data[6] == \"1\"):         \n",
        "        data_list.append(row_data)\n",
        "p_result = pd.DataFrame(data_list, columns = rs.fields)\n",
        "\n",
        "print(p_result.head())\n",
        "\n",
        "bs.logout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unLFYTKkDXEo"
      },
      "source": [
        "print(p_result.loc[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs-RKeyFgyRP"
      },
      "source": [
        "print(p_result.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51tkVCF26Agg"
      },
      "source": [
        "###predict for specific code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q90PLdFU6eYc"
      },
      "source": [
        "                      \n",
        "x_plot = []\n",
        "y_plot = []\n",
        "\n",
        "stk_data = p_result\n",
        "length = len(stk_data)\n",
        "if ((length - predict_next_days) > window_size):\n",
        "  for l in range(length - window_size - predict_next_days):\n",
        "    ohlct = stk_data.iloc[l : l + window_size + predict_next_days] \\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"isST\",\"pctChg\"]].values\n",
        "    ohlct = ohlct.astype(\"float\")\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "      min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "      max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "      scale = (max_price - min_price)\n",
        "      ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:-predict_next_days,:-2], axis = -1)])\n",
        "\n",
        "      # ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "      y_plot.extend([ohlct[-predict_next_days,-1]])\n",
        "\n",
        "  X_plot = np.array(x_plot)\n",
        "  y_plot = np.array(y_plot)\n",
        "  # yy_plot = np.array(yy_plot)  \n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc8MyjZu6Lgo"
      },
      "source": [
        "model.load_weights(\"sh.000016.csv.h5\")\n",
        "# model.load_weights(\"stock_predict_3_1-5.h5\")\n",
        "\n",
        "p = model.predict(X_plot).reshape(-1)\n",
        "\n",
        "p = p * 20.0 - 10.0\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot, color=\"blue\")\n",
        "plt.plot(p, color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBQKvnNyvv76"
      },
      "source": [
        "##model 1-6: CNN + LSTM with dif params by crossentrpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzcJ4msjvv79"
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "input = layers.Input(shape = (window_size, factor_num - 1 ,1))\n",
        "\n",
        "\n",
        "model_1 = layers.Conv2D(64, kernel_size =(1,4))(input)\n",
        "model_1 = layers.Activation(\"relu\")(model_1)\n",
        "model_1 = layers.BatchNormalization()(model_1)\n",
        "model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "model_1 = layers.Reshape((-1,64))(model_1)\n",
        "\n",
        "model_1 = layers.LSTM(256)(model_1)\n",
        "\n",
        "\n",
        "\n",
        "model_5 = layers.Conv2D(128, kernel_size =(5,4))(input)\n",
        "model_5 = layers.Activation(\"relu\")(model_5)\n",
        "model_5 = layers.BatchNormalization()(model_5)\n",
        "model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "model_5 = layers.Reshape((-1,128))(model_5)\n",
        "\n",
        "model_5 = layers.LSTM(256)(model_5)\n",
        "\n",
        "model_10 = layers.Conv2D(256, kernel_size =(5,4))(input)\n",
        "model_10 = layers.Activation(\"relu\")(model_10)\n",
        "model_10 = layers.BatchNormalization()(model_10)\n",
        "model_10 = layers.Dropout(0.5)(model_10)\n",
        "\n",
        "model_10 = layers.Reshape((-1,256))(model_10)\n",
        "\n",
        "model_10 = layers.LSTM(256)(model_10)\n",
        "\n",
        "model_20 = layers.Conv2D(512, kernel_size =(20,4))(input)\n",
        "model_20 = layers.Activation(\"relu\")(model_20)\n",
        "model_20 = layers.BatchNormalization()(model_20)\n",
        "model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "model_20 = layers.Reshape((-1,512))(model_20)\n",
        "\n",
        "model_20 = layers.LSTM(256)(model_20)\n",
        "\n",
        "\n",
        "\n",
        "model_30 = layers.Conv2D(1024, kernel_size =(30,4))(input)\n",
        "model_30 = layers.Activation(\"relu\")(model_30)\n",
        "model_30 = layers.BatchNormalization()(model_30)\n",
        "model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "model_30 = layers.Reshape((-1,1024))(model_30)\n",
        "\n",
        "model_30 = layers.LSTM(256)(model_30)\n",
        "\n",
        "\n",
        "\n",
        "model = layers.concatenate([model_1, model_5, model_10, model_20, model_30], axis = -1)\n",
        "\n",
        "model = layers.Dense(256)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(128)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(64)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(32)(model)\n",
        "model = layers.Activation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(4)(model)\n",
        "model = layers.Activation(\"softmax\")(model)\n",
        "\n",
        "model = Model(inputs = input, outputs = model)\n",
        "\n",
        "opt = optimizers.RMSprop(lr = 0.001, decay = 0.001)\n",
        "\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics = [\"acc\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7ibvQRGvv8G"
      },
      "source": [
        "###dataset 1-6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHX8zzMZvv8H"
      },
      "source": [
        "pct_of_stock  = 0.04\n",
        "stock_qty = len(ts_code)\n",
        "edge = 3.0\n",
        "\n",
        "index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "\n",
        "print(np.sort(index))\n",
        "                      \n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in index:\n",
        "  stk_data = result[result.code == ts_code.iloc[i][\"code\"]]\n",
        "  length = len(stk_data)\n",
        "  if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "    for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "      ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "      if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "        min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "        scale = max_price - min_price\n",
        "        ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "        x.append(np.expand_dims(ohlct[:-predict_next_days,:-2], axis=-1))\n",
        "\n",
        "        # y.extend([utils.to_categorical(ohlct[-1,-1], num_classes=201)])\n",
        "        if ohlct[-predict_next_days,-1] <= -edge:\n",
        "          y.append(0)\n",
        "        elif ohlct[-predict_next_days,-1] > -edge and \\\n",
        "          ohlct[-predict_next_days,-1] <= 0:\n",
        "          y.append(1)\n",
        "        elif ohlct[-predict_next_days,-1] > 0 and \\\n",
        "          ohlct[-predict_next_days,-1] <= edge:\n",
        "          y.append(2)\n",
        "        else:\n",
        "          y.append(3)\n",
        "\n",
        "\n",
        "X = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "\n",
        "dis_count, dis_edge = np.histogram(y, bins = 4)\n",
        "\n",
        "dis_count_max = dis_count.max()\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[d.astype(int)]), \\\n",
        "                                0 , y)\n",
        "\n",
        "X = np.repeat(X, time_count, axis = 0)\n",
        "y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "# shuffle dataset\n",
        "\n",
        "length = X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "# index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "y = utils.to_categorical(y)\n",
        "\n",
        "print(X.shape)\n",
        "print(X[1])\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sbAduiZryBx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQeHx4HBvv8O"
      },
      "source": [
        "###draw data distribution 1-6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN1RgSLWvv8R"
      },
      "source": [
        "plt.hist(y, bins = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJUabvmyvv8e"
      },
      "source": [
        "###training 1-6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qahrtZ2yvv8i"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-6.h5\")\n",
        "\n",
        "history = model.fit(X, y, batch_size = 1024, epochs = 64,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "plt.plot(history.history[\"acc\"])\n",
        "# plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(\"stock_predict_3_1-6.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCk527zzvv85"
      },
      "source": [
        "###drawing validation 1-6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRPaKp41vv87"
      },
      "source": [
        "\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = np.random.randint(0, stock_qty)\n",
        "\n",
        "                      \n",
        "x_plot = []\n",
        "y_plot = []\n",
        "yy_plot = []\n",
        "\n",
        "stk_data = result[result.code == ts_code.iloc[index][\"code\"]]\n",
        "length = len(stk_data)\n",
        "if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "  for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "    ohlct = stk_data.iloc[l : l + window_size + predict_next_days] \\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"isST\",\"pctChg\"]].values\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "      min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "      max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "      scale = (max_price - min_price)\n",
        "      ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:-predict_next_days,:-2], axis = -1)])\n",
        "\n",
        "      # ohlct[-1,-1] = (ohlct[-1,-1] + 10.0) / 20.0\n",
        "      if ohlct[-predict_next_days,-1] <= -edge:\n",
        "        y_plot.append(0)\n",
        "      elif ohlct[-predict_next_days,-1] > -edge and \\\n",
        "        ohlct[-predict_next_days,-1] <= 0:\n",
        "        y_plot.append(1)\n",
        "      elif ohlct[-predict_next_days,-1] > 0 and \\\n",
        "        ohlct[-predict_next_days,-1] <= edge:\n",
        "        y_plot.append(2)\n",
        "      else:\n",
        "        y_plot.append(3)\n",
        "\n",
        "\n",
        "  X_plot = np.array(x_plot)\n",
        "  y_plot = np.array(y_plot)\n",
        "\n",
        "  # yy_plot = np.array(yy_plot)\n",
        "  print(ts_code.iloc[index])    \n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWtYheSQvv9F"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-6.h5\")\n",
        "p = model.predict(X_plot)\n",
        "p = np.apply_along_axis(lambda d: d.argmax() + 1, 1, p)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot[-100:], color=\"blue\")\n",
        "plt.plot(p[-100:], color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nenRGirXVbjw"
      },
      "source": [
        "np.apply_along_axis(lambda d: d.argmax(), 1, p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeq1O0RHvv9b"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-6.h5\")\n",
        "model.evaluate(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ouV4-hQvv9o"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-6.h5\")\n",
        "\n",
        "model.evaluate(X_plot, utils.to_categorical(y_plot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKXBcctzvv95"
      },
      "source": [
        "###get data of specific code 1-6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvEuGsM0vv98"
      },
      "source": [
        "s_code = \"sh.000016\"\n",
        "\n",
        "s_data_file = s_code + \"_1-6.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q46lAJ8Jvv-H"
      },
      "source": [
        "\n",
        "s_code = \"sh.000016\"\n",
        "\n",
        "s_data_file = s_code + \"_1-6.csv\"\n",
        "\n",
        "bs.login()\n",
        "\n",
        "rs = bs.query_history_k_data_plus(s_code, \",\".join(data_fields),\\\n",
        "                          start_date = start_date, end_date = end_date,\\\n",
        "                            frequency = \"d\", adjustflag = adjustflag)\n",
        "\n",
        "data_list = []\n",
        "while (rs.error_code == \"0\") and rs.next():\n",
        "    row_data = rs.get_row_data()\n",
        "    if(row_data[6] == \"1\"):         \n",
        "        data_list.append(row_data)\n",
        "s_result = pd.DataFrame(data_list, columns = rs.fields)\n",
        "s_result.to_csv(s_data_file)\n",
        "print(s_result.head())\n",
        "\n",
        "bs.logout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8iDRzhCvv-S"
      },
      "source": [
        "###dataset of specific code 1-6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vdO3VPbvv-W"
      },
      "source": [
        "edge = 3.0\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "stk_data = pd.read_csv(s_data_file)\n",
        "length = len(stk_data)\n",
        "if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        " for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "  ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "  if np.all(ohlct[:, -2] != 1) and \\\n",
        "     np.all(ohlct[:,-1] > -10.0) and \\\n",
        "      np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "  min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "  scale = max_price - min_price\n",
        "  ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "  x.append(np.expand_dims(ohlct[:-predict_next_days,:-2], axis=-1))\n",
        "\n",
        "  # y.extend([utils.to_categorical(ohlct[-1,-1], num_classes=201)])\n",
        "  # ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "  if ohlct[-predict_next_days,-1] <= -edge:\n",
        "    y.append(0)\n",
        "  elif ohlct[-predict_next_days,-1] > -edge and \\\n",
        "    ohlct[-predict_next_days,-1] <= 0:\n",
        "    y.append(1)\n",
        "  elif ohlct[-predict_next_days,-1] > 0 and \\\n",
        "    ohlct[-predict_next_days,-1] <= edge:\n",
        "    y.append(2)\n",
        "  else:\n",
        "    y.append(3)\n",
        "\n",
        "\n",
        "\n",
        "s_X = np.array(x)\n",
        "s_y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "\n",
        "dis_count, dis_edge = np.histogram(y, bins = 4)\n",
        "\n",
        "dis_count_max = dis_count.max()\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[d.astype(int)]), \\\n",
        "                                0 , s_y)\n",
        "\n",
        "s_X = np.repeat(s_X, time_count, axis = 0)\n",
        "s_y = np.repeat(s_y, time_count, axis = 0)\n",
        "\n",
        "\n",
        "#shuffle\n",
        "length = s_X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "s_X = s_X[index]\n",
        "s_y = s_y[index]\n",
        "\n",
        "# index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "s_X = s_X[index]\n",
        "s_y = s_y[index]\n",
        "\n",
        "s_y = utils.to_categorical(s_y)\n",
        "\n",
        "print(s_X.shape)\n",
        "print(s_X[1])\n",
        "print(s_y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKV8H5ywvv-m"
      },
      "source": [
        "###training basing on model1-6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCWut5rVvv-x"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-6.h5\")\n",
        "# model.load_weights(s_data_file + \".h5\")\n",
        "\n",
        "history = model.fit(s_X, s_y, batch_size = 128, epochs = 64,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "plt.plot(history.history[\"acc\"])\n",
        "# plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(s_data_file + \".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TdN1QEEvv-5"
      },
      "source": [
        "###get more data for specific code 1-6\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG-1xYUVvv-7"
      },
      "source": [
        "bs.login()\n",
        "\n",
        "s_date = \"2018-06-01\"\n",
        "e_date = \"2019-11-13\"\n",
        "\n",
        "rs = bs.query_history_k_data_plus(s_code, \",\".join(data_fields),\\\n",
        "                          start_date = s_date, end_date = e_date,\\\n",
        "                            frequency = \"d\", adjustflag = adjustflag)\n",
        "\n",
        "data_list = []\n",
        "while (rs.error_code == \"0\") and rs.next():\n",
        "    row_data = rs.get_row_data()\n",
        "    if(row_data[6] == \"1\"):         \n",
        "        data_list.append(row_data)\n",
        "p_result = pd.DataFrame(data_list, columns = rs.fields)\n",
        "\n",
        "print(p_result.head())\n",
        "\n",
        "bs.logout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW9oeSHVvv_B"
      },
      "source": [
        "print(p_result.loc[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wauhK0Ivvv_Q"
      },
      "source": [
        "print(p_result.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo_Nwseevv_d"
      },
      "source": [
        "###predict for specific code 1-6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDITjuEOvv_f"
      },
      "source": [
        "                      \n",
        "x_plot = []\n",
        "y_plot = []\n",
        "\n",
        "stk_data = p_result\n",
        "length = len(stk_data)\n",
        "if ((length - predict_next_days) > window_size):\n",
        "  for l in range(length - window_size - predict_next_days):\n",
        "    ohlct = stk_data.iloc[l : l + window_size + predict_next_days] \\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"isST\",\"pctChg\"]].values\n",
        "    ohlct = ohlct.astype(\"float\")\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "      min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "      max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "      scale = (max_price - min_price)\n",
        "      ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:-predict_next_days,:-2], axis = -1)])\n",
        "\n",
        "      # ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "      if ohlct[-predict_next_days,-1] <= -edge:\n",
        "        y_plot.append(0)\n",
        "      elif ohlct[-predict_next_days,-1] > -edge and \\\n",
        "        ohlct[-predict_next_days,-1] <= 0:\n",
        "        y_plot.append(1)\n",
        "      elif ohlct[-predict_next_days,-1] > 0 and \\\n",
        "        ohlct[-predict_next_days,-1] <= edge:\n",
        "        y_plot.append(2)\n",
        "      else:\n",
        "        y_plot.append(3)\n",
        "\n",
        "\n",
        "  X_plot = np.array(x_plot)\n",
        "  y_plot = np.array(y_plot)\n",
        "  # yy_plot = np.array(yy_plot)  \n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz4sXwWZvv_n"
      },
      "source": [
        "model.load_weights(s_data_file + \".h5\")\n",
        "# model.load_weights(\"stock_predict_3_1-6.h5\")\n",
        "\n",
        "p = model.predict(X_plot)\n",
        "p = np.apply_along_axis(lambda d: d.argmax() + 1, 1, p)\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot, color=\"blue\")\n",
        "plt.plot(p, color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVaJgFYXjOtL"
      },
      "source": [
        "model.load_weights(s_data_file + \".h5\")\n",
        "model.evaluate(X_plot, utils.to_categorical(y_plot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fWGVoHUha_5"
      },
      "source": [
        "##model 1-7: CNN + LSTM with dif params by crossentrpy for next few days"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRxx0fuVixHa"
      },
      "source": [
        "predict_next_days = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYH4j1hpiNGs"
      },
      "source": [
        "def make_model(summary = False):\n",
        "  K.clear_session()\n",
        "  # tf.keras.backend.clear_session()\n",
        "  p_lstm = 64\n",
        "\n",
        "  input = layers.Input(shape = (window_size, factor_num - 1 ,1))\n",
        "\n",
        "\n",
        "  model_1 = layers.Conv2D(64, kernel_size =(1,4))(input)\n",
        "  model_1 = layers.Activation(\"relu\")(model_1)\n",
        "  model_1 = layers.BatchNormalization()(model_1)\n",
        "  model_1 = layers.Dropout(0.5)(model_1)\n",
        "\n",
        "  model_1 = layers.Reshape((-1,64))(model_1)\n",
        "\n",
        "  model_1 = layers.LSTM(p_lstm)(model_1)\n",
        "\n",
        "\n",
        "\n",
        "  model_5 = layers.Conv2D(128, kernel_size =(5,4))(input)\n",
        "  model_5 = layers.Activation(\"relu\")(model_5)\n",
        "  model_5 = layers.BatchNormalization()(model_5)\n",
        "  model_5 = layers.Dropout(0.5)(model_5)\n",
        "\n",
        "  model_5 = layers.Reshape((-1,128))(model_5)\n",
        "\n",
        "  model_5 = layers.LSTM(p_lstm)(model_5)\n",
        "\n",
        "  model_10 = layers.Conv2D(256, kernel_size =(10,4))(input)\n",
        "  model_10 = layers.Activation(\"relu\")(model_10)\n",
        "  model_10 = layers.BatchNormalization()(model_10)\n",
        "  model_10 = layers.Dropout(0.5)(model_10)\n",
        "\n",
        "  model_10 = layers.Reshape((-1,256))(model_10)\n",
        "\n",
        "  model_10 = layers.LSTM(p_lstm)(model_10)\n",
        "\n",
        "  model_20 = layers.Conv2D(512, kernel_size =(20,4))(input)\n",
        "  model_20 = layers.Activation(\"relu\")(model_20)\n",
        "  model_20 = layers.BatchNormalization()(model_20)\n",
        "  model_20 = layers.Dropout(0.5)(model_20)\n",
        "\n",
        "  model_20 = layers.Reshape((-1,512))(model_20)\n",
        "\n",
        "  model_20 = layers.LSTM(p_lstm)(model_20)\n",
        "\n",
        "\n",
        "\n",
        "  model_30 = layers.Conv2D(1024, kernel_size =(30,4))(input)\n",
        "  model_30 = layers.Activation(\"relu\")(model_30)\n",
        "  model_30 = layers.BatchNormalization()(model_30)\n",
        "  model_30 = layers.Dropout(0.5)(model_30)\n",
        "\n",
        "  model_30 = layers.Reshape((-1,1024))(model_30)\n",
        "\n",
        "  model_30 = layers.LSTM(p_lstm)(model_30)\n",
        "\n",
        "\n",
        "\n",
        "  model = layers.concatenate([model_1, model_5, model_10, model_20, model_30], axis = -1)\n",
        "\n",
        "\n",
        "  model = layers.Dense(2048)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "\n",
        "  model = layers.Dense(1024)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(512)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(256)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(128)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(64)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(32)(model)\n",
        "  model = layers.Activation(\"relu\")(model)\n",
        "  model = layers.BatchNormalization()(model)\n",
        "  model = layers.Dropout(0.5)(model)\n",
        "\n",
        "  model = layers.Dense(4)(model)\n",
        "  model = layers.Activation(\"softmax\")(model)\n",
        "\n",
        "  model = Model(inputs = input, outputs = model)\n",
        "\n",
        "  # opt = optimizers.RMSprop(lr = 0.0005, decay = 0.001)\n",
        "  opt = optimizers.Adam(lr = 0.0005, decay = 0.001)\n",
        "\n",
        "  model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics = [\"acc\"])\n",
        "\n",
        "  if summary:\n",
        "    model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "model = make_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9ukse_8iNG1"
      },
      "source": [
        "###dataset 1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY25BWpPiNG3"
      },
      "source": [
        "pct_of_stock  = 0.2\n",
        "stock_qty = len(ts_code)\n",
        "edge = 10.0\n",
        "\n",
        "index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "\n",
        "print(np.sort(index))\n",
        "                      \n",
        "x = []\n",
        "y = []\n",
        "\n",
        "count = 0\n",
        "\n",
        "for i in index:\n",
        "  count = count + 1\n",
        "  if count%200 == 0:\n",
        "    print(count)\n",
        "  stk_data = result[result.code == ts_code.iloc[i][\"code\"]]\n",
        "  length = len(stk_data)\n",
        "  if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "    for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "      ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\"]].values\n",
        "      if np.all(ohlct[:,-2] != 1) and \\\n",
        "        np.all(ohlct[:,-3] != 0) and \\\n",
        "       np.all(ohlct[:,-1] >= -10.0) and \\\n",
        "       np.all(ohlct[:,-1] <= 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-3].max()\n",
        "        min_price = ohlct[:-predict_next_days,:-3].min()\n",
        "        scale = max_price - min_price\n",
        "        ohlct[:-predict_next_days,:-3] = ([ohlct[:-predict_next_days,:-3] - min_price]) /scale\n",
        "\n",
        "        x.append(np.expand_dims(ohlct[:-predict_next_days,:-3], axis=-1))\n",
        "\n",
        "        next_days_pctChg = ohlct[-predict_next_days:,-1].sum()\n",
        "\n",
        "        if next_days_pctChg  <= -edge:\n",
        "          y.append(0)\n",
        "        elif next_days_pctChg  > -edge and \\\n",
        "          next_days_pctChg  <= 0:\n",
        "          y.append(1)\n",
        "        elif next_days_pctChg  > 0 and \\\n",
        "          next_days_pctChg  <= edge:\n",
        "          y.append(2)\n",
        "        else:\n",
        "          y.append(3)\n",
        "\n",
        "\n",
        "X = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "\n",
        "dis_count, dis_edge = np.histogram(y, bins = 4)\n",
        "\n",
        "dis_count_max = dis_count.max()\n",
        "\n",
        "dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "dis_count_max = dis_count_max + 1.0 #because dis_count plus 1 above, \n",
        "                    #here plus 1 to avoid time below to be zero\n",
        "\n",
        "time = dis_count_max/dis_count\n",
        "time = time.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: time[d.astype(int)]), \\\n",
        "                                0 , y)\n",
        "\n",
        "X = np.repeat(X, time_count, axis = 0)\n",
        "y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "# shuffle dataset\n",
        "\n",
        "length = X.shape[0]\n",
        "\n",
        "r_index = np.arange(length)\n",
        "np.random.shuffle(r_index)\n",
        "\n",
        "X = X[r_index]\n",
        "y = y[r_index]\n",
        "\n",
        "# r_index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "# X = X[r_index]\n",
        "# y = y[r_index]\n",
        "\n",
        "y = utils.to_categorical(y)\n",
        "\n",
        "print(X.shape)\n",
        "print(X[1])\n",
        "print(y[0])\n",
        "\n",
        "# X.tofile(\"stock_predict_3_1-7-X.dat\", format=\"%f\")\n",
        "# y.tofile(\"stock_predict_3_1-7-y.dat\", format=\"%f\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEVK7MxFVYRG"
      },
      "source": [
        "yy = y.argmax(axis = 1)\n",
        "\n",
        "if np.any(yy == 2): \n",
        "  print(\"ok\")\n",
        "\n",
        "# y[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqH6I3rXiNHC"
      },
      "source": [
        "###draw data distribution 1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqKw1LJRiNHF"
      },
      "source": [
        "plt.hist(y.argmax(axis = 1), bins = 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtW_rDmOiNHJ"
      },
      "source": [
        "###training 1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iROmT0CUFj0-"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL9fSeR9EDbp"
      },
      "source": [
        "X = np.fromfile(\"stock_predict_3_1-7-X.dat\", dtype=np.float).reshape(26654, 90, 4, 1)\n",
        "y = np.fromfile(\"stock_predict_3_1-7-y.dat\", dtype=np.float32).reshape(26654, 4)\n",
        "print(X[100])\n",
        "print(y[180])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L_YQrb0iNHJ"
      },
      "source": [
        "\n",
        "\n",
        "model.load_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "for t in range(20):\n",
        "  history = model.fit(X, y, batch_size = 1024, epochs = 4,\\\n",
        "              validation_split = 0.1, shuffle = True, \\\n",
        "              # callbacks = [tbCallBack]\\\n",
        "              )\n",
        "\n",
        "  model.save_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "plt.plot(history.history[\"acc\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-zJ_Mpr6Jcr"
      },
      "source": [
        "###training by generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsLfRnUL6R8S"
      },
      "source": [
        "####define a generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJGGjQ6C6aYP"
      },
      "source": [
        "def gntor(ts_data, ts_code, pct_of_stock=0.05, edge=10.0, batch_size = 512, generator = True):  \n",
        "\n",
        "  stock_qty = len(ts_code)\n",
        "  # index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "\n",
        "  while True :\n",
        "\n",
        "    index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))                   \n",
        "\n",
        "    # x = []\n",
        "    # y = []\n",
        "\n",
        "    for i in index:\n",
        "    # for i in range(stock_qty):\n",
        "\n",
        "      x = []\n",
        "      y = []\n",
        "\n",
        "      stk_data = ts_data[ts_data.code == ts_code.iloc[i][\"code\"]]\n",
        "      length = len(stk_data)\n",
        "      if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "        for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "          ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "              [[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\"]].values.astype(\"float\")\n",
        "          if np.all(ohlct[:,-2] == 0) and \\\n",
        "            np.all(ohlct[:,-3] == 1) and \\\n",
        "          np.all(ohlct[:,-1] >= -10.0) and \\\n",
        "          np.all(ohlct[:,-1] <= 10.0) :\n",
        "            max_price = ohlct[:-predict_next_days,:-3].max()\n",
        "            min_price = ohlct[:-predict_next_days,:-3].min()\n",
        "            scale = max_price - min_price\n",
        "            ohlct[:-predict_next_days,:-3] = ([ohlct[:-predict_next_days,:-3] - min_price]) /scale\n",
        "\n",
        "            x.append(np.expand_dims(ohlct[:-predict_next_days,:-3], axis=-1))\n",
        "\n",
        "            next_days_pctChg = (ohlct[-1,-4] - ohlct[-predict_next_days,-4]) * 100 / ohlct[-predict_next_days,-4]\n",
        "\n",
        "            if next_days_pctChg  <= -edge:\n",
        "              y.append(0)\n",
        "            elif next_days_pctChg  > -edge and \\\n",
        "              next_days_pctChg  <= 0:\n",
        "              y.append(1)\n",
        "            elif next_days_pctChg  > 0 and \\\n",
        "              next_days_pctChg  <= edge:\n",
        "              y.append(2)\n",
        "            else:\n",
        "              y.append(3)\n",
        "    # print(\"there\")\n",
        "    if len(x) > 0:\n",
        "      # print(\"here\")\n",
        "      X = np.array(x)\n",
        "      y = np.array(y)\n",
        "\n",
        "      # upsample\n",
        "      # dis = district\n",
        "\n",
        "      dis_count, dis_edge = np.histogram(y, bins = 4, range=(0,3))\n",
        "\n",
        "      dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "      dis_count_max = dis_count.max()\n",
        "\n",
        "      time = dis_count_max/dis_count\n",
        "      time = time.astype(int)\n",
        "\n",
        "      time_count = np.apply_along_axis((lambda d: time[d.astype(int)]), \\\n",
        "                                      0 , y)\n",
        "\n",
        "      X = np.repeat(X, time_count, axis = 0)\n",
        "      y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "      # shuffle dataset\n",
        "\n",
        "      length = X.shape[0]\n",
        "\n",
        "      r_index = np.arange(length)\n",
        "      np.random.shuffle(r_index)\n",
        "\n",
        "      X = X[r_index]\n",
        "      y = y[r_index]\n",
        "\n",
        "      # r_index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "      # X = X[r_index]\n",
        "      # y = y[r_index]\n",
        "\n",
        "      y = utils.to_categorical(y, num_classes=4)\n",
        "      if generator:\n",
        "        print(\"a\")\n",
        "        # for ll in range((length-1)//batch_size + 1):\n",
        "        #   start_i = ll*batch_size\n",
        "        #   end_i = start_i + batch_size\n",
        "        #   if end_i <= length:\n",
        "        #     yield(X[start_i:end_i],y[start_i:end_i])\n",
        "        #   else:\n",
        "        #     yield(X[start_i:],y[start_i:])\n",
        "      else:\n",
        "        return 1,2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1qdym0V2iqc"
      },
      "source": [
        "i = random.randint(0,len(ts_code))\n",
        "print(i)\n",
        "stk_data = result[result.code == ts_code.iloc[i][\"code\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEESjjS12jor"
      },
      "source": [
        "####define a generator 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwMnoq0X2jot"
      },
      "source": [
        "def gntor_1(ts_data, ts_code, edge=10.0):\n",
        "  stock_qty = len(ts_code)\n",
        "\n",
        "  # print(np.sort(index))                        \n",
        "\n",
        "\n",
        "  while True:\n",
        "    i = random.randint(0,stock_qty-1)\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    # if count%1 == 0:\n",
        "    #   print(count)\n",
        "    stk_data = ts_data[ts_data.code == ts_code.iloc[i][\"code\"]]\n",
        "    length = len(stk_data)\n",
        "    if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "      for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "        ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "            [[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\"]].values\n",
        "        if np.all(ohlct[:,-2] != 1) and \\\n",
        "          np.all(ohlct[:,-3] != 0) and \\\n",
        "        np.all(ohlct[:,-1] >= -10.0) and \\\n",
        "        np.all(ohlct[:,-1] <= 10.0) :\n",
        "          max_price = ohlct[:-predict_next_days,:-3].max()\n",
        "          min_price = ohlct[:-predict_next_days,:-3].min()\n",
        "          scale = max_price - min_price\n",
        "          ohlct[:-predict_next_days,:-3] = ([ohlct[:-predict_next_days,:-3] - min_price]) /scale\n",
        "\n",
        "          x.append(np.expand_dims(ohlct[:-predict_next_days,:-3], axis=-1))\n",
        "\n",
        "          next_days_pctChg = (ohlct[-1,-4] - ohlct[-predict_next_days,-4]) * 100 / ohlct[-predict_next_days,-4]\n",
        "\n",
        "          if next_days_pctChg  <= -edge:\n",
        "            y.append(0)\n",
        "          elif next_days_pctChg  > -edge and \\\n",
        "            next_days_pctChg  <= 0:\n",
        "            y.append(1)\n",
        "          elif next_days_pctChg  > 0 and \\\n",
        "            next_days_pctChg  <= edge:\n",
        "            y.append(2)\n",
        "          else:\n",
        "            y.append(3)\n",
        "\n",
        "      if len(x) > 0:\n",
        "        X = np.array(x)\n",
        "        y = np.array(y)\n",
        "\n",
        "        # upsample\n",
        "        # dis = district\n",
        "\n",
        "        dis_count, dis_edge = np.histogram(y, bins = 4)\n",
        "\n",
        "        dis_count_max = dis_count.max()\n",
        "\n",
        "        dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "        dis_count_max = dis_count_max + 1.0 #because dis_count plus 1 above, \n",
        "                            #here plus 1 to avoid time below to be zero\n",
        "\n",
        "        time = dis_count_max/dis_count\n",
        "        time = time.astype(int)\n",
        "\n",
        "        time_count = np.apply_along_axis((lambda d: time[d.astype(int)]), \\\n",
        "                                        0 , y)\n",
        "\n",
        "        X = np.repeat(X, time_count, axis = 0)\n",
        "        y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "        # shuffle dataset\n",
        "\n",
        "        length = X.shape[0]\n",
        "\n",
        "        r_index = np.arange(length)\n",
        "        np.random.shuffle(r_index)\n",
        "\n",
        "        X = X[r_index]\n",
        "        y = y[r_index]\n",
        "\n",
        "        # r_index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "        # X = X[r_index]\n",
        "        # y = y[r_index]\n",
        "\n",
        "        y = utils.to_categorical(y, num_classes=4)\n",
        "\n",
        "        batch_size = 512\n",
        "        for ll in range((length-1)//batch_size + 1):\n",
        "          start_i = ll*batch_size\n",
        "          end_i = start_i + batch_size\n",
        "          if end_i <= length:\n",
        "            yield(X[start_i:end_i],y[start_i:end_i])\n",
        "          else:\n",
        "            yield(X[start_i:],y[start_i:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP_3K0IJ8zH4"
      },
      "source": [
        "for i in range(10000):\n",
        "  rr = gntor_1(result, ts_code, edge)\n",
        "  if i%1000 ==0:\n",
        "    print(i)\n",
        "    for ll in rr:\n",
        "      print(ll[0][0,0], ll[1][0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXwWmBX8-cNS"
      },
      "source": [
        "\n",
        "\n",
        "####training by generator 1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwEnxvBo-j-D"
      },
      "source": [
        "\n",
        "pct_of_stock  = 0.005\n",
        "stock_qty = len(ts_code)\n",
        "edge = 10.0\n",
        "\n",
        "step_num = int(stock_qty*pct_of_stock)\n",
        "\n",
        "model.load_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "for i in range(20):\n",
        "  history = model.fit_generator(gntor(result, ts_code, pct_of_stock, edge, batch_size = 512), steps_per_epoch=step_num, epochs=1)\n",
        "  model.save_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "plt.plot(history.history[\"acc\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTFLPfni28mC"
      },
      "source": [
        "edge = 10.0\n",
        "\n",
        "model.load_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "for i in range(20):\n",
        "  history = model.fit_generator(gntor_1(result, ts_code, edge), steps_per_epoch=50000, epochs=1)\n",
        "  model.save_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "plt.plot(history.history[\"acc\"])\n",
        "# plt.plot(history.history[\"loss\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmOyk5_t-Wk6"
      },
      "source": [
        "####define a generator like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-WdEIOh-cZ5"
      },
      "source": [
        "def gntor_like(ts_data, ts_code, pct_of_stock=0.05, edge=10.0):  \n",
        "\n",
        "  stock_qty = len(ts_code)\n",
        "\n",
        "  while True:\n",
        "    index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "    print(len(index))                   \n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in index:\n",
        "    # for i in range(stock_qty):\n",
        "\n",
        "      # x = []\n",
        "      # y = []\n",
        "\n",
        "      stk_data = ts_data[ts_data.code == ts_code.iloc[i][\"code\"]]\n",
        "      length = len(stk_data)\n",
        "      if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "        for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "          ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "              [[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\"]].values.astype(\"float\")\n",
        "          if np.all(ohlct[:,-2] != 1) and \\\n",
        "            np.all(ohlct[:,-3] != 0) and \\\n",
        "          np.all(ohlct[:,-1] >= -10.0) and \\\n",
        "          np.all(ohlct[:,-1] <= 10.0) :\n",
        "            max_price = ohlct[:-predict_next_days,:-3].max()\n",
        "            min_price = ohlct[:-predict_next_days,:-3].min()\n",
        "            scale = max_price - min_price\n",
        "            ohlct[:-predict_next_days,:-3] = ([ohlct[:-predict_next_days,:-3] - min_price]) /scale\n",
        "\n",
        "            x.append(np.expand_dims(ohlct[:-predict_next_days,:-3], axis=-1))\n",
        "\n",
        "            next_days_pctChg = ohlct[-predict_next_days:,-1].sum()\n",
        "\n",
        "            if next_days_pctChg  <= -edge:\n",
        "              y.append(0)\n",
        "            elif next_days_pctChg  > -edge and \\\n",
        "              next_days_pctChg  <= 0:\n",
        "              y.append(1)\n",
        "            elif next_days_pctChg  > 0 and \\\n",
        "              next_days_pctChg  <= edge:\n",
        "              y.append(2)\n",
        "            else:\n",
        "              y.append(3)\n",
        "    # print(\"there\")\n",
        "    if len(x) > 0:\n",
        "      # print(\"here\")\n",
        "      X = np.array(x)\n",
        "      y = np.array(y)\n",
        "\n",
        "      # upsample\n",
        "      # dis = district\n",
        "\n",
        "      dis_count, dis_edge = np.histogram(y, bins = 4, range=(0,3))\n",
        "\n",
        "      dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "      dis_count_max = dis_count.max()\n",
        "\n",
        "      time = dis_count_max/dis_count\n",
        "      time = time.astype(int)\n",
        "\n",
        "      time_count = np.apply_along_axis((lambda d: time[d.astype(int)]), \\\n",
        "                                      0 , y)\n",
        "\n",
        "      X = np.repeat(X, time_count, axis = 0)\n",
        "      y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "      # shuffle dataset\n",
        "\n",
        "      # length = X.shape[0]\n",
        "\n",
        "      # r_index = np.arange(length)\n",
        "      # np.random.shuffle(r_index)\n",
        "\n",
        "      # X = X[r_index]\n",
        "      # y = y[r_index]\n",
        "\n",
        "      # r_index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "      # X = X[r_index]\n",
        "      # y = y[r_index]\n",
        "\n",
        "      y = utils.to_categorical(y, num_classes=4)\n",
        "      return X, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9yUCTn8-cZ-"
      },
      "source": [
        "X, y = gntor_like(result, ts_code, pct_of_stock, edge)\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1VCwWp0M3Ez"
      },
      "source": [
        "####define a generator like using class_weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRcAlNbuMmcH"
      },
      "source": [
        "def gntor_like_cls_w(ts_data, ts_code, pct_of_stock=0.05, edge=10.0,\\\n",
        "                     index=[], shuffle = 0, y_cate = 1):  \n",
        "\n",
        "  stock_qty = len(ts_code)\n",
        "\n",
        "  while True:\n",
        "    if len(index) == 0:\n",
        "      index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "    \n",
        "    print(len(index))                   \n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in index:\n",
        "    # for i in range(stock_qty):\n",
        "\n",
        "      # x = []\n",
        "      # y = []\n",
        "\n",
        "      stk_data = ts_data[ts_data.code == ts_code.iloc[i][\"code\"]]\n",
        "      length = len(stk_data)\n",
        "      if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "        for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "          ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "              [[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\"]].values.astype(\"float\")\n",
        "          if np.all(ohlct[:,-2] != 1) and \\\n",
        "            np.all(ohlct[:,-3] != 0) and \\\n",
        "          np.all(ohlct[:,-1] >= -10.1) and \\\n",
        "          np.all(ohlct[:,-1] <= 10.1) :\n",
        "            max_price = ohlct[:-predict_next_days,:-3].max()\n",
        "            min_price = ohlct[:-predict_next_days,:-3].min()\n",
        "            scale = max_price - min_price\n",
        "            ohlct[:-predict_next_days,:-3] = ([ohlct[:-predict_next_days,:-3] - min_price]) /scale\n",
        "\n",
        "            x.append(np.expand_dims(ohlct[:-predict_next_days,:-3], axis=-1))\n",
        "\n",
        "            # next_days_pctChg = ohlct[-predict_next_days:,-1].sum()  #wrong\n",
        "            next_days_pctChg = (ohlct[-1,-4] - ohlct[-predict_next_days,-4]) * 100 / ohlct[-predict_next_days,-4]\n",
        "\n",
        "            if next_days_pctChg  <= -edge:\n",
        "              y.append(0)\n",
        "            elif next_days_pctChg  <= 0:\n",
        "              y.append(1)\n",
        "            elif next_days_pctChg <= edge:\n",
        "              y.append(2)\n",
        "            else:\n",
        "              y.append(3)\n",
        "    # print(\"there\")\n",
        "    if len(x) > 0:\n",
        "      # print(\"here\")\n",
        "      X = np.array(x)\n",
        "      y = np.array(y)\n",
        "\n",
        "      # upsample\n",
        "      # dis = district\n",
        "\n",
        "      dis_count, dis_edge = np.histogram(y, bins = 4, range=(0,3))\n",
        "\n",
        "      dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "      dis_count_max = dis_count.max()\n",
        "\n",
        "      time = dis_count_max/dis_count\n",
        "\n",
        "      # time = time.astype(int)\n",
        "\n",
        "      # time_count = np.apply_along_axis((lambda d: time[d.astype(int)]), \\\n",
        "      #                                 0 , y)\n",
        "\n",
        "      # X = np.repeat(X, time_count, axis = 0)\n",
        "      # y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "      # shuffle dataset\n",
        "      if shuffle:\n",
        "        length = X.shape[0]\n",
        "\n",
        "        r_index = np.arange(length)\n",
        "        np.random.shuffle(r_index)\n",
        "\n",
        "        X = X[r_index]\n",
        "        y = y[r_index]\n",
        "\n",
        "      # r_index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "      # X = X[r_index]\n",
        "      # y = y[r_index]\n",
        "\n",
        "      if y_cate:\n",
        "        y = utils.to_categorical(y, num_classes=4)\n",
        "      return X, y, time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgEChfSCBMIA"
      },
      "source": [
        "####trainning by generaor like 1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJLhOgd6BdSL"
      },
      "source": [
        "pct_of_stock  = 0.1\n",
        "stock_qty = len(ts_code)\n",
        "edge = 10.0\n",
        "\n",
        "model.load_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "for i in range(10):\n",
        "  X, y = gntor_like(result, ts_code, pct_of_stock, edge)\n",
        "  \n",
        "  print(datetime.datetime.today())\n",
        "\n",
        "  for l in range(10):\n",
        "    history = model.fit(X, y, batch_size = 1024, epochs = 10,\\\n",
        "                validation_split = 0.1, shuffle = True, \\\n",
        "                # callbacks = [tbCallBack]\\\n",
        "                ) \n",
        "\n",
        "    model.save_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "plt.plot(history.history[\"acc\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "incI8NtA8Lp9"
      },
      "source": [
        "t1, t2 = gntor(result, ts_code, pct_of_stock, edge, batch_size = 512, generator = False)\n",
        "print(t1, t2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3ZxQVErSrQO"
      },
      "source": [
        "####trainning by generator like 1-7 using class_weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xwu_B3SS_04"
      },
      "source": [
        "pct_of_stock  = 0.05\n",
        "stock_qty = len(ts_code)\n",
        "edge = 10.0\n",
        "\n",
        "model.load_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "X, y, cls_weight = gntor_like_cls_w(result, ts_code, pct_of_stock, edge)\n",
        "\n",
        "for i in range(5):\n",
        "  \n",
        "  print(datetime.datetime.today())\n",
        "\n",
        "  for l in range(5):\n",
        "    history = model.fit(X, y, batch_size = 1024, epochs = 40,\\\n",
        "                validation_split = 0.1, shuffle = True, \\\n",
        "                class_weight = cls_weight,\\\n",
        "                # callbacks = [tbCallBack]\\\n",
        "                ) \n",
        "\n",
        "    model.save_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "plt.plot(history.history[\"acc\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f15-DeZjU8ha"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIZeDUd339Cy"
      },
      "source": [
        "####trainning by generator like 1-7 using class_weight using specific set of ts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgvgNx7j4MjX"
      },
      "source": [
        "ts_list = [\"sh.600004\", \"sh.600352\", \"sh.601588\", \"sh.601111\", \"sz.000898\",\\\n",
        "       \"sh.601088\", \"sh.600368\", \"sz.002178\", \"sz.000761\", \"sh.600596\",\\\n",
        "       \"sz.300110\", \"sz.300308\", \"sz.000333\", \"sh.600588\", \"sh.600036\",\\\n",
        "       \"sz.002215\", \"sz.002353\", \"sz.002695\", \"sz.002422\", \"sz.000651\",\\\n",
        "       \"sz.002178\", \"sz.002023\", \"sz.300146\", \"sz.002001\", \"sz.300110\",\\\n",
        "       \"sz.000830\", \"sz.000157\"\n",
        "        ]\n",
        "\n",
        "ts_code = ts_code[~ts_code[\"code\"].isin(ts_list)]        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UWUQuQw4VKg"
      },
      "source": [
        "ts_code = ts_code[~ts_code[\"code\"].isin(ts_list)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSa7K-wB8KPP"
      },
      "source": [
        "ts_code.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uuZd5EhXsRz"
      },
      "source": [
        "####dataset preparing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrqizQgqK-Li"
      },
      "source": [
        "#vaildation set\n",
        "\n",
        "ts_set = pd.DataFrame(ts_list, columns = [\"code\"])\n",
        "\n",
        "pct_of_stock  = 1\n",
        "stock_qty = len(ts_code)\n",
        "edge = 10.0\n",
        "\n",
        "X, y, cls_weight = gntor_like_cls_w(result, ts_set, pct_of_stock, edge, [], 1, 0)\n",
        "\n",
        "np.save(\"X-3-1-7-w.npy\", X)\n",
        "np.save(\"y-3-1-7-w.npy\", y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6TFl7ZJDNki"
      },
      "source": [
        "#training set\n",
        "\n",
        "# pct_of_stock = 0.1\n",
        "pct_of_stock = 1\n",
        "edge = 10.0\n",
        "\n",
        "X1, y1, cls_weight = gntor_like_cls_w(result, ts_code[:1000], pct_of_stock, edge, [], 1, 0)\n",
        "# np.save(\"X-3-1-7-w1.npy\", X1)\n",
        "# np.save(\"y-3-1-7-w1.npy\", y1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWSjr0sPVqus"
      },
      "source": [
        "pct_of_stock = 1\n",
        "edge = 10.0\n",
        "\n",
        "X1, y1, cls_weight = gntor_like_cls_w(result, ts_code[:1000], pct_of_stock, edge, [], 1, 0)\n",
        "\n",
        "np.save(\"X-3-1-7-w1000.npy\", X1)\n",
        "np.save(\"y-3-1-7-w1000.npy\", y1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNPqO_57Vy7x"
      },
      "source": [
        "pct_of_stock = 1\n",
        "edge = 10.0\n",
        "\n",
        "X1, y1, cls_weight = gntor_like_cls_w(result, ts_code[1000:2000], pct_of_stock, edge, [], 1, 0)\n",
        "\n",
        "np.save(\"X-3-1-7-w2000.npy\", X1)\n",
        "np.save(\"y-3-1-7-w2000.npy\", y1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk5E0ngDV6vg"
      },
      "source": [
        "pct_of_stock = 1\n",
        "edge = 10.0\n",
        "\n",
        "X1, y1, cls_weight = gntor_like_cls_w(result, ts_code[2000:3000], pct_of_stock, edge, [], 1, 0)\n",
        "\n",
        "np.save(\"X-3-1-7-w3000.npy\", X1)\n",
        "np.save(\"y-3-1-7-w3000.npy\", y1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhUD9mHcWAIG"
      },
      "source": [
        "pct_of_stock = 1\n",
        "edge = 10.0\n",
        "\n",
        "X1, y1, cls_weight = gntor_like_cls_w(result, ts_code[3000:4000], pct_of_stock, edge, [], 1, 0)\n",
        "\n",
        "np.save(\"X-3-1-7-w4000.npy\", X1)\n",
        "np.save(\"y-3-1-7-w4000.npy\", y1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJUpdOI4X-fy"
      },
      "source": [
        "####data set load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zT8Ovsz1ceJ"
      },
      "source": [
        "print(datetime.datetime.today())\n",
        "\n",
        "pct_of_stock = 0.1\n",
        "edge = 10.0\n",
        "v_split_pct = 0.2\n",
        "\n",
        "X = np.load(\"X-3-1-7-w.npy\")\n",
        "y = np.load(\"y-3-1-7-w.npy\")\n",
        "v_split = int(len(X) * (1-0.2))\n",
        "X_t = X[:v_split]\n",
        "X_v = X[v_split:]\n",
        "y_t = y[:v_split]\n",
        "y_v = y[v_split:]\n",
        "\n",
        "X1 = np.load(\"X-3-1-7-w1.npy\")\n",
        "y1 = np.load(\"y-3-1-7-w1.npy\")\n",
        "\n",
        "X_t = np.concatenate((X_t, X1), axis = 0)\n",
        "y_t = np.concatenate((y_t, y1), axis = 0)\n",
        "\n",
        "dis_count, dis_edge = np.histogram(y_t, bins = 4, range=(0,3))\n",
        "\n",
        "dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "dis_count_max = dis_count.max()\n",
        "\n",
        "cls_weight = dis_count_max/dis_count\n",
        "# cls_weight = cls_weight / cls_weight.sum()\n",
        "\n",
        "cls_weight = dict(zip([0,1,2,3],cls_weight))\n",
        "\n",
        "y_t = utils.to_categorical(y_t, num_classes=4)\n",
        "y_v = utils.to_categorical(y_v, num_classes=4)\n",
        "\n",
        "\n",
        "\n",
        "print(datetime.datetime.today())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqElYGyXFvvm"
      },
      "source": [
        "print(datetime.datetime.today())\n",
        "\n",
        "# pct_of_stock = 0.1\n",
        "# edge = 10.0\n",
        "v_split_pct = 0.2\n",
        "\n",
        "X1000 = np.load(\"X-3-1-7-w1000.npy\")\n",
        "y1000 = np.load(\"y-3-1-7-w1000.npy\")\n",
        "X2000 = np.load(\"X-3-1-7-w2000.npy\")  \n",
        "y2000 = np.load(\"y-3-1-7-w2000.npy\")\n",
        "# X3000 = np.load(\"X-3-1-7-w3000.npy\")\n",
        "# y3000 = np.load(\"y-3-1-7-w3000.npy\")\n",
        "# X4000 = np.load(\"X-3-1-7-w4000.npy\")\n",
        "# y4000 = np.load(\"y-3-1-7-w4000.npy\")\n",
        "\n",
        "\n",
        "X = np.load(\"X-3-1-7-w.npy\")\n",
        "y = np.load(\"y-3-1-7-w.npy\")\n",
        "v_split = int(len(X) * (1-v_split_pct))\n",
        "X_t = X[:v_split]\n",
        "X_v = X[v_split:]\n",
        "y_t = y[:v_split]\n",
        "y_v = y[v_split:]\n",
        "\n",
        "\n",
        "X_t = np.concatenate((X_t, \n",
        "                      X1000, \n",
        "                      X2000, \n",
        "                      # X3000, \n",
        "                      # X4000\n",
        "                      ), axis = 0)\n",
        "y_t = np.concatenate((y_t, \n",
        "                      y1000, \n",
        "                      y2000, \n",
        "                      # y3000, \n",
        "                      # y4000\n",
        "                      ), axis = 0)\n",
        "\n",
        "dis_count, dis_edge = np.histogram(y_t, bins = 4, range=(0,3))\n",
        "\n",
        "dis_count = dis_count + 1.0 #plus 1 to avoid devide by zero\n",
        "dis_count_max = dis_count.max()\n",
        "\n",
        "cls_weight = dis_count_max/dis_count\n",
        "# cls_weight = cls_weight / cls_weight.sum()\n",
        "\n",
        "cls_weight = dict(zip([0,1,2,3],cls_weight))\n",
        "\n",
        "y_t = utils.to_categorical(y_t, num_classes=4)\n",
        "y_v = utils.to_categorical(y_v, num_classes=4)\n",
        "\n",
        "\n",
        "\n",
        "print(datetime.datetime.today())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RHZZTa2sdib"
      },
      "source": [
        "print(y_t.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwGP-23dUoKJ"
      },
      "source": [
        "# cls_weight = cls_weight / cls_weight.sum()\n",
        "print(cls_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dZBRMm3-oPK"
      },
      "source": [
        "cls_weight = dict(zip([0,1,2,3],cls_weight))\n",
        "print(cls_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUNnRWjlHKE9"
      },
      "source": [
        "####training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEB7VSOu39C4"
      },
      "source": [
        "\n",
        "\n",
        "chk_point = ModelCheckpoint(filepath='stock_predict_3_1-7-set.h5')\n",
        "# model.load_weights(\"stock_predict_3_1-7-set.h5\")\n",
        "\n",
        "for i in range(8):\n",
        "\n",
        "  print(datetime.datetime.today())\n",
        "  model = make_model()\n",
        "  model.load_weights(\"stock_predict_3_1-7-set.h5\")\n",
        "  for l in range(2):\n",
        "    history = model.fit(X_t, y_t, batch_size = 1024, epochs = 20,\\\n",
        "                # validation_split = 0.1, shuffle = True, \\\n",
        "                class_weight = cls_weight,\\\n",
        "                validation_data = (X_v ,y_v),\\\n",
        "                # callbacks = [tbCallBack]\\\n",
        "                callbacks = [chk_point],\\\n",
        "                # initial_epoch = 5 \n",
        "                ) \n",
        "\n",
        "    model.save_weights(\"stock_predict_3_1-7-set.h5\")\n",
        "\n",
        "  model.save_weights(\"stock_predict_3_1-7-set_a.h5\")\n",
        "  del model\n",
        "  gc.collect()\n",
        "plt.plot(history.history[\"acc\"]) \n",
        "print(datetime.datetime.today())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPT5JRyR39DH"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwNOB7RxxA5e"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnKqpXfOoUOU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9b2gvH491q5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQi1AwMo_LoC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN8mbQlliNHU"
      },
      "source": [
        "###drawing validation 1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIwL0cmpiNHW"
      },
      "source": [
        "\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = np.random.randint(0, stock_qty)\n",
        "\n",
        "                      \n",
        "x_plot = []\n",
        "y_plot = []\n",
        "yy_plot = []\n",
        "\n",
        "stk_data = result[result.code == ts_code.iloc[index][\"code\"]]\n",
        "length = len(stk_data)\n",
        "if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "  for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "    ohlct = stk_data.iloc[l : l + window_size + predict_next_days] \\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"isST\",\"pctChg\"]].values\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "      min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "      max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "      scale = (max_price - min_price)\n",
        "      ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:-predict_next_days,:-2], axis = -1)])\n",
        "\n",
        "\n",
        "      next_days_pctChg = ohlct[-predict_next_days:,-1].sum()\n",
        "\n",
        "      if next_days_pctChg  <= -edge:\n",
        "        y_plot.append(0)\n",
        "      elif next_days_pctChg  > -edge and \\\n",
        "        next_days_pctChg  <= 0:\n",
        "        y_plot.append(1)\n",
        "      elif next_days_pctChg  > 0 and \\\n",
        "        next_days_pctChg  <= edge:\n",
        "        y_plot.append(2)\n",
        "      else:\n",
        "        y_plot.append(3)\n",
        "\n",
        "\n",
        "\n",
        "  X_plot = np.array(x_plot)\n",
        "  y_plot = np.array(y_plot)\n",
        "\n",
        "  # yy_plot = np.array(yy_plot)\n",
        "  print(ts_code.iloc[index])    \n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvs-gaKkiNHf"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-7.h5\")\n",
        "p = model.predict(X_plot)\n",
        "p = np.apply_along_axis(lambda d: d.argmax() + 1, 1, p)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot[-100:], color=\"blue\")\n",
        "plt.plot(p[-100:], color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNFADremiNHi"
      },
      "source": [
        "np.apply_along_axis(lambda d: d.argmax(), 1, p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytjj8iN5iNHk"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-6.h5\")\n",
        "model.evaluate(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-2eEeYniNHo"
      },
      "source": [
        "model.load_weights(\"stock_predict_3_1-6.h5\")\n",
        "\n",
        "model.evaluate(X_plot, utils.to_categorical(y_plot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szBLpsH5iNHt"
      },
      "source": [
        "###get data of specific code 1-7\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZErhAkvHiNHt"
      },
      "source": [
        "s_code = \"sh.600004\"\n",
        "\n",
        "s_data_file = s_code + \"_1-7.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB9ICGkaiNHw"
      },
      "source": [
        "\n",
        "s_code = \"sh.600004\"\n",
        "\n",
        "s_data_file = s_code + \"_1-7.csv\"\n",
        "\n",
        "bs.login()\n",
        "\n",
        "rs = bs.query_history_k_data_plus(s_code, \",\".join(data_fields),\\\n",
        "                          start_date = start_date, end_date = end_date,\\\n",
        "                            frequency = \"d\", adjustflag = adjustflag)\n",
        "\n",
        "data_list = []\n",
        "while (rs.error_code == \"0\") and rs.next():\n",
        "    row_data = rs.get_row_data()\n",
        "    if(row_data[6] == \"1\"):         \n",
        "        data_list.append(row_data)\n",
        "s_result = pd.DataFrame(data_list, columns = rs.fields)\n",
        "s_result.to_csv(s_data_file)\n",
        "print(s_result.head())\n",
        "\n",
        "bs.logout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASbuU208iNHz"
      },
      "source": [
        "###dataset of specific code 1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9MrDiHYiNH1"
      },
      "source": [
        "edge = 10.0\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "stk_data = pd.read_csv(s_data_file)\n",
        "length = len(stk_data)\n",
        "if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        " for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "  ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "  if np.all(ohlct[:, -2] != 1) and \\\n",
        "     np.all(ohlct[:,-1] > -10.0) and \\\n",
        "      np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "  min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "  scale = max_price - min_price\n",
        "  ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "  x.append(np.expand_dims(ohlct[:-predict_next_days,:-2], axis=-1))\n",
        "\n",
        "  # y.extend([utils.to_categorical(ohlct[-1,-1], num_classes=201)])\n",
        "  # ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "  next_days_pctChg = ohlct[-predict_next_days:,-1].sum()\n",
        "\n",
        "  if next_days_pctChg  <= -edge:\n",
        "    y.append(0)\n",
        "  elif next_days_pctChg  > -edge and \\\n",
        "    next_days_pctChg  <= 0:\n",
        "    y.append(1)\n",
        "  elif next_days_pctChg  > 0 and \\\n",
        "    next_days_pctChg  <= edge:\n",
        "    y.append(2)\n",
        "  else:\n",
        "    y.append(3)\n",
        "\n",
        "\n",
        "\n",
        "s_X = np.array(x)\n",
        "s_y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "\n",
        "dis_count, dis_edge = np.histogram(y, bins = 4)\n",
        "\n",
        "dis_count_max = dis_count.max()\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[d.astype(int)]), \\\n",
        "                                0 , s_y)\n",
        "\n",
        "s_X = np.repeat(s_X, time_count, axis = 0)\n",
        "s_y = np.repeat(s_y, time_count, axis = 0)\n",
        "\n",
        "\n",
        "#shuffle\n",
        "length = s_X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "s_X = s_X[index]\n",
        "s_y = s_y[index]\n",
        "\n",
        "# index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "s_X = s_X[index]\n",
        "s_y = s_y[index]\n",
        "\n",
        "s_y = utils.to_categorical(s_y)\n",
        "\n",
        "print(s_X.shape)\n",
        "print(s_X[1])\n",
        "print(s_y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsEa0VvSiNH5"
      },
      "source": [
        "###training basing on model1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENAkiw8GiNH6"
      },
      "source": [
        "# model.load_weights(\"stock_predict_3_1-7.h5\")\n",
        "model.load_weights(s_data_file + \".h5\")\n",
        "\n",
        "history = model.fit(s_X, s_y, batch_size = 128, epochs = 64,\\\n",
        "            validation_split = 0.1, shuffle = True, \\\n",
        "            # callbacks = [tbCallBack]\\\n",
        "            )\n",
        "plt.plot(history.history[\"acc\"])\n",
        "# plt.plot(history.history[\"loss\"])\n",
        "model.save_weights(s_data_file + \".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYmf-CfEiNIB"
      },
      "source": [
        "###get more data for specific code 1-7\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1ioEmf6iNIC"
      },
      "source": [
        "s_code = \"sz.002064\"\n",
        "\n",
        "bs.login()\n",
        "\n",
        "s_date = \"2019-07-01\"\n",
        "e_date = \"2020-09-11\"\n",
        "\n",
        "rs = bs.query_history_k_data_plus(s_code, \",\".join(data_fields),\\\n",
        "                          start_date = s_date, end_date = e_date,\\\n",
        "                            frequency = \"d\", adjustflag = adjustflag)\n",
        "\n",
        "data_list = []\n",
        "while (rs.error_code == \"0\") and rs.next():\n",
        "    row_data = rs.get_row_data()\n",
        "    if(row_data[6] == \"1\"):         \n",
        "        data_list.append(row_data)\n",
        "p_result = pd.DataFrame(data_list, columns = rs.fields)\n",
        "\n",
        "print(p_result.head())\n",
        "print(p_result.tail())\n",
        "\n",
        "bs.logout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIeRKqoMiNIE"
      },
      "source": [
        "print(p_result.loc[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHnZibUYiNIJ"
      },
      "source": [
        "print(p_result.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp1z1px9Qi6w"
      },
      "source": [
        "print(p_result[190:210]['close'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkaEWpKUOPkF"
      },
      "source": [
        "plt.figure(figsize=(12,4.8))\n",
        "plt.plot(p_result['close'].values.astype(\"float\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEzSEPJOfO-m"
      },
      "source": [
        "plt.figure(figsize=(12,4.8))\n",
        "plt.plot(p_result['pctChg'].values.astype(\"float\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhGFJpl_iNIN"
      },
      "source": [
        "###predict for specific code 1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhLSAQ7CiNIO"
      },
      "source": [
        "edge = 10.0\n",
        "                      \n",
        "x_plot = []\n",
        "y_plot = []\n",
        "\n",
        "stk_data = p_result\n",
        "length = len(stk_data)\n",
        "if ((length - predict_next_days) > window_size):\n",
        "  for l in range(length - window_size - predict_next_days):\n",
        "\n",
        "    ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "    [[\"open\", \"high\", \"low\", \"close\", \"tradestatus\",\\\n",
        "      \"isST\", \"pctChg\"]].values.astype(\"float\")\n",
        "    if np.all(ohlct[:,-2] != 1) and \\\n",
        "      np.all(ohlct[:,-3] != 0) and \\\n",
        "    np.all(ohlct[:,-1] >= -10.1) and \\\n",
        "    np.all(ohlct[:,-1] <= 10.1) :\n",
        "      max_price = ohlct[:-predict_next_days,:-3].max()\n",
        "      min_price = ohlct[:-predict_next_days,:-3].min()\n",
        "      scale = max_price - min_price\n",
        "      ohlct[:-predict_next_days,:-3] = ([ohlct[:-predict_next_days,:-3] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:-predict_next_days,:-3], axis = -1)])\n",
        "\n",
        "      # next_days_pctChg = ohlct[-predict_next_days:,-1].sum()\n",
        "      next_days_pctChg = (ohlct[-1,-4] - ohlct[-predict_next_days,-4]) * 100 / ohlct[-predict_next_days,-4]\n",
        "\n",
        "      if next_days_pctChg  <= -edge:\n",
        "        y_plot.append(0)\n",
        "      elif next_days_pctChg <= 0:\n",
        "        y_plot.append(1)\n",
        "      elif next_days_pctChg <= edge:\n",
        "        y_plot.append(2)\n",
        "      else:\n",
        "        y_plot.append(3)\n",
        "\n",
        "\n",
        "  X_plot = np.array(x_plot)\n",
        "  y_plot = np.array(y_plot)\n",
        "  # yy_plot = np.array(yy_plot)  \n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owqZQ4p7ICY1"
      },
      "source": [
        "y_plot.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyYHaVJdiNIR"
      },
      "source": [
        "# model.load_weights(s_data_file + \".h5\")\n",
        "# model.load_weights(\"stock_predict_3_1-6.h5\")\n",
        "model.load_weights(\"stock_predict_3_1-7-set.h5\")\n",
        "\n",
        "p = model.predict(X_plot)\n",
        "p = np.apply_along_axis(lambda d: d.argmax() + 1, 1, p)\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot, color=\"blue\")\n",
        "plt.plot(p, color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruLtDQWal0p9"
      },
      "source": [
        "model.evaluate(X_plot[-180:], utils.to_categorical(y_plot[-180:], num_classes=4) )\n",
        "# model.evaluate(X_plot, utils.to_categorical(y_plot, num_classes=4) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfVIVRZYl8m-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsGAjV9-iNIW"
      },
      "source": [
        "model.load_weights(s_data_file + \".h5\")\n",
        "model.evaluate(X_plot, utils.to_categorical(y_plot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKmHj-mc0vGR"
      },
      "source": [
        "###predict1-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBjNsl7X0zAR"
      },
      "source": [
        "bs.login()\n",
        "\n",
        "ss_date = \"2018-06-01\"\n",
        "ee_date = \"2019-10-30\"\n",
        "\n",
        "# ss_code = \"sh.600004\"\n",
        "ss_code = \"sz.000717\"\n",
        "\n",
        "rs = bs.query_history_k_data_plus(ss_code, \",\".join(data_fields),\\\n",
        "                          start_date = ss_date, end_date = ee_date,\\\n",
        "                            frequency = \"d\", adjustflag = adjustflag)\n",
        "\n",
        "data_list = []\n",
        "while (rs.error_code == \"0\") and rs.next():\n",
        "    row_data = rs.get_row_data()\n",
        "    if(row_data[6] == \"1\"):         \n",
        "        data_list.append(row_data)\n",
        "pp_result = pd.DataFrame(data_list, columns = rs.fields)\n",
        "\n",
        "print(pp_result.head())\n",
        "\n",
        "bs.logout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUuNOI491P9T"
      },
      "source": [
        "x_plot = []\n",
        "y_plot = []\n",
        "\n",
        "stk_data = pp_result\n",
        "length = len(stk_data)\n",
        "if (length > window_size):\n",
        "  for l in range(length - window_size):\n",
        "    ohlct = stk_data.iloc[l : l + window_size] \\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"isST\",\"pctChg\"]].values\n",
        "    ohlct = ohlct.astype(\"float\")\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "      min_price = ohlct[:,:-2].min()\n",
        "      max_price = ohlct[:,:-2].max()\n",
        "      scale = (max_price - min_price)\n",
        "      ohlct[:,:-2] = ([ohlct[:,:-2] - min_price]) /scale\n",
        "\n",
        "      x_plot.extend([np.expand_dims(ohlct[:,:-2], axis = -1)])\n",
        "\n",
        "  XX_plot = np.array(x_plot)\n",
        "\n",
        "\n",
        "else:\n",
        "  print(\"data too short, try again!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfX2iJ_p1lq8"
      },
      "source": [
        "# model.load_weights(s_data_file + \".h5\")\n",
        "model.load_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "p = model.predict(XX_plot[-40:])\n",
        "p = np.apply_along_axis(lambda d: d.argmax() + 1, 1, p)\n",
        "\n",
        "plt.figure(figsize=(15,4.8))\n",
        "plt.plot(y_plot, color=\"blue\")\n",
        "plt.plot(p, color=\"red\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAsGTqSIlZwS"
      },
      "source": [
        "#model 2: LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxoK7Xn8ll45"
      },
      "source": [
        "##model 2-1: LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSjx6XIglpZJ"
      },
      "source": [
        "\n",
        "K.clear_session()\n",
        "input = layers.Input(shape = (window_size, factor_num -1 )) #turn is not involved\n",
        "model = layers.LSTM(64, input_shape = (window_size, factor_num - 1), \\\n",
        "                    return_sequences = True)(input)\n",
        "# # model = layers.LSTM(64, return_sequences = True)(model)\n",
        "model = layers.Acvtivation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.LSTM(32)(model)\n",
        "model = layers.Acvtivation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(32)(model)\n",
        "model = layers.Acvtivation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(16)(model)\n",
        "model = layers.Acvtivation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(8)(model)\n",
        "model = layers.Acvtivation(\"relu\")(model)\n",
        "model = layers.BatchNormalization()(model)\n",
        "model = layers.Dropout(0.5)(model)\n",
        "\n",
        "model = layers.Dense(1)(model)\n",
        "\n",
        "model = Model(inputs = input, outputs = model)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "optimizer = optimizers.RMSprop(lr = 0.005, decay = 1e-5)\n",
        "\n",
        "model.compile(loss = \"mse\", optimizer = optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QXDMeMBn0h6"
      },
      "source": [
        "###dataset 2-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofs8O4q7oF1t"
      },
      "source": [
        "\n",
        "pct_of_stock  = 0.004\n",
        "stock_qty = len(ts_code)\n",
        "\n",
        "index = random.sample(range(0,stock_qty), int(stock_qty*pct_of_stock))\n",
        "\n",
        "print(np.sort(index))\n",
        "                      \n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in index:\n",
        "  stk_data = result[result.code == ts_code.iloc[i][\"code\"]]\n",
        "  length = len(stk_data)\n",
        "  if ((length - days_from_ipo - predict_next_days) > window_size):\n",
        "    for l in range(days_from_ipo, length - window_size - predict_next_days):\n",
        "      ohlct = stk_data.iloc[l : l + window_size + predict_next_days]\\\n",
        "          [[\"open\", \"high\", \"low\", \"close\", \"isST\", \"pctChg\"]].values\n",
        "      if np.all(ohlct[:, -2] != 1) and \\\n",
        "       np.all(ohlct[:,-1] > -10.0) and \\\n",
        "       np.all(ohlct[:,-1] < 10.0) :\n",
        "        max_price = ohlct[:-predict_next_days,:-2].max()\n",
        "        min_price = ohlct[:-predict_next_days,:-2].min()\n",
        "        scale = max_price - min_price\n",
        "        ohlct[:-predict_next_days,:-2] = ([ohlct[:-predict_next_days,:-2] - min_price]) /scale\n",
        "\n",
        "        x.append(ohlct[:-predict_next_days,:-2])\n",
        "\n",
        "        ohlct[-predict_next_days,-1] = (ohlct[-predict_next_days,-1] + 10.0) / 20.0\n",
        "        y.append(ohlct[-predict_next_days,-1])\n",
        "\n",
        "\n",
        "\n",
        "X = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "# upsample\n",
        "# dis = district\n",
        "dis_count, dis_edge = np.histogram(y, bins = 20)\n",
        "\n",
        "dis_count_max = dis_count.max() * 4\n",
        "\n",
        "dis_count = dis_count_max/dis_count\n",
        "dis_count = dis_count.astype(int)\n",
        "\n",
        "time_count = np.apply_along_axis((lambda d: dis_count[(d*100//5).astype(int)]), \\\n",
        "                                0 , y)\n",
        "\n",
        "X = np.repeat(X, time_count, axis = 0)\n",
        "y = np.repeat(y, time_count, axis = 0)\n",
        "\n",
        "length = X.shape[0]\n",
        "\n",
        "index = np.arange(length)\n",
        "np.random.shuffle(index)\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "index = random.sample(range(0,length), int(length / 4))\n",
        "\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "\n",
        "print(X.shape)\n",
        "print(X[1])\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJVW4-P93844"
      },
      "source": [
        "#Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6ZDA4D_Jw0l"
      },
      "source": [
        "##contants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXQ3phe1gj0i"
      },
      "source": [
        "s_data_file = \"sh.600004\" + \"_1-7.csv\"\n",
        "pstart_date = \"2019-03-01\"\n",
        "pend_date = \"2019-10-25\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkuQIxtaJ7jr"
      },
      "source": [
        "##load ts_code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3pIPHpwKAz_"
      },
      "source": [
        "ts_code = pd.read_csv(ts_code_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaQQSrknNZOb"
      },
      "source": [
        "##*get* data in specific period"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upHl1WCa4NDU",
        "outputId": "17c61873-f184-44b9-8f78-1a6ac0a5d44f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "source": [
        "data_list = []\n",
        "\n",
        "\n",
        "bs.login()\n",
        "for i in range(len(ts_code)):\n",
        "  if i%500 == 0 :\n",
        "    print(i)\n",
        "  rs = bs.query_history_k_data_plus(ts_code.iloc[i][\"code\"], \",\".join(data_fields),\\\n",
        "                              start_date = pstart_date, end_date = pend_date,\\\n",
        "                               frequency = \"d\", adjustflag = adjustflag)\n",
        "\n",
        "\n",
        "  while (rs.error_code == \"0\") and rs.next():\n",
        "    row_data = rs.get_row_data()\n",
        "    if(row_data[6] == \"1\"):         \n",
        "          data_list.append(row_data)\n",
        "\n",
        "bs.logout()\n",
        "\n",
        "presult = pd.DataFrame(data_list, columns = rs.fields)\n",
        "print(presult.head())\n",
        "print(presult.tail())\n",
        "print(presult.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "login success!\n",
            "0\n",
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n",
            "2500\n",
            "3000\n",
            "3500\n",
            "logout success!\n",
            "             open            high             low  ... tradestatus isST       code\n",
            "0  112.0159840200  114.3871945200  110.0241672000  ...           1    0  sh.600000\n",
            "1  114.9562850400  117.4223439600  113.3438619000  ...           1    0  sh.600000\n",
            "2  113.9129524200  114.1974976800  112.5850745400  ...           1    0  sh.600000\n",
            "3  113.7232555800  115.2408303000  112.4902261200  ...           1    0  sh.600000\n",
            "4  114.1026492600  114.1974976800  112.5850745400  ...           1    0  sh.600000\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "                  open            high  ... isST       code\n",
            "575136  368.9718880000  370.4658230900  ...    0  sz.300782\n",
            "575137  363.0262268700  380.0009390000  ...    0  sz.300782\n",
            "575138  378.9982980000  378.9982980000  ...    0  sz.300782\n",
            "575139  358.9454780000  366.7460249800  ...    0  sz.300782\n",
            "575140  354.9349140000  365.9639650000  ...    0  sz.300782\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "575141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVbI0AWZz64w",
        "outputId": "db9da453-00ea-4c8d-9600-0ee487062220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "presult[presult.code == \"sh.600526\"].iloc[-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "open           15.6064688000\n",
              "high           15.6364812400\n",
              "low            15.4263941600\n",
              "close          15.6364812400\n",
              "turn                0.244600\n",
              "pctChg              0.192300\n",
              "tradestatus                1\n",
              "isST                       1\n",
              "code               sh.600526\n",
              "Name: 67543, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p09S-yNaQdIG"
      },
      "source": [
        "##predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKOVw9tMJrzd",
        "outputId": "3be2713c-7f19-4871-de27-4c30e295394a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "ts = []\n",
        "pp = []\n",
        "\n",
        "# model.load_weights(s_data_file + \".h5\")\n",
        "model.load_weights(\"stock_predict_3_1-7.h5\")\n",
        "\n",
        "for i in range(len(ts_code)):\n",
        "  if i%500 == 0:\n",
        "    print(i)\n",
        "  x = []\n",
        "  stk_data = presult[presult.code == ts_code.iloc[i][\"code\"]]\n",
        "  length = len(stk_data)\n",
        "  if length >= window_size :\n",
        "    # for l in range(length - window_size):\n",
        "    ohlct = stk_data.iloc[length - window_size : length]\\\n",
        "        [[\"open\", \"high\", \"low\", \"close\", \"tradestatus\", \"isST\", \"pctChg\"]].values.astype(\"float\")\n",
        "    if np.all(ohlct[:, -2] != 1) and \\\n",
        "      np.all(ohlct[:,-3] != 0) and \\\n",
        "      np.all(ohlct[:,-1] >= -10.0) and \\\n",
        "      np.all(ohlct[:,-1] <= 10.0) :\n",
        "      max_price = ohlct[:,:-3].max()\n",
        "      min_price = ohlct[:,:-3].min()\n",
        "      scale = max_price - min_price\n",
        "      ohlct[:,:-3] = ([ohlct[:,:-3] - min_price]) /scale\n",
        "\n",
        "      x.append(np.expand_dims(ohlct[:,:-3], axis=-1))\n",
        "\n",
        "    if len(x) > 0:\n",
        "      pX = np.array([x[-1]])\n",
        "      # print(pX.shape)\n",
        "      p = model.predict(pX)      \n",
        "      if p[-1].argmax() == 3 and p[-1,3] > 0.995:\n",
        "        ts.append(ts_code.iloc[i][\"code\"])\n",
        "        pp.append(p[-1])\n",
        "\n",
        "print(ts)\n",
        "print(len(ts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n",
            "2500\n",
            "3000\n",
            "3500\n",
            "['sh.600009', 'sh.600208', 'sh.600262', 'sh.600271', 'sh.600312', 'sh.600339', 'sh.600436', 'sh.600516', 'sh.600616', 'sh.600633', 'sh.600642', 'sh.600653', 'sh.601900', 'sh.601985', 'sh.603165', 'sh.603444', 'sh.603650', 'sh.603658', 'sh.603757', 'sh.603828', 'sz.000100', 'sz.000403', 'sz.000597', 'sz.000761', 'sz.000932', 'sz.000999', 'sz.001965', 'sz.002148', 'sz.002322', 'sz.002335', 'sz.002545', 'sz.002558', 'sz.002583', 'sz.002588', 'sz.002594', 'sz.002634', 'sz.002706', 'sz.002721', 'sz.002805', 'sz.002912', 'sz.002932', 'sz.002940', 'sz.300192', 'sz.300226', 'sz.300246', 'sz.300393', 'sz.300421', 'sz.300558', 'sz.300572', 'sz.300759']\n",
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E40kbJsKsKNE",
        "outputId": "44c43377-4fe6-4bec-ef4a-835b9318ae0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "for i in range(len(ts)):\n",
        "  if(pp[i][3]) > 0.998:\n",
        "    print(ts[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sh.600009\n",
            "sh.600208\n",
            "sh.600262\n",
            "sh.600271\n",
            "sh.600339\n",
            "sh.600436\n",
            "sh.600516\n",
            "sh.600616\n",
            "sh.600633\n",
            "sh.601900\n",
            "sh.601985\n",
            "sh.603444\n",
            "sh.603650\n",
            "sh.603757\n",
            "sh.603828\n",
            "sz.000100\n",
            "sz.000403\n",
            "sz.000932\n",
            "sz.000999\n",
            "sz.001965\n",
            "sz.002335\n",
            "sz.002545\n",
            "sz.002558\n",
            "sz.002583\n",
            "sz.002588\n",
            "sz.002594\n",
            "sz.002634\n",
            "sz.002706\n",
            "sz.002721\n",
            "sz.002805\n",
            "sz.002912\n",
            "sz.002940\n",
            "sz.300192\n",
            "sz.300226\n",
            "sz.300246\n",
            "sz.300393\n",
            "sz.300421\n",
            "sz.300558\n",
            "sz.300572\n",
            "sz.300759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbBeYEWfnzJf",
        "outputId": "50f29491-46dc-46e6-ca92-92a22685449e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "pp[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.8331841e-07, 8.0780563e-07, 4.6011610e-03, 9.9539739e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8jQDq6fm-Sh",
        "outputId": "62a684e4-d5f5-46e7-f651-c3d2b89c7a0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "for i in range(len(ts)):\n",
        "  if ts[i] == \"sh.600009\":\n",
        "     print(pp[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.6273554e-08 1.2722618e-08 3.0374675e-04 9.9969625e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCP5NnXqsKAK",
        "outputId": "121fea90-3759-4a3a-ffdd-00c52f538e08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "# a = np.array(x[185:188])\n",
        "a =  pd.DataFrame(x[190].reshape(90,-1))\n",
        "print(a.shape)\n",
        "# print(x[187])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-174-714c304c87f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m190\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(x[187])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 344 into shape (90,newaxis)"
          ]
        }
      ]
    }
  ]
}